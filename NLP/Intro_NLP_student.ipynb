{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re9Lt_eGi1R-"
   },
   "source": [
    "# Introducción al Procesado del Lenguaje Natural\n",
    "\n",
    "### Aprendizaje Automático Intermedio e Ingeniería de Características\n",
    "\n",
    "#### Febrero 2021\n",
    "\n",
    "**Vanessa Gómez Verdejo, Emilio Parrado Hernández,  Pablo Martínez Olmos**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UZTTwuuIN0MH"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# Figures plotted inside the notebook\n",
    "%config InlineBackend.figure_format = 'svg'  \n",
    "# High quality figures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWvDrfYgN13e"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Hasta ahora hemos estado trabajando con datos de tipo  numérico o categórico. En esta sesión vamos a ver cómo trabajar con nuestros datos cuando éstos son cadenas de texto. A diferencia de los datos categóricos, en los que tenemos cadenas de texto asociadas a diferentes categorías y que podemos codificar fácilmente (por ejemplo, con un one-hot-encoding), cuando hablamos aquí de información textual nos referimos a frases, documentos y/o corpus de datos con estructura mucho más compleja. Idealmente, a partir de estos datos textuales tenemos que extraer la información necesaria (a poder ser incluyendo contenido semántico) y vectorizarla adecuadamente para poder utilizar o usar modelos de aprendizaje a partir de ella.\n",
    "\n",
    "En general, gran parte de la forma en que nos comunicamos hoy en día es a través de texto escrito, ya sea en servicios de mensajería, medios sociales y/o correo electrónico. Así, por ejemplo, en servicios/aplicaciones como TripAdvisor, Booking, Amazon, etc., los usuarios escriben reseñas de restaurantes/negocios, hoteles, productos para compartir sus opiniones sobre su experiencia. Estas reseñas, todas escritas en formato de texto, contienen una gran cantidad de información que sería útil responder preguntas relevantes para el negocio usando métodos de aprendizaje automático, por ejemplo, para predecir el mejor restaurante en una determinada zona. \n",
    "\n",
    "Este tipo de tarea (preprocesado) se denomina **procesamiento del lenguaje natural** (Natural Language Processing, NLP).\n",
    "El NLP es un subcampo de la lingüística, la informática y la inteligencia artificial que se ocupa de las interacciones entre los ordenadores (o procesadores) y el lenguaje humano; en particular engloba un conjunto de técnicas para permitir que los ordenadores procesen y analicen grandes cantidades de texto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3v4bBAVOM2z"
   },
   "source": [
    "# Pipeline para el procesado de texto \n",
    "\n",
    "Como sabemos, los algoritmos de ML procesan números, no palabras, por lo que necesitamos transformar el texto en números significativos que contengan la información relevante de los documentos. Este proceso de convesión de texto a números es lo que llamaremos **vectorización**. \n",
    "\n",
    "No obstante, para tener una representación útil, se requieren normalmente algunos pasos de **preprocesamiento** previo que limpien y homogenizen los documentos: tokenización, eliminación de *stop-words*, lematización, etc.\n",
    "La siguiente figura muestra los diferentes pasos que debemos seguir para procesar nuestros documentos hasta poder ser utilizados por nuestro modelo de aprendizaje:\n",
    "\n",
    "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/PipelineNLP.png\" width=\"80%\"> \n",
    "\n",
    "A lo largo de este notebook, veremos las herramientas que tenemos disponibles en Python para llevar a cabo todos estos pasos. Concretamente, nos centraremos en el uso de dos librerias:\n",
    "* [NLTK, Natural Language ToolKit](https://www.nltk.org/). Esta libreria es una excelente biblioteca de NLP escrita en Python por expertos tanto del mundo académico como de la industria. NLTK Permite crear aplicaciones con datos textuales rápidamente, ya que proporciona un conjunto de clases básicas para trabajar con corpus de datos, incluyendo colecciones de textos (corpus), listas de palabras clave, clases para representar y operar con datos de tipo texto (documentos, frases, palabras, ...) y funciones para realizar tareas comunes de NLP (conversión a token, conteo de palabras, ...). Por lo que va a ser de gran ayuda para el preprocesado de los documentos.\n",
    "* [Gensim](https://pypi.org/project/gensim/) es otra librería de Python para el modelado por temáticas (*topic modeling*), la indexación de documentos y tareas de recuperación de la información para documentos. Está diseñada para operar con grandes cantidades de información (con implementaciones eficientes y paralelizables/distribuidas) y nos va a ser de gran ayuda para la vectorización de nuestros corpus de datos una vez preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X22h_atA04h5"
   },
   "source": [
    "Empecemos este notebook cargando la librería NLTK y algunas de sus funcionalidades. A continuación, elegiremos un corpus de datos con el que empezar a analizar las funcionalidades básicas que aportan NLTK y Gensim y sobre el que veremos, uno por uno, en qué consisten los diferentes pasos de nuestro pipeline y cómo podemos implementarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zycI8RTyKLaR",
    "outputId": "85a2211c-dfae-4856-b61c-e2cd93f33bb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\e056118\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\e056118\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\e056118\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8JGdmgIKLaW"
   },
   "source": [
    "# 1. Cargando nuestro corpus de datos\n",
    "\n",
    "NLTK incluye diferentes corpus de datos  con los que podemos probar nuestras herramientas. Podemos encontrar información de todos ellos en [NLTK corpus](https://www.nltk.org/book/ch02.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFk2G2d36yzm"
   },
   "source": [
    "## El objeto CorpusReader\n",
    "Para empezar a trabajar vamos a usar el corpus **inaugural**, uno de los corpus de datos incluidos en NLTK y que consiste en 58 documentos de texto con los discursos presidenciales de los presidentes de EEUU.\n",
    "\n",
    "La siguiente celda de código nos muestra cómo cargar el corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5qadzt_ee6Q",
    "outputId": "07ff7084-02d7-43bf-d166-b4233c798ba4"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3Ze2_iK4dY7"
   },
   "source": [
    "Al cargar el corpus, se genera un objeto de tipo `CorpusReader`, llamado `inaugural`, con el contenido del mismo. Dado que un corpus es una colección de documentos/textos, podemos ver que documentos componen este corpus usando la función `.fileids()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7sqiCPK4eFM",
    "outputId": "572ae106-b21b-4101-fd87-05a22e392c43"
   },
   "outputs": [],
   "source": [
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JbA4shyedVq"
   },
   "source": [
    "Una característica de este corpus es que los documentos que lo forman tienen información sobre el año de cada documento. Podemos crear una lista de los años de cada discurso usando [list comprenhension](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gvntUQmhotH",
    "outputId": "32110ad3-f78c-4f97-cf4e-a930065e64d7"
   },
   "outputs": [],
   "source": [
    "years = [fileid[:4] for fileid in inaugural.fileids()]\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5cEbc1o6GGn"
   },
   "source": [
    "También podemos usar la función `.words()` de `inaugural` para acceder a las palabras que componen los documentos del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duI8edHeh7hd",
    "outputId": "7c6086a5-fffd-4688-ecdc-876bb3901238"
   },
   "outputs": [],
   "source": [
    "print(inaugural.words())\n",
    "print(len(inaugural.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEPerLiKLaX"
   },
   "source": [
    "Si queremos podemos acceder a un **documento** concreto del corpus y extraer su contenido en crudo con la función `.raw()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "oXSjzPneeiML",
    "outputId": "72c148db-f73d-4028-c060-c8c202b45a4b"
   },
   "outputs": [],
   "source": [
    "trump_text = inaugural.raw('2017-Trump.txt')\n",
    "trump_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIHDWtJnKLaY"
   },
   "source": [
    "La varaible `trump_text` que obtenemos es un único string (con todas las funciones de los tipos string) que contiene todas las palabras del documento que hemos especificado. Por ejemplo, podemos ver los primeros 20 caracteres de este documento como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhH8ijPXKLaY",
    "outputId": "02b0a43d-736e-4ee8-fef6-6f3d4c1f603d"
   },
   "outputs": [],
   "source": [
    "print(type(trump_text))\n",
    "\n",
    "print(trump_text[:20])\n",
    "\n",
    "print('\\n The total number of characters in the document is %d' %(len(trump_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOK0XTDKLaa"
   },
   "source": [
    "El CorpusReader también nos permite cargar documentos estructurados por **frases**. Para ello tenemos que usar la función `.sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFUZuuBbfdHm",
    "outputId": "2d8a7336-4007-4db0-e779-af80afdf4602"
   },
   "outputs": [],
   "source": [
    "trump_sents = inaugural.sents('2017-Trump.txt')\n",
    "print('\\n The total number of sentences in the document is %d' %(len(trump_sents)))\n",
    "print(trump_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG52tAxTiH-A"
   },
   "source": [
    "O, directamente, cargarlo a nivel de **palabras** (o **tokens**) usando el método `.words()`. Nótese que cuando hablamos de palabra o token no sólo son palabras con significado, sino que pueden ser números o signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eumGbIWLiJRK",
    "outputId": "b6223e01-92a2-46aa-8f71-2f02ec689ccd"
   },
   "outputs": [],
   "source": [
    "trump_words = inaugural.words('2017-Trump.txt')\n",
    "print('\\n The total number of words in the document is %d' %(len(trump_words)))\n",
    "print(trump_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xojODsSNb4G"
   },
   "source": [
    "# 2. Preprocesado del corpus\n",
    "\n",
    "Antes de transformar los datos de entrada de texto en una representación vectorial, necesitamos estructurar y limpiar el texto, y conservar toda la información que permita capturar el contenido semántico del corpus.\n",
    "\n",
    "Para ello, el procesado típico de NLP aplica los siguientes pasos:\n",
    "\n",
    "1. Tokenización\n",
    "2. Homogeneización\n",
    "3. Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBOayTbJNb4G"
   },
   "source": [
    "## 2.1. Tokenization\n",
    "\n",
    "**Tokenización** es el proceso de dividir el texto dado en piezas más pequeñas llamadas tokens. Las palabras, los números, los signos de puntuación y otros pueden ser considerados como tokens.\n",
    "\n",
    "Ya hemos visto que el objeto `CorpusReader` incluye funciones para dividir el corpus en frases o palabras. Pero NLTK incluye también funciones genéricas para hacer estas operaciones sobre cualquier cadena de texto. En concreto, tiene dos funciones:\n",
    "- `sent_tokenize`: es un tokenizador de frases. Este tokenizador divide un texto en una lista de oraciones. Para decidir dónde empieza o acaba una frase NLTK tiene un modelo pre-entrenado para el idioma específico en el que estemos trabajando. Este modelo lo hemos cargado al principio con `nltk.download('punkt')`.\n",
    "- `word_tokenize`/`wordpunct_tokenize`:  Divide un texto en palabras u otros caracteres individuales cómo pueden ser signos de puntuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbCGdSySgtXy"
   },
   "source": [
    "Veamos como funcionan estas funciones con el siguiente texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W73SiY9Wgwkh"
   },
   "outputs": [],
   "source": [
    "texto = 'I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NYRnSP1g4wN",
    "outputId": "d3dda7f4-ff6a-47c1-e033-fe73ee2f3651"
   },
   "outputs": [],
   "source": [
    "sent=nltk.sent_tokenize(texto)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDA-wzX9sGVc",
    "outputId": "b716aa85-3c26-42f7-ca00-941c48b7a0db"
   },
   "outputs": [],
   "source": [
    "sent_tokens1=nltk.wordpunct_tokenize(texto)\n",
    "print(sent_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp6mC8AtsOUP",
    "outputId": "5dded205-6b66-478e-accb-6fab560e754d"
   },
   "outputs": [],
   "source": [
    "sent_tokens2=nltk.word_tokenize(texto)\n",
    "print(sent_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW6OyNgfiwT_"
   },
   "source": [
    "Aunque puede parecer que las funciones `wordpunct_tokenize` y `word_tokenize` hacen lo mismo, con este ejemplo vemos que `wordpunct_tokenize` permite separar los signos de puntuación mientras que `word_tokenize` no.  Nótese la diferencia al dividir `I'm` entre ambas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE4YjZJFrsEk"
   },
   "source": [
    "También podemos combinar `sent_tokenize` y `word_tokenize` para tener frases y cada frase divida en tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PU96hUdSKLao",
    "outputId": "5234ae34-e142-462a-b8ac-ecd07485f589"
   },
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(texto):\n",
    "    print(nltk.wordpunct_tokenize(sent))\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPr7lOOQjYJe"
   },
   "source": [
    "#### Ejercicio 1: Tokenización del texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uckiOuc0fL_l"
   },
   "source": [
    "Seleccionemos ahora uno de los textos de nuestro corpus y veamos cómo aplicar estas funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YN3ElSgsKLao"
   },
   "outputs": [],
   "source": [
    "# Get a text\n",
    "trump_text = inaugural.raw('2017-Trump.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy08XX8Nl-wS"
   },
   "source": [
    "Complete la siguiente celda de código para dividir el texto en frases e imprima las 5 primeras frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pO6aEgfTjjDM",
    "outputId": "68089697-1fc6-4df2-9298-8f6f2acd9d40"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IGfAg7pmXgc"
   },
   "source": [
    "Complete las siguientes celdas de código para dividir el texto en tokens (considerando y sin considerar la puntuación) e imprima los primeros 5 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJv-YzJ8pMih",
    "outputId": "58859023-775e-4177-a569-41ccb432383d"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK8nwtcMfztY",
    "outputId": "e931d3fb-7b23-489d-88f3-4b4ab46a31a1"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T-OErnWNb4H"
   },
   "source": [
    "## 2.2. Homogeneización\n",
    "\n",
    "Al observar los tokens del corpus podemos ver que hay muchos tokens con algunas letras en mayúsculas y otras en minúsculas, el mismo token unas veces aparece en singular y otras en plural, o el mismo verbo que aparece en diferentes tiempos verbales. Para analizar semánticamente el texto, nos interesa  **homogeneizar** las palabras que formalmente son diferentes pero tienen el mismo significado.\n",
    "\n",
    "Para ello podemos usar las herramientas de lematización de NLTK. El proceso habitual de homogeneización consiste en los siguientes pasos:\n",
    "\n",
    "1. Eliminación de las mayúsculas y caracteres no alfanuméricos: de este modo los caracteres alfabéticos en mayúsculas se transformarán en sus correspondientes caracteres en minúsculas y  se eliminarán los caracteres no alfanuméricos, por ejemplo, los signos de puntuación.\n",
    "\n",
    "2. Stemming/Lematización: eliminar las terminaciones de las palabras para preservar su raíz de las palabras e ignorar la información gramatical (eliminamos marcas de plurales, género, conjugaciones verbales, ...).\n",
    "\n",
    "Veamos como ir aplicando uno a uno cada uno de estos pasos sobre el texto anterior una vez tokenizado por palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leDLNxMBoeRH",
    "outputId": "312e3fde-700a-476e-d9f7-e089206e281c"
   },
   "outputs": [],
   "source": [
    "# Get and tokenize the text\n",
    "trump_text = inaugural.raw('2017-Trump.txt')\n",
    "trump_tokens=nltk.wordpunct_tokenize(trump_text)\n",
    "print(trump_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SuawzwINb4H"
   },
   "source": [
    "#### **Ejercicio 2**: eliminación de mayúsculas y caracteres no alfanuméricos\n",
    "\n",
    "Convierte todos los tokens de `trump_tokens` a minúsculas (usando el método `.lower()`) y elimina los tokens no alfanuméricos (que puedes detectar con el método `.isalnum()`). Este procesado puedes codificarlo una sola línea de código usando list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5d22gbQuK7P",
    "outputId": "046f99d9-3375-40a7-8bdb-0318468a16e5"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ozm2RjKLap"
   },
   "source": [
    "**Stemming and Lemmatización**\n",
    "\n",
    "En el lenguaje común, las palabras pueden tomar diferentes formas indicando género, cantidad, tiempo (en el caso de los verbos), formas concretas para nombres/adjetivos o adverbios, ... Para muchas aplicaciones, es útil normalizar estas formas en alguna palabra canónica que facilite su análisis. Hay dos maneras de realizar este proceso:\n",
    "\n",
    "1. El proceso de **stemming** reduce las palabras a su base o raíz \n",
    "\n",
    "      running --> run\n",
    "\n",
    "      flowers --> flower\n",
    "\n",
    "  Para poder hacer esta transformación necesitamos librerías específicas que tienen almacenadas para el vocabulario de cada idioma las raices de dicho vocabulario y hacen esta conversión. En NLTK hay varios stemmers disponibles:\n",
    "  * Lancaster (inglés, es más reciente y bastante agresivo)\n",
    "  * Porter (inglés, es el stemmer original)\n",
    "  * Snowball  (incluye muchos idiomas y es el más nuevo)\n",
    "\n",
    "    \n",
    "2. El objetivo de la **lematización**, al igual que el stemmer, es reducir las formas inflexionales a una forma base común. A diferencia del steming, la lematización no se limita a cortar las inflexiones. En su lugar, utiliza bases de conocimiento léxico para obtener las formas básicas correctas de las palabras. \n",
    "\n",
    "    women   --> woman\n",
    "\n",
    "    foxes   --> fox\n",
    "    \n",
    "  La lematización en NLTK se basa en el léxico de [WordNet](https://wordnet.princeton.edu/). WordNet es un diccionario de inglés de orientación semántica, incluye el inglés WordNet con 155.287 palabras y 117.659 conjuntos de sinónimos. \n",
    "\n",
    "Veamos cómo funcionan el stemming y la lematización con un ejemplo:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKO2XsUjKLap",
    "outputId": "b6025a43-1f92-4527-e965-00562895e7fc"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "# Load several stemmers \n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTfrNKpWKLaq",
    "outputId": "261b6fb8-4ffc-4423-9c96-9f78b5c87815"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Try lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNQXBpS5nah7"
   },
   "source": [
    "Compara cómo los diferentes procesos transforman palabras como `women`, `running` o `computer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPcY6x5HsjzP"
   },
   "source": [
    "Una de las ventajas de la lematización es que el resultado sigue siendo una palabra, lo que es más aconsejable para la presentación de los resultados del procesado de textos.\n",
    "\n",
    "Sin embargo, sin utilizar información contextual, `lemmatize()` no elimina las diferencias gramaticales. Por esta razón, \"running\" o \"passed\" se conservan y no se sustituyen por el infinitivo \"run\" o \"pass\".\n",
    "\n",
    "Como alternativa, podemos aplicar `.lemmatize(word, pos)`, donde `pos` es un código de cadena que especifica función gramatical de las palabras en su oración. Por ejemplo, se puede comprobar la diferencia entre `wnl.lemmatize('running')` y `wnl.lemmatize('running', pos='v')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aVD_Ys1s9ru",
    "outputId": "50835579-28ee-4cc9-9f18-1e2ccf392be0"
   },
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('running'))\n",
    "print(lemmatizer.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKaflPOf-4N-"
   },
   "source": [
    "Notese que ninguno de los dos da una solución perfecta... al final este proceso requiere de supervisión manual que nos permita afinar esta homogeneización lo mejor posible. En ocasiones lo que se hace es que se aplica un primer lematizado y sobre un stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXCRMlFbEBVZ",
    "outputId": "f24773a1-b3ca-42f5-83d5-5e975153839f"
   },
   "outputs": [],
   "source": [
    "stemmed_text = [snowball.stem(t) for t in text]\n",
    "lemmas_text = [lemmatizer.lemmatize(t) for t in stemmed_text]\n",
    "print(lemmas_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v90trBdQ9cyY"
   },
   "source": [
    "#### **Ejercicio 3**: Stemming/Lematización\n",
    "\n",
    "Aplique el proceso de stemming y lematización sobre el texto del discurso inaugural de Trump resultantes del proceso de filtrado anterior (salida del Ejercicio 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms6f0RXL-a4A",
    "outputId": "bac29157-ef19-497b-84fc-08c756ae2008"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCd3LMgCH8Ra"
   },
   "source": [
    "## 2.3. Limpieza\n",
    "\n",
    "El último paso del preprocesado consiste en eliminar las palabras irrelevantes o **stop words** de los documentos. Las **stop words** son las palabras más comunes en un idioma como \"el\", \"a\", \"sobre\", \"es\", \"todo\". Estas palabras no tienen un significado importante y normalmente se eliminan de los textos. Para aplicar este proceso, se cargan librerías específicas que contienen este listado de palabras por cada idioma.\n",
    "\n",
    "Además, este paso suele utilizarse para eliminar las marcas de puntaución (',', '.', '?', ....), para lo que también tenemos que cargar otro módulo con los signos de puntuación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdCAXCR9ALKJ",
    "outputId": "74d057fa-3997-448c-b426-57852979f214"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDPi-iMFqFVF",
    "outputId": "6a84c4e0-fbfb-40a6-b293-c5b0d72d6f39"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4qC3yWvp1mo"
   },
   "source": [
    "\n",
    "Veamos como aplicarlo con un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W-y1Usvp2xa",
    "outputId": "2c0b72ff-bd35-4ea6-eba0-988aa3e66f41"
   },
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize('I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \"Hey\", she\\'ll say!')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsnBEDfEqeCj",
    "outputId": "fa7dbae9-c961-40d1-db34-bb3727756dcd"
   },
   "outputs": [],
   "source": [
    "clean_text1 = [token for token in text if (token not in stopwords_en)] \n",
    "print(clean_text1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSxNb9HRqdbO",
    "outputId": "7d0b7733-9f98-4719-d947-e1eb10fdc11b"
   },
   "outputs": [],
   "source": [
    "clean_text2 = [token for token in text if (token not in punctuation)] \n",
    "print(clean_text2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jyiGiS7Ajuf"
   },
   "source": [
    "#### **Ejercicio 4**: Eliminación de stop-words y puntuación\n",
    "\n",
    "Aplique conjuntamente el proceso de eliminación de stop-words y puntuación sobre el texto del discurso inaugural de Trump resultantes del proceso de filtrado y lematización anterior (salida Ejercicio 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_QYLhQ3BYOK",
    "outputId": "d3f21253-a6d7-4a43-f95c-c9474d6cb55c"
   },
   "outputs": [],
   "source": [
    "print('Texto filtrado y lematizado:')\n",
    "print(trump_tokens_stem)\n",
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIMy4KgqHFut"
   },
   "source": [
    "Nota: En el primer paso ya eliminamos las marcas de puntuación con la función es `.isalnum()`; por lo que no sería necesario eliminar de nuevo la puntuación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKx8y63AAis6"
   },
   "source": [
    "## 2.4 Pipeline de preprocesado o normalización del texto\n",
    "\n",
    "Por último, y para facilitar su uso posterior, vamos a juntar estos tres pasos en una única función que nos permita realizar la tokenización, homogeneización y limpieza con una única llamada a una función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG8F3-2Frhzq"
   },
   "source": [
    "#### **Ejercicio 5**: Función para la normalización de textos\n",
    "\n",
    "Complete el código de la siguiente función para poder hacer todos los pasos anteriores en una única función y luego pruebe a utilizarla sobre el texto del discurso inaugural de Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6BlL_PAKLaq"
   },
   "outputs": [],
   "source": [
    "## Load Modules\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "snowball = SnowballStemmer('english')\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        #<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3N0vvTVNIKf",
    "outputId": "0539a2da-d8c0-47c7-ac55-38a88d10cd3d"
   },
   "outputs": [],
   "source": [
    "print('Número de documentos en el corpus preprocesado:')\n",
    "print(len(corpus_prec))\n",
    "print('**********')\n",
    "print('Algunos de los elementos del primer documento preprocesado')\n",
    "print(corpus_prec[0][:20])\n",
    "print('**********')\n",
    "print('Algunos de los elementos del segundo documento preprocesado')\n",
    "print(corpus_prec[1][:20])\n",
    "print('**********')\n",
    "print('Algunos de los elementos del tercer documento preprocesado')\n",
    "print(corpus_prec[3][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUmd8ms2Nb4J"
   },
   "source": [
    "# 3. Vectorización\n",
    "\n",
    "Hasta este punto, hemos transformado la colección de textos en bruto en una lista de textos, en la que cada texto es una colección de las raíces de las palabras más relevantes para el análisis semántico. Ahora, necesitamos convertir estos datos (una lista de listas de tokens) en una representación numérica (una lista de vectores, o una matriz). \n",
    "\n",
    "Antes de pasar a hacer esta vectorización, documento a documento, vamos a hacer un **análisis frecuencial** del contenido del corpus preprocesado. Para ello vamos a obtener:\n",
    "- Un recuento de palabras: número de veces que aparece cada palabra en el corpus.\n",
    "- El vocabulario: conjunto de palabras únicas dentro del corpus.\n",
    "- La diversidad léxica: la relación entre el número de palabras y el vocabulario.\n",
    "\n",
    "Y para ello vamos a usar la dos clases muy útiles de NLTK que nos permiten hacer estos análisis de frecuencia:\n",
    "\n",
    "- `FreqDist`\n",
    "- `ConditionalFreqDist` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dhfTeTtKLai"
   },
   "source": [
    "## 3.1 Análisis de frecuencias del corpus\n",
    "\n",
    "Para empezar a hacer este análisis, vamos a convertir nuestra lista de documentos, donde cada documento tiene una lista de tokens, en una única lista con todos los tokens del corpus. Una vez hecha esta conversión, utilizamos `FreqDist` y algunos de sus métodos para analizar frecuencialmente el contenido del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB_PeBSnKLai"
   },
   "outputs": [],
   "source": [
    "tokens_corpus = [token for doc in corpus_prec for token in doc]\n",
    "counts  = nltk.FreqDist(tokens_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tQQr-aSKLai"
   },
   "source": [
    "Podemos comprobar que `counts` es un diccionario que contiene el número de veces que cada palabra aparece en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEF4pjjLKLai",
    "outputId": "9e26af6b-9463-4cac-a310-8b2646822840"
   },
   "outputs": [],
   "source": [
    "counts # counts is a FreqDist object, a dictionary data type with additional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ou-1tNAyKLai",
    "outputId": "5081b9ca-6c9b-45e7-c3d4-df32a2328d64"
   },
   "outputs": [],
   "source": [
    "counts['citizen']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThfnxpjlKLai"
   },
   "source": [
    "Simplemente operando sobre `counts` podemos calcular fácilmente el tamaño del vocabulario y la diversidad léxica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXNqNSXTKLaj",
    "outputId": "1fc11f0e-edd5-42c3-9b26-f88049fc885f"
   },
   "outputs": [],
   "source": [
    "vocab   = len(counts.keys()) \n",
    "words   = sum(counts.values())\n",
    "lexdiv  = float(words) / float(vocab)\n",
    "\n",
    "print(\"El corpus tiene %i palabras únicas y un total de %i palabras con una diversidad léxica de %0.3f\" % (vocab, words, lexdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRAg-kG4KLak"
   },
   "source": [
    "Con el método  `most_common(n)` obtenemos una lista de las palabras más comunes. Haciendo `counts.plot(n,cumulative=True)` podemos tener una idea de cuánto dominan las palabras más comunes en el corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lqFOCZ1KLak",
    "outputId": "33680450-c9b9-4134-dda4-a364b186babe"
   },
   "outputs": [],
   "source": [
    "counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "Ez_aDeoZKLak",
    "outputId": "95c8f793-79c9-4ac7-a862-dbcbcd15ec2f"
   },
   "outputs": [],
   "source": [
    "counts.plot(40, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Bn4vDgL7IL"
   },
   "source": [
    "Otra forma de analizar la frecuencia de aparición de las palabras en el corpus es dibujando una nube llena de muchas palabras de diferentes tamaños, que representan la frecuencia o la importancia de cada palabra. Esto es lo que se conoce como **word cloud** o **nube de palabras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "L7LetRlNKeMb",
    "outputId": "4e006466-d62a-4f81-a604-f1d176523f78"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=40, background_color=\"white\").generate(' '.join(tokens_corpus))\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su_mnHqTegXo"
   },
   "source": [
    "## 3.2 Bag of Words (BOW)\n",
    "\n",
    "Si este análisis de frecuencia lo hicieramos a nivel de documento, en vez de a nivel de corpus, nos estaría indicando la ocurrencia de cada palabra por documento. De hecho, este conteo de palabras por documento es lo que se conoce como **bag-of-words** (bolsa de palabras) o, de manera abreviada como , **BoW**. En concreto, **bag-of-words** es una representación de texto que describe la ocurrencia de palabras dentro de un documento. \n",
    "\n",
    "Se llama \"bolsa\" de palabras, porque cualquier información sobre el orden o la estructura de las palabras en el documento es descartada. El modelo sólo se ocupa de si las palabras conocidas aparecen en el documento, no de dónde aparecen en el documento.\n",
    "\n",
    "Para obtener la representación BoW podemos utilizar varias librerías. `Sklearn` incluye funcionalidades para el análisis de texto, y las propias funcionalidades de NLTK para el conteo de palabras o análisis frecuencial de palabras podrían valernos. Sin embargo, en esta sección vamos a ver cómo usar la librería de [`gensim`](https://pypi.org/project/gensim/) diseñada específicamente para el procesado de texto, permitiendo un procesado muy eficiente para grandes corpus e incluyendo funcionalides adicionales de gran utilidad. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeqIS57CM1j1"
   },
   "source": [
    "El proceso de generación del BoW tiene dos pasos:\n",
    "1. Generación del vocabulario del corpus. Este vocabulario se genera mediante un **diccionario** que almacena de manera ordenada un vocabulario de palabras conocidas (a nivel de corpus).\n",
    "2. **Vectorización de documentos** o generación del BoW. Se calcula una medida de la presencia de palabras conocidas (las que están en el vocabulario); para ello, realiza  un conteo del número de veces que cada palabra del diccionario aparece dentro de cada documento.\n",
    "\n",
    "Veamos a continuación, como implementar estos pasos: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH6M2ORpPjUf"
   },
   "source": [
    "### Generación del diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfBctuyENb4J",
    "outputId": "27243e3d-c4a7-4aad-ed0a-b63fc9d28e7f"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Create dictionary of tokens: the input is the preprocessed corpus \n",
    "D = gensim.corpora.Dictionary(corpus_prec)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUDcbY1LYGVz"
   },
   "source": [
    "Como vemos el diccionario no es más que una lista de palabras. Pero el orden de esta lista será muy importante para la vectorización de cada documento, ya que generaremos tuplas de (`id`, `conteo`) y los `id` serán las posiciones de estas palabras en el diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdazZ16qNb4K"
   },
   "source": [
    "### Gestión del Vocabulario\n",
    "\n",
    "A medida que el tamaño del vocabulario aumenta, también lo hace la representación vectorial de los documentos. En el ejemplo anterior, la longitud del vector de los documentos es igual al número de palabras conocidas.\n",
    "\n",
    "Para un corpus muy grande, como miles de documentos, la longitud del vector que representará cada documento puede ser de miles o millones de posiciones. \n",
    "\n",
    "Además, cada documento puede contener muy pocas de las palabras conocidas en el vocabulario, lo que dificulta el procesado. \n",
    "\n",
    "Para evitar esto, es recomendable analizar el diccionario y eliminar tokens que no resulten relevantes: marcas de puntuación que todavía queden o términos que aparecen en muy pocos casos (por lo que no son términos poco informativos) o, incluso, tokens que figuran en todos los documentos del corpus (y no son discriminativos). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBWSTmNYUHO3"
   },
   "source": [
    "El diccionario de Gensim nos permite filtrar estos terminos fácilmente:\n",
    "\n",
    "* `.filter_tokens()` nos permite indicar con el parámetro `bad_ids` la lista de ids de las palabras a eliminar o con `good_ids` la lista de los ids de las palabras a dejar en el diccionario (eliminando el resto).\n",
    "\n",
    "* `.filter_extremes()` permite eliminar palabras/tokens poco o muy frecuentes indicando con los parámetros:\n",
    "  * `no_below`: se queda con el número de tokens que están contenidas en al menos `no_below` documentos.\n",
    "  * `no_above`:  se queda con el porcentaje  (fracción del tamaño total del corpus, no un número absoluto) de tokens que no están en más de `no_above` documentos.\n",
    "  * `keep_n`: directamente se queda con los `keep_n` tokens más frecuentes.\n",
    "  * `keep_tokens`: listado de tokens que deben permanecer en el diccionario después de ser filtradas.\n",
    "\n",
    "Veamos como utilizar estas dos funcionalidades..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrIZU0YTTvVt",
    "outputId": "85adaa72-1a44-4c1e-b0cd-cbd8c524e758"
   },
   "outputs": [],
   "source": [
    "D.filter_tokens(bad_ids=[0])  # quitamos del diccionario el término \"14th\"\n",
    "n_tokens = len(D)\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hpUv3RrNb4L",
    "outputId": "6a408cdf-fbbc-457c-8352-b540e13685f9"
   },
   "outputs": [],
   "source": [
    "no_below = 5 #Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .75 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "\n",
    "D.filter_extremes(no_below=no_below,no_above=no_above, keep_n=1500)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')\n",
    "\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWpsHENXWufn"
   },
   "source": [
    "Esto elimina todas los tokens en el diccionario que son:\n",
    "1. Los `no_below=5` tokens menos frecuentes  \n",
    "2. El `no_above=0.75` ($75\\%$) de tokens más frecuentes.\n",
    "3. Después de (1) y (2), guarda sólo los primeros `keep_n` tokens más frecuentes (o guarda todos si `keep_n=None`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFfkJlaOu1-Q"
   },
   "source": [
    "#### **Ejercicio 7**: Refinando el diccionario\n",
    "A continuación, vamos a volver a generar el diccionario de nuestro corpus y analizar la aparición de palabras por documento. A partir de este análisis decida qué palabras eliminaría del diccionario y use las funciones anteriores para refinar el diccionario.\n",
    "\n",
    "Para este análisis usamos algunos de los métodos de la clase diccionario de gensim como `dfs` (calcula en cuántos documentos del corpus aparece cada palabra del vocabulario). Puede ver la ayuda en este [link](https://radimrehurek.com/gensim/corpora/dictionary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tP_YOOwwNGk",
    "outputId": "e1b819bb-8aa2-4462-bae1-aa00f7f55804"
   },
   "outputs": [],
   "source": [
    "# Recompute the dictionary\n",
    "D = gensim.corpora.Dictionary(corpus_prec)\n",
    "n_tokens = len(D)\n",
    "print(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "KySFQCkQxtE1",
    "outputId": "0a0a2ad4-c478-4c2a-e654-5f27c1899906"
   },
   "outputs": [],
   "source": [
    "# Analize the word frequency per document \n",
    "plt.hist(D.dfs.values(),58)\n",
    "plt.show()\n",
    "# we have 58 documents, some words are in all of them -> no_above= 0.8% \n",
    "# There are many words in a single document -> no_below = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs70mrV50Uaw"
   },
   "outputs": [],
   "source": [
    "# Incluya aqui su código para limpiar el diccionario\n",
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJxAXOGgNb4L",
    "outputId": "3965773e-9d30-46f1-a1a7-cfcc2b24ec0c"
   },
   "outputs": [],
   "source": [
    "n_doc=50\n",
    "print('Original document (after cleaning):')\n",
    "print(corpus_prec[n_doc])\n",
    "print('Sparse vector representation (first 10 components):')\n",
    "print(corpus_bow[n_doc][:10])\n",
    "print('Word counts for the document (first 10 components):')\n",
    "list_word_counts = [(D[doc_bow[0]], doc_bow[1]) for doc_bow in corpus_bow[n_doc][:10]]\n",
    "print(list_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "w8AChxOQPylP",
    "outputId": "a94d15fd-88e6-4754-b467-f20c372daf7a"
   },
   "outputs": [],
   "source": [
    "(words, counts) = zip(*list_word_counts)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(words,counts, use_line_collection = True)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygjdET3RPQ-m"
   },
   "source": [
    "## 3.3 Representación TF-IDF \n",
    "\n",
    "Un problema del BoW es que la frecuencia de las palabras muy frecuentes empiezan a dominar en el documento respecto al resto; por ejemplo, verbos muy comunes o términos habituales en el contexto del corpus pero no tienen tanto \"contenido informativo\" para el modelo como pueden ser palabras más raras pero tal vez específicas del dominio. Al realizar la gestión del vocabulario hemos visto que podemos eliminar estas palabras tan frecuentes, pero este proceso es bastante manual y es preferible tener una vectorización robusta a esto.\n",
    "\n",
    "Para ello, la representación TF-IDF (Term Frequency–Inverse Document Frequency) propone reajustar la frecuencia de las palabras en función de la frecuencia con que aparecen en todos los documentos, de modo que se penalicen las puntuaciones de palabras frecuentes si también son frecuentes en todos los documentos. Para ello, el TF-IDF implica el cálculo de dos valores:\n",
    "\n",
    "\n",
    "**Frecuencia de término (TF)**\n",
    "\n",
    "Por frecuencia de término $\\text{TF}(w)$ nos referimos al número de veces que una palabra $w$ dada ocurre en un documento, dividido por el número total de palabras en dicho documento.\n",
    "$$ \\text{TF}(w,d) =\\frac{\\text{# veces que $w$ aparece en el documento $d$}}{\\text{# total de palabras en el documento $d$}}$$\n",
    "\n",
    "**Frecuencia de Documento Inversa (IDF)**\n",
    "\n",
    "Es una medida de cuánta información proporciona la palabra $w$, es decir, si es común o rara en todos los documentos del corpus $D$. Se calcula de la siguiente manera:\n",
    "$$ \\text{IDF}(w,D) =\\log \\frac{\\text{# palabras en el corpus}}{1+\\text{# documentos donde la palabra $w$ aparece}}$$\n",
    " \n",
    "A partir de estos valores el **TF-IDF** se calcula de la siguiente manera:\n",
    "\n",
    "$$\\text{TF-IDF}(w,d,D) = \\text{TF}(w,d) * \\text{IDF}(w,D)$$\n",
    "\n",
    "Un peso alto en TF-IDF se consigue cuando la palabra tiene una frecuencia alta en el documento y, a la vez, una frecuencia baja en el corpus; por lo tanto, los pesos tienden a filtrar los términos que son comunes a muchos documentos. \n",
    "\n",
    "Obsérvese que, a diferencia de la codificación BoW, para el TF-IDF tenemos que parender la codificación conjuntamente con todo el corpus. No obstante, una vez que hemos calculado el BoW para todos los documentos, aprender el modelo TF-IDF es sencillo usando la función [TfidfModel](https://radimrehurek.com/gensim/models/tfidfmodel.html) de Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KacBPzeawCw",
    "outputId": "22f442f2-cea8-45a0-b942-5f5636ee65ae"
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# fit TFIDF model for all the corpus\n",
    "model = TfidfModel(corpus_bow)  \n",
    "\n",
    "# apply model to the first corpus document\n",
    "vector = model[corpus_bow[0]]  # apply model to the first corpus document\n",
    "print(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qCGLB9r6bKJ"
   },
   "source": [
    "#### **Ejercicio 8**: Obtenga el TFIDF de todos los documentos del corpus\n",
    "\n",
    "Almacene la salida en una lista llamada `corpus_tfifd` donde cada elemento de la lista sea el TFIDF de un documento, para a continuación poder analizar la salida y representarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKYbo-AU6a_j"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6i-3FWkbUY4"
   },
   "source": [
    "Analicemos esta transformación en comparación con el BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMMsxdtGbJr6",
    "outputId": "eb21a746-9d81-47bb-f1c9-39a90f1b0ffc"
   },
   "outputs": [],
   "source": [
    "n_doc=50\n",
    "print('Original document (after cleaning):')\n",
    "print(corpus_prec[n_doc])\n",
    "print('Sparse TFIDF vector representation (first 10 components):')\n",
    "print(corpus_tfidf[n_doc][:10])\n",
    "print('Word counts for the document (first 10 components):')\n",
    "list_word_counts = [(D[doc_bow[0]], doc_bow[1]) for doc_bow in corpus_bow[n_doc][:10]]\n",
    "print(list_word_counts)\n",
    "print('TF-IDF for the document (first 10 components):')\n",
    "list_tfidf = [(D[doc_tfidf[0]], doc_tfidf[1]) for doc_tfidf in corpus_tfidf[n_doc][:10]]\n",
    "print(list_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "S1mcxp-hcy6G",
    "outputId": "4f5c7ecc-2ba7-4028-a04e-4c279f48893a"
   },
   "outputs": [],
   "source": [
    "#Plot BoW\n",
    "(words, counts) = zip(*list_word_counts)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(words,counts, use_line_collection = True)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "ESHTVrHxc1yk",
    "outputId": "bbecf8fc-2550-4096-828d-c8903343588f"
   },
   "outputs": [],
   "source": [
    "#Plot Tf-IDF\n",
    "(words, counts) = zip(*list_tfidf)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(words,counts, use_line_collection = True)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGgPmALDeZ2w"
   },
   "source": [
    "Compare ambas representaciones... ¿Qué palabras tenían más/menos peso en la representación BoW? ¿Y en la representación TF-IDF? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgUmbh18SevB"
   },
   "source": [
    "## Limitaciones de BoW y TF-IDF\n",
    "\n",
    "Las representaciones vectoriales que acabamos de ver son sencillas de comprender y aplicar y ofrecen mucha flexibilidad para manejo de información textual. De hecho, se han utilizado con gran éxito en problemas de predicción y clasificación de documentos.\n",
    "\n",
    "Sin embargo, sufre de algunas deficiencias que tenemos que tener en cuenta:\n",
    "* El vocabulario: El vocabulario requiere un diseño cuidadoso, más específicamente para gestionar su tamaño, lo que afecta a la dispersión de las representaciones de los documentos.\n",
    "* La dispersión: Las representaciones dispersas son más difíciles de modelar tanto por razones computacionales como por razones de información, en las que el reto es que los modelos aprovechen la poca información común en un espacio de representación tan grande.\n",
    "* Significado: Al descartar el orden de las palabras se ignora el contexto y, a su vez, el significado de las palabras del documento (semántica). El contexto y el significado pueden ofrecer mucho al modelo, que si se modela podría diferenciar entre las mismas palabras dispuestas de manera diferente (\"esto es interesante\" vs \"es esto interesante?\"), sinónimos (\"bicicleta vieja\" vs \"bicicleta usada\"), y mucho más.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QJG_FyAe3L8"
   },
   "source": [
    "# 4. Utilizando la representación vectorial en modelos de aprendizaje\n",
    "\n",
    "Una vez tenemos una representación vetorial para cada documento del corpus podemos usar los modelos de aprendizaje que conocemos para resolver diferentes tareas. Por ejemplo, si este corpus estuviese distribuido por categorías, podríamos aprender un modelo para clasificar automáticamente los textos en las diferentes categorías. Como el corpus con el que estamos trabajando no está etiquetado, vamos a usar la representación vectorial que hemos obtenido para hacer un agrupamiento de los documentos usando un K-means y luego analizar qué información nos da este agrupamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm_Llf0bjAxq"
   },
   "source": [
    "## 4.1 Codificación de los datos\n",
    "\n",
    "Primero de todo, para poder usar las librerías de sklearn, tenemos que convertir nuestra representación vectorial en numpy arrays. Para ello, gensim nos incluye dos funciones: [corpus2dense](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2dense.html), [corpus2csc](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2csc.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YngbiGdNg8sw"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "\n",
    "n_tokens = len(D)\n",
    "num_docs = len(corpus_bow)\n",
    "# Convert BoW representacion\n",
    "corpus_bow_dense = corpus2dense(corpus_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
    "corpus_bow_sparse = corpus2csc(corpus_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
    "# Convert TFIDF representacion\n",
    "corpus_tfidf_dense = corpus2dense(corpus_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n",
    "corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPlWj4QRlUwS"
   },
   "source": [
    "Veamos qué han hecho estas transformaciones, por ejemplo, para la representación BoW: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDvsDI0hiTEx",
    "outputId": "e7bd3f6c-b403-40e7-e7e6-d7a5fac3ce98"
   },
   "outputs": [],
   "source": [
    "print(corpus_bow[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXQzeE7IhEqt",
    "outputId": "8d9c9579-9d11-4b0b-99c4-712443daaf17"
   },
   "outputs": [],
   "source": [
    "print(corpus_bow_dense[50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JTqEb-Gi2Rf",
    "outputId": "5e3aa89e-537a-4294-85e1-e0e5f4271c0a"
   },
   "outputs": [],
   "source": [
    "print(corpus_bow_sparse[50,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1NscM9Mk_cY"
   },
   "source": [
    "En general, cuando tengamos grandes corpus de datos, nos interesará manejar la representación dispersa o sparse de los datos para ahorrar en coste computacional. Si en nuestro procesado de ML usamos sklearn podremos trabajar con este formato sparse ya que la mayoría de clasificadores/regresores puede trabajar con ambos datos densos y dispersos.\n",
    "\n",
    "Pasemos ahora a aplicar un agrupamiento o clustering de nuestros documentos usando ambas representaciones vectoriales (BoW y TFIDF). En ambos casos usaremos la representación sparse de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRqFJx2jHQs0"
   },
   "source": [
    "## 4.2 Calculo de distancias entre representaciones BoW y TF-IDF\n",
    "\n",
    "Una vez que tenemos la representación vectorial de nuestros documentos (cada una de las filas de la matriz BoW o tfidf), muchos de nuestros modelos de ML necesitarán calcular similitudes entre ellos, como puede ser un algoritmo K-NN para regresión o un K-means para agrupamiento. Para ello, cuando trabajamos  con características BoW o TFIDF, donde la magnitud de los vectores no importa, suele utilizarse como métrica la **similitud del coseno**. \n",
    "\n",
    "Podríamos suponer que cuando una palabra (por ejemplo, ciencia) aparece con más frecuencia en el documento 1 que en el 2, ese documento 1 está más relacionado con el tema de la ciencia. Sin embargo, también podría darse el caso de que estemos trabajando con documentos de longitudes desiguales (artículos de Wikipedia, por ejemplo). En ese caso, es probable que la ciencia aparezca más en el documento 1 sólo porque es mucho más largo que el documento 2. La similitud del coseno corrige esto.\n",
    "\n",
    "Por esta razón, cuando se trabaja con documentos codificados con BoW o TF-IDF, se tiende a utilizar la similitud del coseno. Si $\\mathbf{v}_1$ y $\\mathbf{v}_2$ son dos vectores TF-IDF, la similitud del coseno se calcula como sigue:\n",
    "\n",
    "\n",
    "$$\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2) = \\frac{\\mathbf{v}_1^T~\\mathbf{v}_2}{||\\mathbf{v}_1||_2 ||\\mathbf{v}_2||}$$\n",
    "\n",
    "O, más bien se usa la distancia coseno definda como $1-\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2) $, es decir,\n",
    "$$d_\\text{cos}(\\mathbf{v}_1,\\mathbf{v}_2) = 1- \\frac{\\mathbf{v}_1^T~\\mathbf{v}_2}{||\\mathbf{v}_1||_2 ||\\mathbf{v}_2||}$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/NLP1.png\" width=\"100%\"> \n",
    "\n",
    "Y como podemos ver en la siguiente figura, sus resultados no tienen nada que ver con la distancia euclídea (no dependen de la longitud de los vectores)\n",
    "\n",
    "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/NLP/NLP2.png\" width=\"60%\"> \n",
    "\n",
    "\n",
    "Otro aspecto que tenemos que tener en cuenta es que la distancia coseno no es una distancia como tal (no cumple la desigualdad triangular) e implementaciones por defecto de nuestros modelos de aprendizaje como el K-NN o K-means de sklearn usan la distancia euclídea por defecto y no permiten incluir la distancia coseno. \n",
    "\n",
    "Para solventar este problema tenemos dos opciones:\n",
    "* Usar otras implementaciones como K-means de NLTK que nos permite usar la distancia coseno. Aunque el problema de esta implementación es que no nos deja trabajar con matrices sparse. \n",
    "* Reescalar nuestros datos para poder utilizar implementaciones basadas en la distancia euclídea... (luego explicaremos en qué consiste esta aproximación).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64pjt0btUgh4"
   },
   "source": [
    "## 4.3 ¿Normalización de los datos?\n",
    "\n",
    "Hasta ahora siempre hemos visto en nuestro pipeline de ML que debemos normalizar los datos por columnas (por características) antes de pasarlos al modelo de aprendizaje.\n",
    "\n",
    "Pero trabajando con representaciones vectoriales de texto como BoW o TF-IDF, ¿necesitamos normalizar? Aunque no hay una respuesta cerrada, la idea más aceptada es que las representaciones BoW o TF-IDF u otras transformaciones NLP que veremos más adelante deberían dejarse tal y como están para obtener mejores resultados.\n",
    "\n",
    "Esto se debe a que la normalización de las variables BoW no se considera algo natural, perdemos la importancia de cada palabra dentro del documento. Mientras que la codificación TF-IDF ya se considera que está normalizada; nótese que:\n",
    "* El cálculo del TF normaliza cada documento a longitud 1 (normalización por filas), eliminado la dependencia con documentos más largos o más cortos. \n",
    "*  En segundo lugar, el IDF es una normalización entre documentos que da menos peso a los términos comunes y más a los raros (normalizacióón por columnas), normalizando (ponderando) cada palabra con la frecuencia inversa del corpus.\n",
    "\n",
    "Por lo que el TF-IDF está pensado para ser utilizado en su forma directa en un algoritmo. \n",
    "\n",
    "No obstante, tal y como veremos, algoritmos como el K-means que suelen ser muy sensibles al escalado de características la ponderación IDF ayuda a mejorar el resultado del agrupamiento y, por tanto, suelen obtenerse resultados mejores con representaciones TF-IDF que con BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95gH-uEGhB0h"
   },
   "source": [
    "## 4.4 K-means con NLTK\n",
    "\n",
    "Empecemos haciendo agrupamiento de datos usando la librería de NLTK que nos permite usar la distancia coseno. \n",
    "\n",
    "Aunque, tal y como hemos comentado, el problema de esta implementación es que no nos deja trabajar con matrices sparse; así que tenemos que usar el método `.toarray()` para convertir nuestros vectores sparse en numpy arrays (o usar la conversión a matrices densas de gensim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MGcrwJrKTKz"
   },
   "outputs": [],
   "source": [
    "# Transform data to dense\n",
    "corpus_bow_array = corpus_bow_sparse.toarray()\n",
    "corpus_tfidf_array = corpus_tfidf_sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_cgxl_1hmCQ"
   },
   "source": [
    "Ahora entrenaremos el K-means con 4 centroides, tanto para la representación BoW como TF-IDF y analizaremos los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDupiI8Ch_zM"
   },
   "source": [
    "**Clustering de documentos con BoW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYZsbestI3MX"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "K=4\n",
    "kclusterer = KMeansClusterer(4, distance=nltk.cluster.util.cosine_distance, repeats=10)\n",
    "\n",
    "y_kmeans = kclusterer.cluster(corpus_bow_array, assign_clusters=True)\n",
    "centers = kclusterer.means()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNZ6WY3HmGXl"
   },
   "source": [
    "¿Qué documentos están en cada grupo? ¿Qué información (textual) hay en cada grupo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OghwJN9zlbur",
    "outputId": "b9b550b2-e40f-460b-d01d-0eaadcf34b53"
   },
   "outputs": [],
   "source": [
    "# Palabras más relevantes por centroide\n",
    "n_center = 0\n",
    "fileids = inaugural.fileids()\n",
    "y_kmeans = np.array(y_kmeans)\n",
    "for i, center_i in enumerate(centers):\n",
    "  print('Centroide %d' %(i))\n",
    "  # Find documents in this centroid\n",
    "  pos_y = np.where(y_kmeans==i)[0]\n",
    "  print('Elementos grupo %d' %(i))\n",
    "  id_grupo = [fileids[pos] for pos in pos_y]\n",
    "  print(id_grupo)\n",
    "\n",
    "  # Find the ten most relevant words for this centroid\n",
    "  pos_sort = np.argsort(centers[i])[::-1][:10]\n",
    "  words_center = [D[p] for p in pos_sort]\n",
    "  print(words_center)\n",
    "  print('********')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HODxKfMPqlX5"
   },
   "source": [
    "**Clustering de documentos con TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4VM2jZHL0La"
   },
   "outputs": [],
   "source": [
    "K=4\n",
    "kclusterer = KMeansClusterer(4, distance=nltk.cluster.util.cosine_distance, repeats=10)\n",
    "\n",
    "y_kmeans = kclusterer.cluster(corpus_tfidf_array, assign_clusters=True)\n",
    "centers = kclusterer.means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4jwiMfdqyl8",
    "outputId": "bccc852f-e74f-4684-f012-218ecbb8b731"
   },
   "outputs": [],
   "source": [
    "# Palabras más relevantes por centroide\n",
    "n_center = 0\n",
    "fileids = inaugural.fileids()\n",
    "y_kmeans = np.array(y_kmeans)\n",
    "for i, center_i in enumerate(centers):\n",
    "  print('Centroide %d' %(i))\n",
    "  # Find documents in this centroid\n",
    "  pos_y = np.where(y_kmeans==i)[0]\n",
    "  print('Elementos grupo %d' %(i))\n",
    "  id_grupo = [fileids[pos] for pos in pos_y]\n",
    "  print(id_grupo)\n",
    "\n",
    "  # Find the ten most relevant words for this centroid\n",
    "  pos_sort = np.argsort(centers[i])[::-1][:10]\n",
    "  words_center = [D[p] for p in pos_sort]\n",
    "  print(words_center)\n",
    "  print('********')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jgSEESzrFEJ"
   },
   "source": [
    "Analice y discuta los resultados de ambos clusters para cada tipo de vectorización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYQwYUbjh37h"
   },
   "source": [
    "## 4.5 K-means con sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KvTY0e4ilEK"
   },
   "source": [
    "Como hemos dicho anteriormente, no podemos usar las librerías de K-means o K-NN de sklearn con represntaciones BOW o TF-IDF porque solo permiten usar la distancia euclídea. \n",
    "\n",
    "Sin embargo, cuando nuestros vectores $x$ e $y$ están normalizados ($||x||^2 = x^Tx = 1$), sus distancias euclídeas $||x-y||^2$  y coseno $d(x,y)$ están relacionas por la siguiente igualdad:\n",
    "\n",
    "$$||x-y||^2 = x^Tx + y^Ty - 2 x^Ty = 2  (1 - x^Ty) = 2  d(x,y)$$\n",
    "\n",
    " En este caso, usar la distancia euclídea nos va a dar los mismos resultados que la distancia coseno.\n",
    "\n",
    "Nótese que esta normalización es parecida al cálculo TF que compensa en la representación BoW la longitud de los documentos. \n",
    "\n",
    "Así que comencemos normalizando nuestros datos para que cada vector tenga norma unidad (vamos a hacerlo sobre las reprentaciones sparse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sATYDcPiLPj"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def normalize_sparse_vector(s):\n",
    "  norm_s = np.array(np.sqrt(s.multiply(s).sum(1)))\n",
    "  pos_zero = np.where(np.sqrt(s.multiply(s).sum(1))==0)[0]\n",
    "  norm_s[pos_zero] = 1\n",
    "  return s.multiply(sparse.csr_matrix(1/norm_s))\n",
    "\n",
    "corpus_bow_sparse_norm = normalize_sparse_vector(corpus_bow_sparse)\n",
    "corpus_tfidf_sparse_norm = normalize_sparse_vector(corpus_tfidf_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFELzI9Hz2zs"
   },
   "source": [
    "Ahora claculemos el clustering con el K-means de sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBsqS5cg6z_R"
   },
   "source": [
    "**Clustering de documentos con BoW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuJZ4ZjpPQ-r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# K-means with K=4 (we could use the silhouette score to adjust this parameter) and 10 initializations\n",
    "K=4\n",
    "kmeans = KMeans(n_clusters=K, n_init=10) # Definimos objeto con parámetros por defecto\n",
    "\n",
    "kmeans.fit(corpus_bow_sparse_norm) # Entrenamos k-means usando el BoW\n",
    "y_kmeans = kmeans.predict(corpus_bow_sparse_norm) # Obtenemos el identificador del grupo para cada dato\n",
    "centers = kmeans.cluster_centers_ # Obtenemos los centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNhtGYPRz-JG",
    "outputId": "b2556f64-3576-4870-9b71-6c8f5b63dc10"
   },
   "outputs": [],
   "source": [
    "# Palabras más relevantes por centroide\n",
    "n_center = 0\n",
    "fileids = inaugural.fileids()\n",
    "y_kmeans = np.array(y_kmeans)\n",
    "for i, center_i in enumerate(centers):\n",
    "  print('Centroide %d' %(i))\n",
    "  # Find documents in this centroid\n",
    "  pos_y = np.where(y_kmeans==i)[0]\n",
    "  print('Elementos grupo %d' %(i))\n",
    "  id_grupo = [fileids[pos] for pos in pos_y]\n",
    "  print(id_grupo)\n",
    "\n",
    "  # Find the ten most relevant words for this centroid\n",
    "  pos_sort = np.argsort(centers[i])[::-1][:10]\n",
    "  words_center = [D[p] for p in pos_sort]\n",
    "  print(words_center)\n",
    "  print('********')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ8i80s_61iR"
   },
   "source": [
    "**Clustering de documentos con TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVWYNh3Wqomm"
   },
   "outputs": [],
   "source": [
    "# K-means with K=4 (we could use the silhouette score to adjust this parameter) and 10 initializations\n",
    "K=4\n",
    "kmeans = KMeans(n_clusters=K, n_init=10) # Definimos objeto con parámetros por defecto\n",
    "\n",
    "kmeans.fit(corpus_tfidf_sparse_norm) # Entrenamos k-means usando el BoW\n",
    "y_kmeans = kmeans.predict(corpus_tfidf_sparse_norm) # Obtenemos el identificador del grupo para cada dato\n",
    "centers = kmeans.cluster_centers_ # Obtenemos los centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBfcoWN90OBZ",
    "outputId": "7a064df9-0ccc-4829-8f24-3bd2426efb51"
   },
   "outputs": [],
   "source": [
    "# Palabras más relevantes por centroide\n",
    "n_center = 0\n",
    "fileids = inaugural.fileids()\n",
    "y_kmeans = np.array(y_kmeans)\n",
    "for i, center_i in enumerate(centers):\n",
    "  print('Centroide %d' %(i))\n",
    "  # Find documents in this centroid\n",
    "  pos_y = np.where(y_kmeans==i)[0]\n",
    "  print('Elementos grupo %d' %(i))\n",
    "  id_grupo = [fileids[pos] for pos in pos_y]\n",
    "  print(id_grupo)\n",
    "\n",
    "  # Find the ten most relevant words for this centroid\n",
    "  pos_sort = np.argsort(centers[i])[::-1][:10]\n",
    "  words_center = [D[p] for p in pos_sort]\n",
    "  print(words_center)\n",
    "  print('********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewKUSJZK11fh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QWvDrfYgN13e"
   ],
   "name": "Intro_NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
