{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensembles_students.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6eAsdbmD4uOX"},"source":["# Modelos basados en conjuntos \n","\n","### Curso Intermedio de Aprendizaje Automático 2021\n","\n","**Vanessa Gómez Verdejo, Emilio Parrado Hernández**\n","\n","Departamento de Teoría de la Señal y Comunicaciones\n","\n","**Universidad Carlos III de Madrid**\n","\n","<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"]},{"cell_type":"code","metadata":{"id":"2hrn3wI1dpoe"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%config InlineBackend.figure_format = 'retina'  \n","plt.rcParams[\"figure.figsize\"] = [6,6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xGWmrlvljmmv"},"source":["#1. Introducción a los conjuntos\n","\n","El objetivo de los conjuntos es combinar un conjunto de aprendices (clasificadores/regresores base) para construir un modelo con prestaciones superiores a cualquiera de los modelos base.\n","\n","La mayoría de los errores del aprendizaje de un modelo provienen de tres factores principales: varianza, ruido y sesgo. Usando conjuntos, somos capaces de aumentar la estabilidad del modelo final y reducir los errores mencionados anteriormente. \n","\n","La idea clave de los conjuntos consiste en explotar la diversidad entre los modelos base y la forma de generar esta diversidad nos permite clasificar estos métodos en dos tipos principales:\n","\n","* **Bagging**: la diversidad entre los modelos base se genera utilizando diferentes particiones de los datos de entrenamiento.\n","* **Boosting**: entrena secuencialmente un conjunto de modelos base débiles utilizando versiones modificadas de los datos.\n","\n","Ya hemos visto un tipo de estos métodos: *Random Forests (RF)*. Recuerda que RF entrena un conjunto de árboles, cada árbol utiliza un subconjunto diferente de muestras y características, y más tarde combina sus salidas. Por lo tanto, podemos decir que la RF es un método de Bagging. \n","\n","\n","**NOTA**: aquí, vamos a revisar estos métodos (y sus implementaciones) para el caso de la clasificación binaria, pero podemos encontrar enfoques similares para problemas de regresión y multiclase.\n"]},{"cell_type":"markdown","metadata":{"id":"QLCRyCKjH0r6"},"source":["# 2. Bagging: Boostrap Aggregating\n","\n","Un conjunto de Bagging combina un conjunto de clasificadores donde cada modelo base se entrena con un subconjunto submuestrado de las muestras de entrenamiento. Este submuestreo, llamado Bagging, consiste en elegir aleatoriamente, y con reemplazo, múltiples muestras aleatorias de los datos originales de entrenamiento. \n","\n","Una vez que el conjunto de modelos base es entrenado, el resultado final del conjunto se obtiene promediando todos los resultados de los modelos base. En el caso de la clasificación, normalmente se aplica un voto mayoritario.\n","\n","\n","<img align=\"center\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/ensembles/Bagging1.png\" width=60%>\n","\n","**References**\n","\n","L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.\n"]},{"cell_type":"markdown","metadata":{"id":"O9POJ5HNaBTA"},"source":["### Generación de un problema de juguete"]},{"cell_type":"code","metadata":{"id":"N_lmPAzCZYnT"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.datasets import make_blobs\n","n_samples = 1000\n","random_state = 170\n","X, y = make_blobs(n_samples=n_samples, centers=6,cluster_std = 2, random_state=random_state)\n","# Convert to binary\n","y[y==2]=0\n","y[y==3]=0\n","y[y==4]=1\n","y[y==5]=1\n","\n","# Create data partitions\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.4)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","plt.figure()\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train)\n","plt.title(\"Toy problem\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jiR7cu2rcWsn"},"source":["## Creando nuestro conjunto por Bagging\n","\n","Comencemos creando un conjunto de clasificadores por Bagging, para ello consideremos que los clasificadores base son árboles de decisión.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ak7F2dhKRM3-"},"source":["### Paso 1: Submuestreo\n","\n","Entrenemos varios árboles de decisión con versiones submuestreadas de los datos de entrenamiento "]},{"cell_type":"code","metadata":{"id":"zoVxTWjAdIiR"},"source":["# Some utility functions\n","\n","def plot_boundary(clf, X, Y, plt):\n","    \"\"\"Plot the classification regions for a given classifier.\n","\n","    Args:\n","        clf: scikit-learn classifier object.\n","        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only first two dimensions are plotted\n","        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n","        plt: graphic object where you wish to plot                                             \n","   \n","    \"\"\"\n","\n","    plot_colors = \"brymc\"\n","    plot_step = 0.02\n","    n_classes = np.unique(Y).shape[0]\n","    # Plot the decision regions\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                        np.arange(y_min, y_max, plot_step))\n","\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.axis(\"tight\")\n","\n","    # Plot the training points\n","    plt.scatter(X[:, 0], X[:, 1], c=Y)\n","\n","    plt.axis(\"tight\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vd-IBilYbu3n"},"source":["from sklearn import tree\n","import copy\n","\n","# Some parameters\n","max_depth = 4\n","T =10\n","nperc = 0.4\n","N = X_train.shape[0]\n","Ntest = X_test.shape[0]\n","Nsub = np.int(nperc*N)\n","\n","# Define the base classifier\n","clf_tree = tree.DecisionTreeClassifier(max_depth=max_depth)\n","ensemble_clf = []\n","o_train = np.zeros((T,N))\n","o_test = np.zeros((T,Ntest))\n"," \n","plt.figure(figsize=(25,10))\n","np.random.seed(0)\n","\n","# Repeat this T times....\n","for t in range(T):\n","  # Subsampling with replacement\n","  samples_bag = np.random.choice(N, Nsub, replace=True)\n","  X_train_sub  = X_train[samples_bag,:]\n","  Y_train_sub  = Y_train[samples_bag]\n","\n","  # Train a bagged tree\n","  clf_tree.fit(X_train_sub, Y_train_sub)\n","  ensemble_clf.append(copy.copy(clf_tree))\n","  \n","  # Get and save soft-output\n","  o_train[t,:] = clf_tree.predict_proba(X_train)[:,1]\n","  o_test[t,:] = clf_tree.predict_proba(X_test)[:,1]\n","\n","  # Compute test predictions\n","  acc = clf_tree.score(X_test, Y_test)\n","  print('Accuracy tree %d: %2.2f'%(t, acc))\n","  \n","  # Plot the solution\n","  plt.subplot(2,5,t+1)\n","  plot_boundary(clf_tree, X_train, Y_train, plt)\n","  plt.title('Tree '+str(t))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXKWSzLF748I"},"source":["### Paso 2: Combinando las salidas\n","\n","En problemas de clasificación podemos encontrarnos dos aproximaciones para obtener la salida final del conjunto:\n","\n","1. **Calculando la moda de las salidas de los clasificadores base**\n","\n","Cuando trabajamos con problemas de clasificación, es bastante común obtener la salida de conjunto como la salida más frecuente. Con este esquema, se considera que la salida de cada clasificador base es un voto y se calcula la salida final del conjunto como el voto mayoritario.\n","\n","\n","2. **Calculando el promedio de las salidas de los clasificadores base**\n","\n","Otra opción consiste en aprovechar los resultados de los clasificadores para calcular la predicción final como un promedio de todas las predicciones de los clasificadores base. Esta opción también se utiliza en los problemas de clasificación, pero es el enfoque típico para los problemas de regresión.\n","\n"]},{"cell_type":"code","metadata":{"id":"kW3BR_XFfZZz"},"source":["# Solution computing the ensemble output with the mode\n","from scipy import stats\n","from sklearn.metrics import accuracy_score\n","# Use the values saved in o_train and o_test\n","\n","# Convert soft-output to target values\n","o_train_pred = np.where(o_train > 0.5, 1, 0)\n","o_test_pred = np.where(o_test > 0.5, 1, 0)\n","\n","# Compute the mode\n","f_train_pred = np.squeeze(stats.mode(o_train_pred, axis=0)[0])\n","f_test_pred = np.squeeze(stats.mode(o_test_pred, axis=0)[0])\n","\n","# Final ensemble performance\n","acc_ens = accuracy_score(f_test_pred, Y_test)\n","print('Ensemble accuracy: %2.2f'%(acc_ens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xN0VQdUZBVBo"},"source":["def plot_boundary(ensemble_clf, X, Y, plt):\n","    \"\"\"Plot the classification regions for a given classifier.\n","\n","    Args:\n","        clf: scikit-learn classifier object.\n","        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only first two dimensions are plotted\n","        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n","        plt: graphic object where you wish to plot                                             \n","   \n","    \"\"\"\n","\n","    plot_colors = \"brymc\"\n","    plot_step = 0.02\n","    n_classes = np.unique(Y).shape[0]\n","    # Plot the decision regions\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                        np.arange(y_min, y_max, plot_step))\n","    Z = np.zeros((len(ensemble_clf), xx.ravel().shape[0]))\n","    for t, clf in enumerate(ensemble_clf):\n","      Z[t,:] = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","      \n","    Z_ens_pred = np.squeeze(stats.mode(Z, axis=0)[0])\n","\n","    Z_ens_pred = Z_ens_pred.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z_ens_pred, cmap=plt.cm.Paired)\n","\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.axis(\"tight\")\n","\n","    # Plot the training points\n","    #for i, color in zip(range(n_classes), plot_colors):\n","    #    idx = np.where(Y == i)\n","    #    plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n","    plt.scatter(X[:, 0], X[:, 1], c=Y)\n","\n","    plt.axis(\"tight\")\n","\n","plt.figure()\n","plot_boundary(ensemble_clf, X_train, Y_train, plt)\n","plt.title('Ensemble')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyMIUA4c9dqU"},"source":["# Solution computing the ensemble output with the mean\n","# Use the values saved in o_train and o_test\n","\n","# Convert soft-output to target values\n","o_train_pred = np.where(o_train > 0.5, 1, 0)\n","o_test_pred = np.where(o_test > 0.5, 1, 0)\n","\n","# Compute the mode\n","f_train_pred = np.where(np.mean(o_train, axis=0) > 0.5, 1, 0)\n","f_test_pred = np.where(np.mean(o_test, axis=0) > 0.5, 1, 0)\n","\n","# Final ensemble performance\n","acc_ens = accuracy_score(f_test_pred, Y_test)\n","print('Ensemble accuracy: %2.2f'%(acc_ens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvcjeeiOD43j"},"source":["def plot_boundary(ensemble_clf, X, Y, plt):\n","    \"\"\"Plot the classification regions for a given classifier.\n","\n","    Args:\n","        clf: scikit-learn classifier object.\n","        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only first two dimensions are plotted\n","        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n","        plt: graphic object where you wish to plot                                             \n","   \n","    \"\"\"\n","\n","    plot_colors = \"brymc\"\n","    plot_step = 0.02\n","    n_classes = np.unique(Y).shape[0]\n","    # Plot the decision regions\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                        np.arange(y_min, y_max, plot_step))\n","    Z = np.zeros((len(ensemble_clf), xx.ravel().shape[0]))\n","    for t, clf in enumerate(ensemble_clf):\n","      Z[t,:] = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n","\n","    Z_ens_pred = np.where(np.mean(Z, axis=0) > 0.5, 1, 0)\n","    Z_ens_pred = Z_ens_pred.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z_ens_pred, cmap=plt.cm.Paired)\n","\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.axis(\"tight\")\n","\n","    # Plot the training points\n","    #for i, color in zip(range(n_classes), plot_colors):\n","    #    idx = np.where(Y == i)\n","    #    plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n","    plt.scatter(X[:, 0], X[:, 1], c=Y)\n","\n","    plt.axis(\"tight\")\n","\n","plt.figure()\n","plot_boundary(ensemble_clf, X_train, Y_train, plt)\n","plt.title('Ensemble')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fj4hTq7OZXPD"},"source":["## Modelos de Bagging en sklearn\n","\n","Los modelos de Bagging pueden entrenarse fácilmente en sklearn usando la clase [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) que permite definir un clasificador base junto con diferentes parámetros para definir la estrategia exacta para la generación de subconjuntos de datos de entrenamiento. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"z9X9LRhCjif9"},"source":["### Ejercicio 1\n","\n","Utiliza el modelo `BaggingClassifier` de sklearn para entrenar un conjunto de T árboles con una tasa de submuestreo del 40% (`nperc = 0.4`). Para este ejercicio, considere que cada árbol tiene una profundidad máxima de 4 y utilice los valores predeterminados para el resto de los parámetros.\n","\n","Compare los resultados con los de nuestra implementación anterior.\n"]},{"cell_type":"markdown","metadata":{"id":"3CRK54qrWHN-"},"source":["#### Solución"]},{"cell_type":"code","metadata":{"id":"iIOJ2Rumjtzo"},"source":["from sklearn.ensemble import BaggingClassifier\n","max_depth = 4\n","T =10\n","nperc = 0.4\n","#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFYd30yWGhV_"},"source":["## Profundizando en el funcionamiento de los modelos de Bagging "]},{"cell_type":"markdown","metadata":{"id":"D-Ij9Lf2FY_S"},"source":["Vamos  utilizar la librería de sklearn para ver como evoluciona el error según añadimos árboles al conjunto."]},{"cell_type":"code","metadata":{"id":"zTpdug0DFYum"},"source":["T=20\n","n_perc = 0.4\n","bagg1_acc_tr=[]\n","bagg1_acc_test=[]\n","for t in range(1,T):\n","  clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(max_depth=4), n_estimators=t, max_samples=n_perc).fit(X_train, Y_train)\n","  bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","  bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","\n","plt.figure()\n","plt.plot(bagg1_acc_tr, label='Train acc')\n","plt.plot(bagg1_acc_test, label='Test acc')\n","\n","plt.legend()\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6EkeRFCwQMA"},"source":["Y si repetimos ahora el experimento...."]},{"cell_type":"code","metadata":{"id":"hs6D48t1qB4Z"},"source":["T=20\n","n_perc = 0.4\n","bagg1_acc_tr=[]\n","bagg1_acc_test=[]\n","for t in range(1,T):\n","  clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(max_depth=4), n_estimators=t, max_samples=n_perc).fit(X_train, Y_train)\n","  bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","  bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","\n","plt.figure()\n","plt.plot(bagg1_acc_tr, label='Train acc')\n","plt.plot(bagg1_acc_test, label='Test acc')\n","\n","plt.legend()\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvyhRc2wwUKs"},"source":["Aunque el error final tiende a ser el mismo, vemos que las curvas son muy ruidosas y hay muchas diferencias entre ellas (principalmente, al principio). Vamos a repetir este experimento varias veces y promediar los resultados..."]},{"cell_type":"code","metadata":{"id":"KyeB0QLaqG7p"},"source":["T=80\n","n_perc = 0.4\n","niter = 20\n","\n","bagg1_acc_tr_iter=[]\n","bagg1_acc_test_iter=[]\n","for n in range(niter):\n","  bagg1_acc_tr=[]\n","  bagg1_acc_test=[]\n","  for t in range(1,T+1):\n","    clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(max_depth=4), n_estimators=t, max_samples=n_perc).fit(X_train, Y_train)\n","    bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","    bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","  bagg1_acc_tr_iter.append(bagg1_acc_tr)\n","  bagg1_acc_test_iter.append(bagg1_acc_test)\n","\n","\n","bagg1_acc_tr_iter = np.array(bagg1_acc_tr_iter)\n","bagg1_acc_test_iter= np.array(bagg1_acc_test_iter)\n","\n","plt.figure(figsize=(20,10))\n","plt.subplot(2,2,1)\n","plt.plot(bagg1_acc_tr_iter.T, label='Train acc')\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title('train acc')\n","\n","plt.subplot(2,2,2)\n","plt.plot(bagg1_acc_test_iter.T, label='Test acc')\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title('test acc')\n","\n","\n","plt.subplot(2,2,3)\n","plt.errorbar(np.arange(T),np.mean(bagg1_acc_tr_iter, axis=0), np.std(bagg1_acc_tr_iter, axis=0), fmt='-o')\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title('train acc')\n","\n","plt.subplot(2,2,4)\n","plt.errorbar(np.arange(T),np.mean(bagg1_acc_test_iter, axis=0), np.std(bagg1_acc_test_iter, axis=0), fmt='-o')\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title('test acc')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-jVP72FwlCy"},"source":["Viendo las curvas promedio (y su varianza) vemos cosas muy interesantes:\n","* Cada árbol utiliza particiones muy diferentes de los datos, proporcionando soluciones muy dispares (esto lo vemos cuando hay pocos árboles en el conjunto). \n","\n","* Ésta es justamente la **diversidad** entre sus salidas que vamos a aprovechar en la combinación.\n","\n","* Según se añaden árboles (y promediamos sus salidas), las prestaciones del modelo se vuelven más estables. Al combinar muchos modelos conseguimos **reducir la varianza del error** obteniendo unas mejores prestaciones.\n","\n","* Simplemente, por el hecho de combinar modelos, conseguimos tener un **modelo más estable**.\n","\n","* Además, este método no corre peliro de sobreajustar, podemos añadir todos los clasificadores que queramos al conjunto sin correr peligro de sobreentrenar.\n"]},{"cell_type":"markdown","metadata":{"id":"tdEWpUnKpsSD"},"source":["## Influencia del parámetro `n_perc`\n","\n","Hasta ahora, hemos usado el 40% de los datos para entrenar cada clasificador base. Uno podría pensar, que si usamos más muestras, mejoraríamos las prestaciones de estos clasificadores base y, por tanto, las prestaciones del conjunto final. Sin embargo, esto no garantiza la mejora de los resultados. \n","\n","En los modelos de Bagging siempre hay un compromiso entre las prestaciones del clasificador base y la ganancia que se obtiene en el conjunto. El usar más muestras para entrenar puede aportar ventajas al conjunto si los clasificadores base son muy malos, pero si estos modelos ya funcionan bastante bien, son muy estables y sus salidas son muy parecidas (no hay diversidad), la mejora aportada por el conjunto se reduce...\n","\n","Para analizar esto vamos a entrenar diferentes conjuntos con diferentes valores de `n_perc` y analizar las prestaciones..."]},{"cell_type":"code","metadata":{"id":"GjmzZGUhlfEX"},"source":["T=40\n","range_perc = np.arange(0.05, 1.01, 0.05)\n","# First, let's train a single tree for comparison purposes\n","\n","N = X_train.shape[0]\n","tree_acc_tr_all=[]\n","tree_acc_test_all=[]\n","for n in range(100):\n","  tree_acc_tr=[]\n","  tree_acc_test=[]\n","  for n_perc in range_perc:\n","    # Subsampling with replacement\n","    Nsub = np.int(n_perc*N)\n","    samples_bag = np.random.choice(N, Nsub, replace=True)\n","    X_train_sub  = X_train[samples_bag,:]\n","    Y_train_sub  = Y_train[samples_bag]\n","    # Train a tree\n","    clf_tree = tree.DecisionTreeClassifier().fit(X_train_sub, Y_train_sub )\n","    tree_acc_tr.append(clf_tree.score(X_train, Y_train))\n","    tree_acc_test.append(clf_tree.score(X_test, Y_test))\n","  tree_acc_tr_all.append(tree_acc_tr)\n","  tree_acc_test_all.append(tree_acc_test)\n","\n","tree_acc_tr_all = np.array(tree_acc_tr_all)\n","tree_acc_test_all= np.array(tree_acc_test_all)\n","\n","\n","# Now, let's train our ensemble\n","bagg1_acc_tr_iter=[]\n","bagg1_acc_test_iter=[]\n","for n in range(40):\n","  bagg1_acc_tr=[]\n","  bagg1_acc_test=[]\n","  for n_perc in range_perc:\n","    clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(), n_estimators=T, max_samples=n_perc).fit(X_train, Y_train)\n","    bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","    bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","  bagg1_acc_tr_iter.append(bagg1_acc_tr)\n","  bagg1_acc_test_iter.append(bagg1_acc_test)\n","\n","\n","bagg1_acc_tr_iter = np.array(bagg1_acc_tr_iter)\n","bagg1_acc_test_iter= np.array(bagg1_acc_test_iter)\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(range_perc, np.mean(tree_acc_tr_all,axis=0), label='Single tree')\n","plt.plot(range_perc, np.mean(bagg1_acc_tr_iter,axis=0), label='Ensemble')\n","plt.xlabel('n_perc')\n","plt.ylabel('Accuracy')\n","plt.title('Train acc')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","\n","plt.plot(range_perc, np.mean(tree_acc_test_all,axis=0), label='Single tree')\n","plt.plot(range_perc, np.mean(bagg1_acc_test_iter,axis=0), label='Ensemble')\n","plt.xlabel('n_perc')\n","plt.ylabel('Accuracy')\n","plt.title('Test acc')\n","plt.legend()\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1cEq9Ma8wK6"},"source":["Este efecto viene generado por lo que se conoce como **diversidad**, si queremos que el conjunto sea capaz de mejorar las prestaciones de los clasificadores base, necesitamos que haya diversidad entre ellos, es decir, que proporcionen salidas diferentes. Si tenemos árboles muy parecidos (con salidas muy similares), el conjunto no es capaz de mejorar la salida final combinando salidas similares, simplemente, dará más de lo mismo.\n","\n","Veamos esto calculando la diversidad entre los árboles. Para ello vamos a calcular la diverdad como $1-\\rho$, siendo $\\rho$ la correlación entre sus salidas."]},{"cell_type":"code","metadata":{"id":"v2hVLTqP_MNW"},"source":["T=40\n","range_perc = np.arange(0.05, 1.01, 0.05)\n","\n","# Now, let's train our ensemble\n","bagg1_acc_tr_iter=[]\n","bagg1_acc_test_iter=[]\n","Diversity_iter=[]\n","for n in range(40):\n","  bagg1_acc_tr=[]\n","  bagg1_acc_test=[]\n","  Diversity=[]\n","  for n_perc in range_perc:\n","    clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(), n_estimators=T, max_samples=n_perc).fit(X_train, Y_train)\n","    bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","    bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","    f_train = np.zeros((T,X_train.shape[0]))\n","    for t in range(T):\n","      f_train[t,:] = clf1.estimators_[t].predict_proba(X_train)[:,0]\n"," \n","    Matrix_corr = np.corrcoef(f_train)\n","    corrValues = np.triu(Matrix_corr, k=1)\n","    corrValues = corrValues[np.nonzero(corrValues)] \n","    Diversity.append(1-np.abs(np.mean(corrValues)))\n","  bagg1_acc_tr_iter.append(bagg1_acc_tr)\n","  bagg1_acc_test_iter.append(bagg1_acc_test)\n","  Diversity_iter.append(Diversity)\n","\n","bagg1_acc_tr_iter = np.array(bagg1_acc_tr_iter)\n","bagg1_acc_test_iter = np.array(bagg1_acc_test_iter)\n","Diversity_iter = np.array(Diversity_iter)\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(range_perc, np.mean(Diversity_iter,axis=0))\n","plt.xlabel('n_perc')\n","plt.ylabel('Diversity')\n","plt.title('Diversity')\n","\n","plt.subplot(1,2,2)\n","plt.plot(range_perc, np.mean(bagg1_acc_test_iter,axis=0))\n","plt.xlabel('n_perc')\n","plt.ylabel('Accuracy')\n","plt.title('Test accuracy')\n","\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwRp7uUQBMq_"},"source":["## ¿Podemos aumentar la diversidad con otros esquemas?\n","\n","Una manera muy sencilla de aumentar la diversidad es hacer selección de muestras y de características al mismo tiempo (submuestrear la matriz de datos de entrenamiento por filas y por columnas). Para ello, la clase `BaggingClassifier` al igual que tiene el parámetro `max_samples` para indicar el número o porcentaje de muestras de entrenamiento a usar, tiene otro parámetro `max_features` que permite controlar el número o porcentaje de variables que se usan.\n","\n","<img align=\"center\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/ensembles/Bagging2.png\" width=60%>"]},{"cell_type":"markdown","metadata":{"id":"aUstdWWupe-L"},"source":["A continuación vamos a generar otro problema de juguete con más variables de entrada y analizar la influencia de submuestrear por features.\n"]},{"cell_type":"code","metadata":{"id":"WWkDbTLTqEKJ"},"source":["from sklearn.datasets import make_classification\n","# Generate a binary classification dataset.\n","X, y = make_classification(n_samples=1000, n_features=25,\n","                           n_clusters_per_class=1, n_informative=20,\n","                           random_state=0)\n","# Create data partitions\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.4)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ttlx96PhgXIB"},"source":["T=40\n","range_feat = np.arange(0.1, 1.01, 0.1)\n","n_perc = 0.5\n","# Now, let's train our ensemble\n","bagg1_acc_tr_iter=[]\n","bagg1_acc_test_iter=[]\n","Diversity_iter=[]\n","for n in range(40):\n","  bagg1_acc_tr=[]\n","  bagg1_acc_test=[]\n","  Diversity=[]\n","  for n_feat in range_feat:\n","    clf1 = BaggingClassifier(base_estimator= tree.DecisionTreeClassifier(), n_estimators=T, max_samples=n_perc, max_features=n_feat).fit(X_train, Y_train)\n","    bagg1_acc_tr.append(clf1.score(X_train, Y_train))\n","    bagg1_acc_test.append(clf1.score(X_test, Y_test))\n","    f_train = np.zeros((T,X_train.shape[0]))\n","    for t in range(T):\n","      # Here, we have to  use the infromation about the selected feature\n","      f_train[t,:] = clf1.estimators_[t].predict_proba(X_train[:,clf1.estimators_features_[t]])[:,0]\n"," \n","    Matrix_corr = np.corrcoef(f_train)\n","    corrValues = np.triu(Matrix_corr, k=1)\n","    corrValues = corrValues[np.nonzero(corrValues)] \n","    Diversity.append(1-np.abs(np.mean(corrValues)))\n","  bagg1_acc_tr_iter.append(bagg1_acc_tr)\n","  bagg1_acc_test_iter.append(bagg1_acc_test)\n","  Diversity_iter.append(Diversity)\n","\n","bagg1_acc_tr_iter = np.array(bagg1_acc_tr_iter)\n","bagg1_acc_test_iter = np.array(bagg1_acc_test_iter)\n","Diversity_iter = np.array(Diversity_iter)\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(range_feat, np.mean(Diversity_iter,axis=0))\n","plt.xlabel('n_features')\n","plt.ylabel('Diversity')\n","plt.title('Diversity')\n","\n","plt.subplot(1,2,2)\n","plt.plot(range_feat, np.mean(bagg1_acc_test_iter,axis=0))\n","plt.xlabel('n_features')\n","plt.ylabel('Accuracy')\n","plt.title('Test accuracy')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6k6uXOMdHkdG"},"source":["Además, sklearn también incluye dos métodos específicos para diseñar conjuntos por bagging, con diversidad adicional, cuando los clasificadores base son árboles de decisión:\n","* **[Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)**. Como ya sabemos los RF no son más que un conjunto de árboles donde cada árbol se entrena con un subconjunto de datos de entrenameinto y un subconjunto de características.\n","* **[Extremely Randomized Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)**. En este caso la diversidad se aumenta con los umbrales de decisión y no con las características, ya que este conjunto funciona como un Bagging de árboles de decisión normal, pero cada árbol, en lugar de buscar los umbrales más discriminantes, fuerza una diversidad adicional mediante el uso de umbrales aleatorios. Es decir, cada vez que analiza una variable candidata, no busca el mejor umbral sino que lo fija aleatoriamente y el mejor de estos pares variable-umbral se elige como la regla de división.\n"]},{"cell_type":"markdown","metadata":{"id":"ILkeQIyk1sm3"},"source":["### Ejercicio 2\n","Compare las prestaciones del esquema de Bagging normal, como el del Ejercicio 1, frente a las prestaciones de un RF y un ERT. Utilice 40 árboles con una profundidad máxima de 4 y un 40% de muestras de entrenamiento y 50% de características para el RF.\n"]},{"cell_type":"markdown","metadata":{"id":"5C4S1x9ZqRAu"},"source":["#### Solution"]},{"cell_type":"code","metadata":{"id":"YhU-C6Z6kCWk"},"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","max_depth = 4\n","T =40\n","n_perc = 0.4\n","n_feat = 0.5\n","#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MZ2q3Adn4Wz"},"source":["##Ventajas adicionales de los modelos de Bagging\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E4PPW2PInZgz"},"source":["**1. Estimación Out-of-bag (OOB)**\n","\n","Como sabemos, para el entrenamiento de cada clasificador sólo utilizamos un subconjunto de muestras, por lo que en la práctica cada muestra de entrenamiento sólo participa en el entrenamiento de algunos clasificadores. Podemos utilizar esto para obtener una estimación del error de esta muestra clasificándola con el conjunto de clasificadores en los que no ha participado. Si hacemos esto con todas las muestras y promediamos estos errores, obtenemos lo que se conoce como \"out-of-bag (OOB) estimation\". \n","\n","Veamos como podemos utilizar el OOB para seleccionar el número óptimo de datos y características a utilizar en el submuestreo de un RF.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xDtvBBhDsjFN"},"source":["range_feat = np.arange(0.1, 1.01, 0.1)\n","range_perc = np.arange(0.05, 1.01, 0.05)\n","\n","clf = RandomForestClassifier(n_estimators=T, oob_score=True, max_depth=4)\n","\n","error_oob_all = []\n","for n_perc in range_perc:\n","  clf.set_params(max_samples=nperc)\n","  error_oob = []\n","  for n_feat in range_feat:\n","    clf.set_params(max_features=n_feat)\n","    clf.fit(X, y)\n","    # Record the OOB error for each `n_estimators=i` setting.\n","    error_oob.append(1 - clf.oob_score_)\n","  error_oob_all.append(error_oob)     \n","\n","error_oob_all = np.array(error_oob_all)\n","\n","# Plot OOB\n","fig, ax = plt.subplots(1,1)\n","img= ax.imshow(error_oob_all , cmap=plt.cm.hot)\n","ax.set_xlabel('n_features')\n","ax.set_ylabel('n_perc')\n","pos_x = [1, 3, 5, 7, 9]\n","ax.set_xticks(pos_x )\n","ax.set_xticklabels((100*range_feat[pos_x]).astype(np.int))\n","pos_y = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n","ax.set_yticks(pos_y )\n","ax.set_yticklabels((100*range_perc[pos_y]).astype(np.int))\n","fig.colorbar(img)\n","fig.show()\n","\n","# select optimum values\n","ind = np.unravel_index(np.argmin(error_oob_all, axis=None), error_oob_all.shape)\n","feat_opt=range_feat[ind[1]]\n","perc_opt=range_perc[ind[0]]\n","print('Optimum number of features: %d'%(100*feat_opt))\n","print('Optimum number of data: %d' %(100*perc_opt))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIp-hJd9oFr9"},"source":["**2. Importancia de las características de entrada**\n","\n","Cuando usamos árboles como clasificadores base del conjunto, podemos evaluar la importancia de las características en una tarea de clasificación. Si tenemos en cuenta que la utilización de una variable de entrada por un árbol y su  profundidad nos proporciona una importancia relativa de esa característica, analizando esta información sobre todos los árboles del conjunto, podemos obtener una buena estimación de la importancia de cada característica. Podríamos, incluso, utilizar esta información para hacer selección de características. \n","\n","\n","\n","Tanto `RandomForestClassifier()` como `ExtraTreesClassifier()` son capaces de aprender durante su entrenamiento las **relevancias de las características**. Analicemos para el ejemplo anterior cuáles son las variables más importantes; nótese que cuando generamos los datos indicamos que solo 20 de las 25 variables eran relevantes."]},{"cell_type":"code","metadata":{"id":"k5AQphHj5gW3"},"source":["# Train the RF with the OOB selected parameters\n","clf_RF  = RandomForestClassifier(n_estimators=T, max_depth=4, max_samples=perc_opt, max_features=feat_opt).fit(X_train, Y_train)\n","acc_RF = clf_RF.score(X_test, Y_test)\n","print('RF accuracy: %2.2f'%(acc_RF))\n","\n","importances = clf_RF.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in clf_RF.estimators_],\n","             axis=0)\n","indices = np.argsort(importances)[::-1]\n","\n","# Print the feature ranking\n","print(\"Feature ranking:\")\n","\n","for f in range(X.shape[1]):\n","    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n","\n","# Plot the feature importances of the forest\n","plt.figure()\n","plt.title(\"Feature importances\")\n","plt.bar(range(X.shape[1]), importances[indices],\n","       color=\"r\", yerr=std[indices], align=\"center\")\n","plt.xticks(range(X.shape[1]), indices)\n","plt.xlim([-1, X.shape[1]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blmHK6yPH3Wn"},"source":["# 3. Boosting: Real Adaboost\n","Los métodos de Boosting entrenan una secuencia de clasificadores base/débil con versiones ponderadas o enfatizadas de los datos de entrenamiento. Cada uno de estos clasificadores es débil ya que su tasa de error puede ser ligeramente mejor que un clasificador aleatorio. Para obtener el resultado final del conjunto, las salidas de todos estos clasificadores débiles se combinan mediante una combinación ponderada.\n","\n","El algoritmo de Boosting más popular se llama **AdaBoost** (Adaptive Boosting). Este método de Boosting entrena una secuencia de clasificadores débiles de tal manera que cada nuevo clasificador presta más atención a las muestras mal clasificadas por los clasificadores anteriores. \n","\n","Podemos encontrarnos con dos versiones de este algoritmo:\n","* AdaBoost.M1 o \"AdaBoost Discreto\" donde las salidas de los clasificadores base son estimaciones discretas de la clase de salida.\n","* \"AdaBoost Real\", en este caso, el clasificador base devuelve una predicción de valor real (por ejemplo, una probabilidad asignada al intervalo [-1,1]).\n","\n","Profundicemos ahora en los principios de funcionamiento del algoritmo **Real AdaBoost**.\n","\n","\n","**References**\n","\n","* Schapire, R.E. \"The strength of weak learnability\". Machine Learning, 5(2): 1651-1686, 1990.\n","\n","* Freund, Y. and Schapire, R.E. \"Experiments with a new boosting algorithm\". Proc. of the 13th International Conference on Machine Learning. pp. 148-156, 1996.\n","\n","* Friedmann, J. H.,  “Stochastic Gradient Boosting”, 2007.\n"]},{"cell_type":"markdown","metadata":{"id":"y9xauQN8CSOp"},"source":["## Real Adaboost\n","Considere que tenemos un problema de clasificación binaria dado por el conjunto de datos de entrenamiento $S$ que consiste en $N$ pares $(\\mathbf{x}^{(i)},y^{(i)})$, donde $\\mathbf{x}^{(i)}\\in\\mathbb{R}^L$ es la observación $i$-ésima e   $y^{(i)}\\in\\{-1,1\\}$  es su etiqueta asociada.\n","\n","El algoritmo Real Adaboost (RA) entrena secuencialmente un conjunto de  $T$  clasificadores base donde cada clasificador implementa una función de predicción $o_t(x) \\in [-1,1]$. Para aprender esta función de predicción cada clasificador observa todo el conjunto de datos de entrenamiento , $S$, pero durante su entrenamiento se utiliza una función de énfasis $D_t(\\mathbf{x})$ para que el clasificador preste más atención a las muestras más difíciles de clasificar.\n","\n","Finalmente, la salida final del conjunto se obtiene como una suma ponderada de todos las salidas de los clasificadores base:\n","\n","$$ f_T({\\bf x}) = \\displaystyle \\sum_{t=1}^T \\alpha_t o_t({\\bf x})$$\n","\n","<img align=\"center\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/ensembles/Boosting.png\" width=80%>\n","\n","\n","\n","* **Función de énfasis**:\n","Para forzar la diversidad entre los clasificadores base, RA pondera (asigna difernetes importancias) a los datos de entrenamiento por medio de una función de énfasis. Inicialmente todos los pesos son iguales:\n","$$ D_{1}({\\bf x}^{(i)}) = \\frac{1}{N} $$\n","\n","para que el primer clasificador sea entrenado de la manera habitual. \n","\n","Para las siguientes iteraciones, $t = 2,3,\\ldots,T$, la función de énfasis se actualiza con la siguiente regla:\n","$$ D_{t+1}({\\bf x}^{(i)}) = \\frac{D_{t}({\\bf x}^{(i)}) \\exp \\left( - \\alpha_t o_t({\\bf x}^{(i)}) {y}^{(i)} \\right)}{Z_t}   $$\n","  donde $Z_t$ es una constante de normalización que hace que  $\\sum_{i=1}^N D_{t+1}({\\bf x}^{(i)})  = 1$. \n","  \n","Obsérvese que esta regla de actualización aumenta el peso de énfasis para aquellas observaciones que fueron mal clasificadas por los clasificadores anteriores, mientras que los pesos se reducen para las que fueron clasificadas correctamente. Por lo tanto, a medida que se añaden nuevos alumnos al conjunto, las muestras más erróneas recibirán una mayor atención.\n","\n","* **Pesos de salida ($\\alpha_t$)**\n","Para obtener los pesos de salida, RA minimiza la función de pérdida exponencial:\n","$${\\bf \\alpha}^* =\\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - \\sum_{t=1}^T \\alpha_t o_t({\\bf x}^{(i)}) {y}^{(i)} \\right) = \\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - f_T({\\bf x}^{(i)}) {y}^{(i)} \\right)$$\n","cuya solución puede obtenerse analíticamente como\n","$$\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1+\\gamma_t}{1-\\gamma_t}\\right)$$\n","donde $\\gamma_t = \\sum_{i=1}^N   D_{t}({\\bf x}^{(i)}) o_t({\\bf x}^{(i)}) {y}^{(i)} $. El efecto de estos pesos es dar mayor importancia a los clasificadores que tienen mejores prestaciones dentro del conjunto. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"MiLPEEKbbCEW"},"source":["**Función de coste exponencial**\n","Esta función de coste es bastante similar a la utilizada por los modelos de regresión logística y es una cota superior del error de clasificación.\n","\n"]},{"cell_type":"code","metadata":{"id":"d6a-vbVEb_fr"},"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPzqAQDVjoiZ"},"source":["# Plot the exponential loss function (un upper bound of the classfication error)\n","f = np.arange(-2,2,0.01)\n","y = 1\n","\n","l_alpha = np.exp(-y*f)\n","plt.figure()\n","plt.plot(y*f,l_alpha, label='Exponential loss')\n","\n","\n","# Compare with binomial deviance\n","l_w = np.log(1+ np.exp(f))-y*f\n","plt.plot(y*f,l_w, label='Binomial deviance')\n","\n","# Classification error\n","e_class = np.zeros(f.shape)\n","e_class[y*f<0] =1\n","plt.plot(y*f,e_class, label='Classification error')\n","\n","plt.legend()\n","plt.xlabel('y*f')\n","plt.ylabel('Loss function')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8gHaTVpRTp7"},"source":["## Creemos nuestro conjunto por Boosting (Real Adaboost)"]},{"cell_type":"markdown","metadata":{"id":"omy1moxeQ8Os"},"source":["Generemos de nuevo nuestro problema de clasificación bidimensional para ir viendo como construimos un conjunto RA. De nuevo, vamos a considerar como clasificador base un árbol de decisión. "]},{"cell_type":"code","metadata":{"id":"mX65f5pSQ7Tp"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.datasets import make_blobs\n","n_samples = 1000\n","random_state = 170\n","X, y = make_blobs(n_samples=n_samples, centers=6, cluster_std = 2, random_state=random_state)\n","# Convert to binary\n","y[y==2]=0\n","y[y==3]=0\n","y[y==4]=1\n","y[y==5]=1\n","\n","# Create data partitions\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.4)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","plt.figure()\n","plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train)\n","plt.title(\"Toy problem\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DlMVoB-LRauO"},"source":["### Paso 1: Entrenamos el primer clasificador"]},{"cell_type":"code","metadata":{"id":"GKVTrfr8TwfP"},"source":["from sklearn import tree\n","import copy\n","\n","N = X_train.shape[0]\n","max_depth =4\n","clf_ens =[]\n","alpha_ens = []\n","f_pred_train = np.zeros((N,))\n","# Initialize emphasis function\n","Dt = (1./N) * np.ones((N,))\n","clf_tree = tree.DecisionTreeClassifier(max_depth=max_depth) \n","\n","# Train a boosted tree (use sample_weight parameter)\n","clf_tree.fit(X_train, Y_train, sample_weight=Dt)\n","clf_ens.append(copy.copy(clf_tree))\n","# Compute tree soft-outputs (interval [-1,1])\n","o_t = 2*clf_tree.predict_proba(X_train)[:,1]-1\n","  \n","# Compute output weights\n","gamma_t = Dt @ (o_t*(2*Y_train-1))\n","alpha_t = 0.5* np.log ((1+gamma_t)/(1-gamma_t))\n","alpha_ens.append(alpha_t)\n","# Update ensemble  output\n","f_pred_train = f_pred_train + alpha_t * o_t\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRPcOA7ZVE4t"},"source":["Veamos la salida de este clasificador y las muestras más erróneas"]},{"cell_type":"code","metadata":{"id":"j9Zeu9zcjOA0"},"source":["def plot_boundary(clf, X, Y, plt):\n","    \"\"\"Plot the classification regions for a given classifier.\n","\n","    Args:\n","        clf: scikit-learn classifier object.\n","        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only first two dimensions are plotted\n","        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n","        plt: graphic object where you wish to plot                                             \n","   \n","    \"\"\"\n","\n","    plot_colors = \"brymc\"\n","    plot_step = 0.02\n","    n_classes = np.unique(Y).shape[0]\n","    # Plot the decision regions\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                        np.arange(y_min, y_max, plot_step))\n","\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.axis(\"tight\")\n","\n","    # Plot the training points\n","    plt.scatter(X[:, 0], X[:, 1], c=Y)\n","\n","    plt.axis(\"tight\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lN8keBG7VbYY"},"source":["def plot_boundaryRA(clf_ens, alpha_ens, X, Y, plt):\n","    \"\"\"Plot the classification regions for a given classifier.\n","\n","    Args:\n","        clf: scikit-learn classifier object.\n","        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only first two dimensions are plotted\n","        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n","        plt: graphic object where you wish to plot                                             \n","   \n","    \"\"\"\n","\n","    plot_colors = \"brymc\"\n","    plot_step = 0.02\n","    X=X_train\n","    Y=Y_train\n","    n_classes = np.unique(Y).shape[0]\n","    # Plot the decision regions\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                        np.arange(y_min, y_max, plot_step))\n","\n","    Z = np.zeros((xx.ravel().shape))\n","    f_t = np.zeros((Y.shape))\n","    for t, clf in enumerate(clf_ens):\n","      Z_t = 2*clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]-1\n","      Z = Z + alpha_ens[t]*Z_t\n","      o_t = 2*clf.predict_proba(X)[:,1]-1\n","      f_t = f_t + alpha_ens[t]* o_t\n","    Z = np.where(Z > 0, 1, 0)\n","    Z = Z.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","    # Mark erroneous samples\n","    # Compute output weights\n","    error_t =  f_t*(2*Y-1)\n","    pos_error = np.where(error_t<=0)[0]\n","\n","\n","    plt.xlabel('Feature 1')\n","    plt.ylabel('Feature 2')\n","    plt.axis(\"tight\")\n","\n","    # Plot the training points\n","    plt.scatter(X[:, 0], X[:, 1], c=Y, label='Training data')\n","    size=20*np.abs(error_t[pos_error])\n","    # Plot erroneous data\n","    plt.scatter(X[pos_error, 0], X[pos_error, 1], c='r', s = size, marker='x', label='Erroneous sample')\n","    plt.axis(\"tight\")\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pjb7bEgxbMSk"},"source":["plt.figure()\n","plot_boundaryRA(clf_ens, alpha_ens, X_train, Y_train, plt)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gb_I4UMNhG_B"},"source":["### Paso 2: Actualizamos la función de énfasis y añadimos otro árbol"]},{"cell_type":"code","metadata":{"id":"uNim_RDvhNeZ"},"source":["# Update emphasis function\n","Dt = Dt * np.exp(-alpha_t * (o_t*Y_train))\n","Dt = Dt /np.sum(Dt)\n","# Train a boosted tree (use sample_weight parameter)\n","clf_tree.fit(X_train, Y_train, sample_weight=Dt)\n","clf_ens.append(copy.copy(clf_tree))\n","# Compute tree soft-outputs (interval [-1,1])\n","o_t = 2*clf_tree.predict_proba(X_train)[:,1]-1\n","  \n","# Compute output weights\n","gamma_t = Dt @ (o_t*(2*Y_train-1))\n","alpha_t = 0.5* np.log ((1+gamma_t)/(1-gamma_t))\n","alpha_ens.append(alpha_t)\n","# Update ensemble  output\n","f_pred_train = f_pred_train + alpha_t * o_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7U1UbdRhbn4"},"source":["plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plot_boundary(clf_ens[-1], X_train, Y_train, plt)\n","plt.title('Salida del último árbol')\n","\n","plt.subplot(1,2,2)\n","plot_boundaryRA(clf_ens, alpha_ens, X_train, Y_train, plt)\n","plt.title('Salida del conjunto RA')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFdtQb5_hiBH"},"source":["### Paso T: Repetimos este proceso T veces..."]},{"cell_type":"code","metadata":{"id":"QJor00vMhhTB"},"source":["from sklearn import tree\n","N = X_train.shape[0]\n","max_depth =4\n","clf_ens =[]\n","alpha_ens = []\n","f_pred_train = np.zeros((N,))\n","# Initialize emphasis function\n","Dt = (1./N) * np.ones((N,))\n","clf_tree = tree.DecisionTreeClassifier(max_depth=max_depth) \n","\n","T = 20\n","plt.figure(figsize=(20,25))\n","for t in range(T):\n","  # Train a boosted tree (use sample_weight parameter)\n","  clf_tree.fit(X_train, Y_train, sample_weight=Dt)\n","  clf_ens.append(copy.copy(clf_tree))\n","  # Compute tree soft-outputs (interval [-1,1])\n","  o_t = 2*clf_tree.predict_proba(X_train)[:,1]-1\n","    \n","  # Compute output weights\n","  gamma_t = Dt @ (o_t*(2*Y_train-1))\n","  alpha_t = 0.5* np.log ((1+gamma_t)/(1-gamma_t))\n","  alpha_ens.append(alpha_t)\n","  # Update ensemble  output\n","  f_pred_train = f_pred_train + alpha_t * o_t\n","  # Update emphasis function\n","  Dt = Dt * np.exp(-alpha_t * (o_t*Y_train))\n","  Dt = Dt /np.sum(Dt)\n","  # Plot the solution\n","  plt.figure(figsize=(10,5))\n","  plt.subplot(1,2,1)\n","  plot_boundary(clf_ens[-1], X_train, Y_train, plt)\n","  plt.title('Paso '+str(t)+': salida del último árbol')\n","  plt.subplot(1,2,2)\n","  plot_boundaryRA(clf_ens, alpha_ens, X_train, Y_train, plt)\n","  plt.title ('Paso '+str(t)+': salida del conjunto')\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SpCOlE2-GrdG"},"source":["## Modelos de Boosting en sklearn\n","\n","Podemos encontrar una implementación del clasificador Real AdaBoost en la biblioteca de sklearn. Esta clase, llamada [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier), nos permite utilizar las versiones Discreta y Real del Adaboost ajustando el parámetro `algorithm` a `SAMME` o `SAMME.R`, respectivamente.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b6GakRXL4x7j"},"source":["### Ejercicio 3\n","\n","Usa el modelo `AdaBoostClassifier` para entrenar un conjunto de 10 árboles de decisión con profundidad 4. Utiliza las dos implementaciones del Adaboost (`algorithm` = `SAMME` y `algorithm` =  `SAMME.R`) y compara el resultado con el conjunto construido por Bagging en el Ejercicio 1.\n"]},{"cell_type":"markdown","metadata":{"id":"HNtNTp329Bgq"},"source":["#### Solution"]},{"cell_type":"code","metadata":{"id":"HfqArWS84x7s"},"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","max_depth = 4\n","T =10\n","nperc = 0.4\n","#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vA5lAMzp6I2F"},"source":["## Profundizando en el funcionamiento del Boosting"]},{"cell_type":"markdown","metadata":{"id":"R5VoV1Wm6Is-"},"source":["Al igual que con los métodos de Bagging, vamos a analizar la evolución del error, según añadimos clasificadores al conjunto, para entender así su funcionamiento. Para ello, la siguiente celda de código analiza esta evolución para las dos versiones de Adaboost que incluye sklearn."]},{"cell_type":"code","metadata":{"id":"b7HSl2UjR6sP"},"source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","T =50\n","\n","DA_acc_tr=[]\n","DA_acc_test=[]\n","RA_acc_tr=[]\n","RA_acc_test=[]\n","\n","for t in range(1,T+1):\n","  clf_DA = AdaBoostClassifier(base_estimator= tree.DecisionTreeClassifier(max_depth=4), n_estimators=t, algorithm ='SAMME').fit(X_train, Y_train)\n","  DA_acc_tr.append(clf_DA.score(X_train, Y_train))\n","  DA_acc_test.append(clf_DA.score(X_test, Y_test))\n","\n","  clf_RA = AdaBoostClassifier(base_estimator= tree.DecisionTreeClassifier(max_depth=4),n_estimators=t, algorithm ='SAMME.R').fit(X_train, Y_train)\n","  RA_acc_tr.append(clf_RA.score(X_train, Y_train))\n","  RA_acc_test.append(clf_RA.score(X_test, Y_test))\n","\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1,2,1)\n","plt.plot(DA_acc_tr, label='Train acc')\n","plt.plot(DA_acc_test, label='Test acc')\n","plt.legend()\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title(\"Discrete AdaBoost\")\n","\n","plt.subplot(1,2,2)\n","plt.plot(RA_acc_tr, label='Train acc')\n","plt.plot(RA_acc_test, label='Test acc')\n","plt.legend()\n","plt.xlabel('# trees')\n","plt.ylabel('Accuracy')\n","plt.title(\"Real AdaBoost\")\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eepUAr4dSMtQ"},"source":["Analizando estas curvas en detalle, podemos observar varias cosas interesantes que los diferencian de los conjuntos por Bagging.\n","- Si repetimos el experimento el resultado es el mismo, no hay variabilidad.\n","- A diferencia del Bagging, la ganancia en el conjunto viene dada por una reducción del sesgo del error (el error sistemático) y no de la varianza. \n","- Al añadir más clasificadores al conjunto, podemos encontrarnos dos efectos:\n","  - Sobreajuste: al seguir enfatizando muestras muy erróneas (outliers) podemos llegar a forzar una frontera que las clasifique correctamente y genere sobreajuste.\n","  - Aunque el error de entrenamiento llegue al 0%, puede interesar seguir añadiendo clasificadores al conjunto porque podemos ir definiendo una solución que generalice mejor; es decir, el error de test puede mejorar aunque el de entrenamiento sea 0%.\n","\n","En estas figuras vemos como ambas tendencias se combinan y el error de test fluctúa. \n","\n","En resumen, la fortaleza de los modelos de Boosting reside en construir conjuntos con buenas capacidades predictivas conseguidas reduciendo el sesgo del error. Pero puede tender a sobreajustar, por lo que el ajuste de los parámetros del modelo (número de elementos/clasificadores) se convierte en una parte crucial para poder evitar el sobreajuste."]},{"cell_type":"markdown","metadata":{"id":"yriujcNaRqSq"},"source":["## Boosting como un algoritmo de descenso por gradiente\n","\n","El algoritmo Real AdaBoost puede reformularse como un algoritmo de construcción de características (*feature engineering*) en el que cada clasificador proporciona una característica nueva al conjunto y estas se combinan con unos pesos $\\alpha_t$ para minimizar la función de coste exponencial que minimiza el RA.\n","\n","De este modo podemos considerar que RA entrena una secuencia de clasificadores para construir unas características $o_t(\\mathbf x)$, $t=1, \\ldots, T$, de modo que la salida fina del clasificador o conjunto vienen dada por una combinación lineal de las mismas\n","\n","$$ f_T({\\bf x}) = \\displaystyle \\sum_{t=1}^T \\alpha_t o_t({\\bf x})$$\n","\n","y los pesos de esta combinación se calculan de modo que se minice la siguiente función de coste:\n","$${\\bf \\alpha}^* =\\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - \\sum_{t=1}^T \\alpha_t o_t({\\bf x}^{(i)}) {y}^{(i)} \\right) = \\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - f_T({\\bf x}^{(i)}) {y}^{(i)} \\right)$$\n","\n","Además, para el diseño de estas características, en cada iteración del conjunto, RA añade una nueva variable  $o_t(\\mathbf x)$ de modo que se consiga la mayor reducción posible de la función de coste hasta el paso $t$, es decir,\n","\n","$$ \\underset{{o_t(\\mathbf x)}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - \\sum_{t'=1}^t \\alpha_{t'} o_{t'}({\\bf x}^{(i)}) {y}^{(i)} \\right) = \\underset{{o_t(\\mathbf x)}}{\\operatorname{max}} \\sum_{i=1}^N   D_{t}({\\bf x}^{(i)}) o_t({\\bf x}^{(i)}) {y}^{(i)} $$\n","\n","Es decir, buscar el cada paso el $o_t(\\mathbf x)$ que proporcione la mayor reducción del coste equivale a maximizar una versión enfatizada de los aciertos del clasificador!!!\n","\n","Dado que el hecho de añadir  en cada paso un nuevo clasificador o *feature* que minimice la función de coste equivale a ir moviendote en la dirección de su gradiente, esta reformulación o reinterpretetación del RA entra dentro de la familia de modelos de **Gradient Boosting**. La ventaja de esta visión del problema es que podemos elegir diferentes funciones de coste para obtener diferentes modelos de Boosting. De hecho, las versiones para regresión se formulan considerando esta formulación del problema para el error cuadrático medio.\n","\n","\n","Sklearn también incluye una implementación de esta familia de modelos cuando los clasificadores base son árboles y se encuentran dentro del modelo [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier). Además, esta clase nos permite elegir entre dos funciones de coste para el diseño del conjunto: (1) la función exponencial del RA estandar y (2) la función binomial usada por los modelos de regresión logística.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vH7ImzJ7FEw-"},"source":["### Ejercicio 4\n","\n","Usa el modelo `GradientBoostingClassifier` para entrenar un conjunto de 10 árboles de decisión con profundidad 4 mediante un Gradient Boosting con función de coste exponencial (`loss= ‘exponential’`) y binomial (`loss= ‘deviance’`). Compara el resultado con la obtenida por el `AdaBoostClassifier` con `algorithm` =  `SAMME.R`."]},{"cell_type":"code","metadata":{"id":"h1L20o3zF2o5"},"source":["from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n","\n","max_depth = 4\n","T =10\n","#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tXmgpRVPDuzx"},"source":["## Libreria XGBoost \n","\n","Para la implementación de los métodos de Gradient Boosting es bastante común el uso de la [biblioteca XGBoost](https://xgboost.readthedocs.io/en/latest/index.html), ya que proporciona una implementación eficiente con capacidades de paralelización (en caso de que necesitemos trabajar con grandes conjuntos de datos). Además, tiene una [interfaz de sklearn](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) que nos permite integrarla fácilmente con nuestro código.\n","\n","Al igual que la implementación de sklearn está diseñada para trabajar con árboles de decisión, pero nos aporta algunas utilidades adicionales:\n","* Permite hacer *early_stopping*: si porporcionamos un conjunto de validación, va evaluando las prestaciones del conjunto sobre él para decidir en que momento dejar de añadir nuevos árboles (evitando efectos de sobreajuste).\n","* Permite evaluar el conjunto con un subconjunto de los árboles entrenados.\n"," \n","Veamos su funcionamiento y estas utilidades con los siguientes ejemplos:"]},{"cell_type":"markdown","metadata":{"id":"3Yi7YJzfMb2q"},"source":["### Ejemplo 1: Funcionamiento estandar"]},{"cell_type":"code","metadata":{"id":"200QCOepHgcz"},"source":["import xgboost as xgb\n","T = 10\n","clf_XGB =xgb.XGBClassifier(n_estimators= T, max_depth=4).fit(X_train, Y_train)\n","acc_ens = clf_XGB.score(X_test, Y_test)\n","\n","print('XGBoosting accuracy: %2.2f'%(acc_ens))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiVmI3DtNeiB"},"source":["### Ejemplo 2: Evaluación con solo $t$ árboles"]},{"cell_type":"code","metadata":{"id":"ZsCmcyxpOF0x"},"source":["from sklearn.metrics import accuracy_score\n","T = 10\n","clf_XGB =xgb.XGBClassifier(n_estimators= T, max_depth=4).fit(X_train, Y_train)\n","Y_pred = clf_XGB.predict(X_test, ntree_limit=1)\n","acc_ens = accuracy_score(Y_test,Y_pred)\n","print('XGBoosting accuracy: %2.2f'%(acc_ens))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5E1zHlgPCDW"},"source":["### Ejemplo 3: Validación del número de árboles"]},{"cell_type":"code","metadata":{"id":"U58Bn3xsMZqt"},"source":["X_train_sub, X_val, Y_train_sub, Y_val = train_test_split(X_train, Y_train, test_size=.4)\n","\n","T = 10\n","clf_XGB =xgb.XGBClassifier(n_estimators= T, max_depth=4)\n","clf_XGB.fit(X_train_sub, Y_train_sub, early_stopping_rounds=4, eval_metric=\"error\",\n","        eval_set=[(X_val, Y_val)])\n","\n","evals_result = clf_XGB.evals_result()\n","evals_result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJ5xPSg3KWdk"},"source":["# 4. Combinación de clasificadores o *Stacking*\n","\n","El *stacking* es otro tipo de conjuntos, en el que directamente se obtiene un conjunto a partir de la combinación de las predicciones de varios clasificadores diferentes como puede ser un K-NN, LR, SVM, Tree, .... La idea es que si estos modelos proporcionan salidas diferentes (diversidad) simplemente por el hecho de ser modelos diferentes, combinando sus salidas podremos reducir la varianza final del error.\n","\n","Hay esquemas que utilizan esta idea haciendo diferentes capas de agrupamiento donde los clasificadores se agrupan en diferentes niveles. El esquema más sencillo, y habitual, con una única capa de agregación se muestra en la siguiente figura:\n","<img align=\"center\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/BBVA/ensembles/Stacked.png\" width=80%>\n","\n","Si queremos  implementar directamente este modelo y emplear esquemas de CV utilizando herramentas como GridSearch no es sencillo definir el pipeline.  Sin embargo, la libreria [ML-Ensemble](https://mlens.readthedocs.io/en/0.1.x/) es totalmente compatible con skelarn y nos simplifica este proceso.\n","\n","En el siguiente ejemplo vemos como definir un conjunto por *stacking*  tomando como  clasificadores base `Random Forest`, `Extra Trees`, `KNeighbors`, `SVC`, `Ridge Classifier` y usando la clase [`SuperLearner`](https://mlens.readthedocs.io/en/0.1.x/source/mlens.ensemble/) de ML-Ensemble.\n"]},{"cell_type":"code","metadata":{"id":"8NdimmJxfo0N"},"source":["!pip install mlens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4WTOsvVcdNn"},"source":["from mlens.ensemble import SuperLearner\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Define ensemble model\n","ensemble = SuperLearner()\n","\n","# Define set of classifiers of the input layer and add them to the ensemble\n","learners = [KNeighborsClassifier(), DecisionTreeClassifier(), SVC()]\n","ensemble.add(learners)\n","\n","# Define output layer\n","ensemble.add_meta(LogisticRegression())\n","\n","# Train and test the model\n","ensemble.fit(X_train, Y_train)\n","Y_pred = ensemble.predict(X_test)\n","acc_ens = accuracy_score(Y_test,Y_pred)\n","\n","print('Stacked ensemble accuracy: %2.2f'%(acc_ens))"],"execution_count":null,"outputs":[]}]}