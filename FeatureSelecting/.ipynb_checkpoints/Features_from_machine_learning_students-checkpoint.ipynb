{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McTkC40Tcst3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXyR3B1-cst0"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "# Construcción de características sintéticas\n",
    "\n",
    "### Aprendizaje Automático Intermedio e Ingeniería de Características\n",
    "\n",
    "#### Febrero 2021\n",
    "\n",
    "**Emilio Parrado Hernández, Vanessa Gómez Verdejo, Pablo Martínez Olmos**\n",
    "\n",
    "Departamento de Teoría de la Señal y Comunicaciones\n",
    "\n",
    "**Universidad Carlos III de Madrid**\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkFqhwCGcst8"
   },
   "source": [
    "## Contenidos que se van a tratar a lo largo de la sesión\n",
    "\n",
    "- *Feature templates*\n",
    "    - Aplicación a características con fechas y horas\n",
    "    - Aplicación a características con información geográfica\n",
    "    \n",
    "- Construcción de características sintéticas empleando aprendizaje automático:\n",
    "    - mediante *ensembles*\n",
    "    - mediante *kernels*\n",
    "    \n",
    "- Introducción a la creación automática de características sintéticas con *Featuretools*\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96c98i1MKslH"
   },
   "source": [
    "# 1 Introducción a la construcción de características sintéticas\n",
    "\n",
    "## Transformaciones de variables en características\n",
    "\n",
    "En la primera sesión del curso presentamos transformaciones más o menos sencillas que podemos aplicar a las variables de entrada para transformarlas en características con las que poder alimentar a los algoritmos de entrenamiento de modelos de aprendizaje automático. A modo de revisión:\n",
    "\n",
    "- Variables numéricas:\n",
    "    - Transformaciones logarítmicas (`apply/np.log10)`)\n",
    "    - Discretización o cuantificación de variables continuas (`DataFrame.quantile()`, `np.digitize`)\n",
    "    - Escalados (`StandardScaler()`, `MinMaxScaler()`, `MaxAbsScaler()`)\n",
    "    - Transformaciones de histograma (`quantile_transform()`, Box-Cox)\n",
    "    - Combinaciones de características (`PolynomialFeatures()`)\n",
    "- Variables categóricas:\n",
    "    - *One-Hot Encoding* y derivados\n",
    "    - *Hashing* de características (`FeatureHasher()`)\n",
    "    - Conteos de bin\n",
    "\n",
    "Mediante estas funcionalidades básicas podemos:\n",
    "- Adaptar variables categóricas para manejar modelos numéricos\n",
    "- Adaptar rangos de variables para reducir las limitaciones matemáticas de estos modelos\n",
    "- Ayudar al modelo a capturar patrones que dependen de no-linealidades y de relaciones entre grupos de variables. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuaNkAvEV_tB"
   },
   "source": [
    "En este notebook vamos a extender estas funcionalidades con diferentes técnicas que **transformen** las características que se extraen directamente de las variables que recibimos en la definición del problema en juegos de características **orientados a mejorar la precisión del modelo** de aprendizaje automático de la etapa posterior.\n",
    "\n",
    "El empleo de características sintéticas puede **oscurecer de algún modo la interpretabilidad** detallada de los modelos, pero sí que va a contribuir (en combinación con métodos de selección de características) a ayudarnos a identificar **qué variables o combinaciones de variables son relevantes** para la definición de los patrones subyacentes al conjunto de datos y que necesitamos capturar para resolver la tarea de aprendizaje.\n",
    "\n",
    "Estas transformaciones están orientadas fundamentalmente a conseguir **representaciones alternativas** de información que **ya estaba contenida en el conjunto de entrenamiento** pero que al modelo de aprendizaje automático le cuesta recuperar por dos causas fundamentales:\n",
    "- No tiene suficiente capacidad expresiva para capturar el patrón que define el mapeo entre observaciones y targets\n",
    "- Tiene capacidad expresiva suficiente pero el conjunto de entrenamiento es corto en datos, no tiene suficientes ejemplos para ajustar todos los parámetros del modelo.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX4CwZd2V_tC"
   },
   "source": [
    "### Ejemplos\n",
    "\n",
    "#### Ejemplo 1\n",
    "\n",
    "Ajustar un modelo lineal a los datos  de la izquierda de la figura. El modelo verdadero es razonablemente suave, pero el modelo lineal no es suficientemente expresivo para capturarlo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1614846010636,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "ljdqApjYV_tC",
    "outputId": "f84d3727-a094-470f-ce95-899977da92c8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(-10,10,20)\n",
    "x = 10*np.sin(.7*t)+3*t\n",
    "ff,aa = plt.subplots(1,2,sharex=True,sharey=True, figsize=(12,6))\n",
    "aa[0].scatter(t,x,s=30,label='data')\n",
    "aa[0].set_xlabel('x')\n",
    "aa[0].set_ylabel('f(x)')\n",
    "aa[0].legend()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(t.reshape(-1,1),x)\n",
    "y_lr = lr.predict(t.reshape(-1,1))\n",
    "\n",
    "aa[1].scatter(t,x,s=30,label='data')\n",
    "aa[1].plot(t,y_lr,linewidth=2,label='linear model')\n",
    "aa[1].plot(t,x,linestyle=':',label='true model')\n",
    "\n",
    "aa[1].set_xlabel('x')\n",
    "aa[1].set_ylabel('f(x)')\n",
    "_=aa[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rYzxHlQV_tE"
   },
   "source": [
    "#### Ejemplo 2\n",
    "\n",
    "Tenemos un problema de clasificación de imágenes con datos de caras digitalizadas. Nuestra tarea es predecir la etiqueta correcta de de la tercera imagen, sabiendo únicamente que la etiqueta de la primera imagen es $+1$ y la etiqueta de la segunda es $-1$\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTERMEDIO/caras1.png' width=400 />\n",
    "\n",
    "\n",
    "Necesitamos un número **suficiente de observaciones** para saber si lo que **el modelo debe capturar es que** la tercera imagen es otra imagen del sujeto de la segunda imagen (predecir etiqueta $-1$) u otra imagen con gafas (predecir etiqueta $+1$)\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIzaLNQ_cst8"
   },
   "source": [
    "## Ejemplos sobre la idoneidad de las características\n",
    "\n",
    "En general, al decidir emplear un modelo de aprendizaje automático, **asumimos que la información que necesitamos capturar** para la construcción del modelo está contenida en los datos, y que es posible **deconstruir** las transformaciones inherentes a los datos originales hasta recuperar el patrón que estamos buscando. Usando un símil de cocina, cuando probamos algo intentamos imaginar/evocar los ingredientes originales y las transformaciones realizadas sobre éstos hasta llegar al resultado. Y asumimos que si aparece un sabor en el resultado final, es que ese ingrediente estaba en el principio del proceso: si sabe a frutos rojos, es que había frutos rojos... o las *features* que hacen que los frutos rojos sepan a frutos rojos...\n",
    "\n",
    "Vamos a ilustrar los diferentes grados de dificultad para **deconstruir** el modelo utilizando aprendizaje automático en unas tareas de regresión sencillas. La siguiente tabla describe 8 tareas de regresión que se construyen mezclando hasta 4 variables de entrada $\\{x_1, x_2, x_3, x_4\\}$ que toman valores continuos entre $0$ y $10$.\n",
    "\n",
    "| Descripción    | Expresión |  Índice    |\n",
    "| :---        |    :------------:   |          ---: |\n",
    "| Diferencia      | $x_1 - x_2$       | f1   |\n",
    "| Logaritmo   | $\\log(x_1)$        | f2      |\n",
    "| Polinomio   | $2+4x_1x_2+5 x_1^2x_2^2$        | f3      |\n",
    "| Potencia   | $x_1^2$        | f4      |\n",
    "| Cociente   | $\\frac{x_1}{x_2} $       | f5      |\n",
    "| Cociente de Diferencias   | $\\frac{x_1 - x_2}{x_3 - x_4}$        | f6   |\n",
    "| Cociente de Polinomios   | $$\\frac{1}{2+4x_1x_2+5 x_1^2x_2^2}$$        | f7     |\n",
    "| Raíz cuadrada   | $\\sqrt{x_1}$        | f8   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcyU1uDTV_tF"
   },
   "outputs": [],
   "source": [
    "# Construcción de las funciones. **Sin ruido**\n",
    "\n",
    "def f1(x):\n",
    "    return x[:,:2], x[:,0] - x[:,1]\n",
    "def f2(x):\n",
    "    return x[:,0].reshape(-1,1), np.log(x[:,0])\n",
    "def f3(x):\n",
    "    return x[:,:2], 2 + 4*x[:,0]*x[:,1] + 5*(x[:,0]*x[:,1])**2\n",
    "def f4(x):\n",
    "    return x[:,0].reshape(-1,1), x[:,0]**2\n",
    "def f5(x):\n",
    "    return x[:,:2], x[:,0] / x[:,1]\n",
    "def f6(x):\n",
    "    return x, (x[:,0] - x[:,1]) / (x[:,2] - x[:,3])\n",
    "\n",
    "def f7(x):\n",
    "    return x[:,:2], 1/(5*(x[:,0]*x[:,1])**2 + 2 + 4*x[:,0]*x[:,1])\n",
    "def f8(x):\n",
    "    return x[:,0].reshape(-1,1), x[:,0]**.5\n",
    "\n",
    "# Generar conjuntos de entrenamiento y test independientes\n",
    "xr = np.random.rand(100,4)*10\n",
    "xt = np.random.rand(100,4)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSHxBBGkV_tF"
   },
   "source": [
    "Vamos a emplear un modelo de regresión lineal sencillo [LinearRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) y una alternativa no lineal más sofisticada que es [*Kernel Ridge Regression*](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html). Lo veremos en un poco más de detalle luego, pero adelantamos que Kernel Ridge Regression aprende una combinación lineal de funciones kernel centradas en los datos de entrenamiento (parecido a la SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ8sm366V_tG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# preparar el modelo Kernel Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "v_gamma = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "params = {'gamma':v_gamma}\n",
    "grid_kr = GridSearchCV(KernelRidge(alpha=0.001,\n",
    "                                  kernel='rbf'), \n",
    "                      param_grid=params)\n",
    "\n",
    "# preparar el modelo lineal sencillo\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "executionInfo": {
     "elapsed": 3093,
     "status": "ok",
     "timestamp": 1614846015091,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "Jj0aptypV_tH",
    "outputId": "a07b3bb3-cf08-4b85-adeb-4cb468866fd3"
   },
   "outputs": [],
   "source": [
    "#Aplicar machine learning\n",
    "\n",
    "v_f = [f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "v_f_names = ['Diferencia',\n",
    "            'Logaritmo',\n",
    "            'Polinomio',\n",
    "            'Potencia',\n",
    "            'Cociente',\n",
    "            'Cociente de Diferencias',\n",
    "            'Cociente de Polinomios',\n",
    "            'Raíz cuadrada']\n",
    "av_error = np.empty(len(v_f))\n",
    "std_error = np.empty(len(v_f))\n",
    "min_error = np.empty(len(v_f))\n",
    "max_error = np.empty(len(v_f))\n",
    "l_av_error = np.empty(len(v_f))\n",
    "l_std_error = np.empty(len(v_f))\n",
    "l_min_error = np.empty(len(v_f))\n",
    "l_max_error = np.empty(len(v_f))\n",
    "for iff,ff in enumerate(v_f):\n",
    "    x1, y1 = ff(xr)\n",
    "    x2, y2 = ff(xt)\n",
    "    grid_kr.fit(x1, y1)\n",
    "    er = np.absolute(y2 - grid_kr.predict(x2))\n",
    "    av_error[iff] = np.mean(er)\n",
    "    std_error[iff] = np.std(er)\n",
    "    min_error[iff] =np.min(er)\n",
    "    max_error[iff] =np.max(er)\n",
    "    lr.fit(x1, y1)\n",
    "    ler = np.absolute(y2 - lr.predict(x2))\n",
    "    l_av_error[iff] = np.mean(ler)\n",
    "    l_std_error[iff] = np.std(ler)\n",
    "    l_min_error[iff] =np.min(ler)\n",
    "    l_max_error[iff] =np.max(ler)\n",
    "    # en los problemas 1D podemos representar gráficamente\n",
    "    if x1.shape[1]==1:\n",
    "        xs = np.sort(np.squeeze(x1))\n",
    "        plt.figure()\n",
    "        plt.scatter(x1,y1,label='data')\n",
    "        plt.plot(xs, lr.predict(xs.reshape(-1,1)), label='linear')\n",
    "        plt.plot(xs, grid_kr.predict(xs.reshape(-1,1)), label='kernel')\n",
    "        plt.legend()\n",
    "        plt.title(v_f_names[iff])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or_XUtibV_tH"
   },
   "source": [
    "La siguiente tabla muestra lo bien o mal que cada uno de los dos modelos has sido capaz de capturar los patrones incluidos en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 1652,
     "status": "ok",
     "timestamp": 1614846015320,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "_o9MyJvpV_tI",
    "outputId": "7b0b7938-e32b-40a8-8d79-afc6a75015bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "resultados = pd.DataFrame()\n",
    "resultados['función'] = v_f_names\n",
    "resultados['Linear Mean Error'] = l_av_error\n",
    "resultados['Linear StdDev Error'] = l_std_error\n",
    "resultados['K Ridge Mean Error'] = av_error\n",
    "resultados['K Ridge StdDev Error'] = std_error\n",
    "\n",
    "for cc in resultados.columns[1:]:\n",
    "    resultados[cc] = resultados[cc].apply(lambda x:np.round(x,2))\n",
    "\n",
    "display(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zJbfzeXcst9"
   },
   "source": [
    "# 2. Plantillas de características (*feature templates*)\n",
    "\n",
    "Esta sección **extiende las transformaciones de características** que vimos en la primera clase mediante la definición de automatizaciones que nos permitan generar características de modo masivo y sencillo a partir de los datos de entrada. \n",
    "\n",
    "El primer paso para abordar la resolución de un problema mediante aprendizaje automático es la **elección del modelo**. Aquí elegiremos si queremos un modelo lineal, polinómico, SVM, red neuronal, etc. Cuando instanciamos un modelo, por ejemplo \n",
    "~~~\n",
    "modelo = sklearn.svm.SVC(kernel='rbf', C=10, gamma=1)\n",
    "~~~\n",
    "\n",
    "estamos creando un objeto software con un potencial conjunto de parámetros libres cuyo valor se optimizará durante el entrenamiento del modelo.\n",
    "\n",
    "Una vez elegido el modelo, es necesario aplicar **ingeniería de características** para obtener las características con las que se va a alimentar el modelo. Elegir las características nos **determina la matriz de datos** $X$ donde cada observación es una fila y las columnas vienen dadas por esas características. Cambiar de características implica cambiar las columnas de esta matriz. El modelo sólo va a poder explotar información contenida en la matriz de datos que definen las características calculadas. Por lo tanto, la **elección de las características** que definen el problema  puede entenderse como una **acotación del espacio de modelos**. El objeto `sklearn.svm.SVC` que hemos creado antes puede aprender potencialmente infintas SVMs (los vectores soporte pueden ser cualquier cosa), eso sí todas ellas con *kernels* RBF con `gamma=1` y con coeficientes $0\\le \\alpha\\le 10$. Pero una vez que definimos las características y el conjunto de entrenamiento, acotamos ese conjunto a combinaciones lineales de *kernels* centrados en las observaciones del conjunto de entrenamiento, y estas observaciones están definidas con las características.\n",
    "\n",
    "Finalmente se invoca el método `fit()` correspondiente para que encuentre el modelo óptimo que se devolverá como solución para resolver el problema de tratamiento de datos. Este paso, que es lo que habitualmente se conoce como **aprendizaje** consiste en fijar los parámetros libres del modelo para acomodar la participación de cada una de las características en la definición del modelo concreto. El método `fit()` **no puede buscar soluciones fuera** del espacio de búsqueda delimitado por el modelo elegido y por las características calculadas.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTERMEDIO/fe_templates1.png' width=800 />\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seZBLlRBKslI"
   },
   "source": [
    "## 2.1 Ejemplo para motivar: clasificador de direcciones de *email*\n",
    "\n",
    "Vamos a pensar en un modelo que reciba como entrada una cadena de texto y su salida tenga que ser $+1$ si la cadena es una dirección de *email* válida y $-1$ si no es una dirección de *email* válida.\n",
    "\n",
    "Además, especificamos que el modelo sea **lineal** por su sencillez de manejo y para poder interpretar el impacto de cada característica en la clasificación final.\n",
    "\n",
    "$$\n",
    "x=\\mbox{'hola@tumail.com'} \\rightarrow \\boldsymbol \\phi(x) \\rightarrow f_{\\mathbf w}(x) =  \\mbox{signo}\\left(\\mathbf w^\\top \\boldsymbol \\phi(x) \\right) \\rightarrow \\left\\{ \\begin{array}{ll} +1 & \\mbox{si } x \\mbox{ es un email válido} \\\\ -1 & \\mbox{en cualquier otro caso} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Los pasos que necesitamos completar hasta obtener el modelo son:\n",
    "1. Extraer características numéricas que **parametricen** (capturen información relevante) de cada dirección de correo \n",
    "2. Aprender los parámetros del modelo (vector de pesos y término de sesgo o `intercept_`)\n",
    "3. Una vez que el modelo esté aprendido podremos hacer **inferencia**, es decir, usarlo para predecir la clase correcta de las observaciones **de test**.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE81V1T7V_tJ"
   },
   "source": [
    "### Extractor de características\n",
    "\n",
    "El paso donde ponemos el foco en este curso es en el diseño de **extractor de características**. Este módulo debe construir $\\boldsymbol \\phi(x)$ a partir de la cadena de texto $x$. Algunas características que podrían tener interés:\n",
    "\n",
    "$$\n",
    "x=\\mbox{hola@tumail.com} \\rightarrow \\boldsymbol \\phi(x) = \\left [ \\begin{array}{ll} \\mbox{len}(x)>12  & : 1 \\\\ \\mbox{RatioAlfanumerico}(x) & : 13/15 \\\\\n",
    "\\mbox{Contiene@} & : 1 \\\\ \\mbox{TerminaEn_com} & : 1 \\\\ \\mbox{TerminaEn_org} & : 0 \\end{array}\\right]\n",
    "$$\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqPaMD3bKslI"
   },
   "source": [
    "### Características *ad hoc*\n",
    "\n",
    "En el ejemplo anterior todos estábamos de acuerdo en que esas 5 características podían ser una buena representación para resolver el problema con un clasificador lineal. \n",
    "\n",
    "En este sentido esperaríamos que el peso de la característica $\\mbox{len}(x)>12$ fuese negativo (las direcciones de correo no suelen tener tantos caracteres, los pesos de $\\mbox{Contiene@}$, $\\mbox{TerminaEn_com}$ y $\\mbox{TerminaEn_org}$ fuesen positivos. Y en general obtener una tasa de aciertos aceptables con ese clasificador lineal.\n",
    "\n",
    "Sin embargo, estas características se han definido de un modo **muy ad hoc** para el problema, usando información de contexto (nuestra experiencia de años intercambiando correos electrónicos). Este procedimiento sería difícil de automatizar y exportar a dominios en los que el *data scientist* no tenga tanto conocimiento del problema a la hora de encontrar características que tengan sentido para el modelo.\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRxOC5k-V_tK"
   },
   "source": [
    "## 2.2 Plantillas de características (*feature templates*)\n",
    "\n",
    "Una **plantilla de características** agrupa características que se obtienen de un modo similar. En el ejemplo que estamos viendo, las características  $\\mbox{TerminaEn_com}$ y $\\mbox{TerminaEn_org}$ se calculan del mismo modo: mirando a los 3 últimos caracteres de la cadena de texto de entrada.\n",
    "\n",
    "Una **plantilla de características** define **tipos de patrones a buscar** no patrones concretos:\n",
    "\n",
    "$$\n",
    "x=\\mbox{hola@tumail.com} \\rightarrow  \\left [ \\begin{array}{ll} \\mbox{TerminaEn_aaa} & : 0 \\\\  \\mbox{TerminaEn_aab} & : 0 \\\\ \\cdots & \\\\ \\mbox{TerminaEn_com} & : 1 \\\\ \\cdots & \\\\ \\mbox{TerminaEn_org} & : 0 \\\\ \\cdots & \\\\ \\mbox{TerminaEn_zzz} & : 0\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "No necesitamos saber qué patrones específicos serán útiles (esto se lo dejamos al algoritmo de aprendizaje), sólo que **existen ciertos patrones** que merece la pena explorar.\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4xd0JWCV_tK"
   },
   "source": [
    "### Ejemplos de plantillas de características\n",
    "\n",
    "Entrada:\n",
    "$$\n",
    "x=\\mbox{hola@tumail.com}\n",
    "$$\n",
    "\n",
    "| Plantilla    | Ejemplo |  valor    |\n",
    "| :---        |    :------------   |          ---: |\n",
    "| Los últimos tres caracteres son *---*      | Los últimos tres caracteres son *com*      | 1   |\n",
    "| Longitud de la cadena mayor que ---      | Longitud de la cadena mayor que 12      | 1   |\n",
    "| Fracción de caracteres alfanuméricos     | Fracción de caracteres alfanuméricos       | 13/15   |\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6D2Su63V_tK"
   },
   "source": [
    "### Dispersión de los pesos del modelo lineal\n",
    "\n",
    "El empleo de plantillas de características trae como consecuencia el aumento del número de estas características hasta órdenes de magnitud considerables. Por ejemplo la plantilla sobre los 3 últimos caracteres tiene variaciones (se pueden repetir) de 26 elementos tomados de 3 en 3: $26^3=17576$.\n",
    "\n",
    "Estas técnicas deben combinarse con:\n",
    "- una codificación eficiente de la información, por ejemplo usando **diccionarios** de python\n",
    "- un algoritmo de entrenamiento que incluya la **selección de características** (como los métodos de *embedded feature selection* que vimos en la sesión anterior), o bien un *pipeline* que combine el aprendizaje del modelo con la selección de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpWPuqtvV_tK"
   },
   "source": [
    "## 2.3 DictVectorizer, generar *one-hot encoded features* desde diccionarios\n",
    "\n",
    "La funcionalidad de sklearn [`DictVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) precisamente genera una matriz de características *one-hot encoded* a partir de variables de entrada que vienen definidas en modo disperso mediante diccionarios. \n",
    "\n",
    "El resultado de la aplicación de *feature templates* cuando el número  de características sintéticas generadas es grande va a ser una codificación dispersa en la que cada observación está definida por un diccionario donde las claves son las características que sí están presentes en la observación. En el ejemplo de las direcciones de *email*\n",
    "\n",
    "| x    | observación   |\n",
    "| :---        |    :------------:   |\n",
    "|`\"hola@tumail.com\" ` |`{\"len(x)>12\" : True,  \"Contiene@\" : True, \"TerminaEn_com\": True }`|\n",
    "|`\"@john_rambo\"`| `{\"len(x)>12\" : False,  \"Contiene@\" : True, \"TerminaEn_mbo\": True }`|\n",
    "\n",
    "\n",
    "La salida de `DictVectorizer` sería la siguiente\n",
    "\n",
    "| \"len(x)>12\" |  Contiene@  | ... | TerminaEn_com |  ...    |TerminaEn_mbo | ... |\n",
    "|    :------------:   | :------------:   | :------------:   | :------------:   | :------------:   | :------------:   |          ---: |\n",
    "| 1       | 1   | ... | 1 | ... | 0 | ... |\n",
    "| 1       | 1   | ... | 0 | ... | 1 | ... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P4LDyUeKslL"
   },
   "source": [
    "### Ejemplo de uso de DictVectorizer con la base de datos YELP academic\n",
    "\n",
    "Volvemos a cargar la base de datos [Yelp_academic_dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset) que usamos en el primer notebook y aplicamos los siguientes procesamientos\n",
    "\n",
    "- codificar en escala logarítmica el conteo de revisiones de cada negocio\n",
    "- Eliminar todos los negocios que no sean \"Restaurants\"\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPYDc3r-KslM"
   },
   "outputs": [],
   "source": [
    "def get_category(df, category='Restaurants'):\n",
    "    categories_ = df[\"categories\"].dropna()\n",
    "    is_member = categories_[categories_.apply(lambda x:category in x.split(\", \"))]\n",
    "    return df.loc[is_member.index].copy()\n",
    "\n",
    "\n",
    "import pandas as pd # organize data in dataframes\n",
    "import json # read the json file with data\n",
    "from zipfile import ZipFile \n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "\n",
    "url = urllib.request.urlopen(\"http://www.tsc.uc3m.es/~emipar/BBVA/INTERMEDIO/DATASETS/yelp_academic_dataset_business.json.zip\")\n",
    "contained_file = 'yelp_academic_dataset_business.json'\n",
    "with ZipFile(BytesIO(url.read())) as my_zip_file:\n",
    "    data_yelp_file = my_zip_file.open(contained_file)\n",
    "    data_yelp_df = pd.DataFrame([json.loads(x) for x in data_yelp_file.readlines()])\n",
    "    \n",
    "\n",
    "# log de conteo de revisiones    \n",
    "    \n",
    "data_yelp_df['log10_review_count'] = data_yelp_df['review_count'].apply(np.log10)\n",
    "    \n",
    "# quedarnos solo con restaurantes\n",
    "    \n",
    "restaurantes = get_category(data_yelp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVWvAuCMV_tL"
   },
   "source": [
    "Recordamos que la tarea que estamos tratando de resolver es predecir el *rating* de los negocios. Vamos a intentar enriquecer las predicciones con características extraídas del campo `attributes`. Representamos el campo `attributes` de uno de los registros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67338,
     "status": "ok",
     "timestamp": 1614846089715,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "blB5-lGcV_tL",
    "outputId": "565d0e3c-97e8-4809-b68f-0c2f083b7347"
   },
   "outputs": [],
   "source": [
    "restaurantes.iloc[23]['attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBgk-bpJV_tL"
   },
   "source": [
    "Tenemos una estructura de diccionarios de 2 niveles: alguno de los valores del diccionario `attributes` es a su vez un diccionario. El siguiente código *aplana* esta estructura de diccionarios en un diccionario con un único nivel.\n",
    "\n",
    "Notad que el diccionario *interno* está representado como una cadena de caracteres, es decir, **no es un diccionario de python** sino en código que escribiríamos para definir el diccionario. Esto es particular de esta base de datos, es decir, que (hasta donde yo sé) no hay un estándar para codificar esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mJmHovAV_tM"
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    # d: diccionario donde alguno de los valores puede ser otro diccionario\n",
    "    # ojo esta funcion solo vale para 1 nivel de anidamiento!!\n",
    "    \n",
    "    # Lo primero es devolver un diccionario {'empty_attributes':True} \n",
    "    # para todos aquellos restaurantes en los que el campo *attributes* está vacío\n",
    "    # \n",
    "    if d is None:\n",
    "        result= {'empty_attributes':True}\n",
    "    else:\n",
    "        result={} # diccionario aplanado que devolvemos\n",
    "        for k,v in d.items(): \n",
    "            if \":\" in v: # detectar que el valor es una cadena de caracteres codificando un diccionario en python\n",
    "                d1 = eval(v) # construir un diccionario d1 con esa cadena de caracteres\n",
    "                # Eliminamos los items que tengan value None\n",
    "                d1 = {clave:valor for clave, valor in d1.items() if valor is not None}\n",
    "                # las claves en el diccionario \"aplanado\" se construyen concatenando la clave del diccionario\n",
    "                # externo (k) con la clave del diccionario interno (k1) mediante un \"_\"\n",
    "                \n",
    "                for k1,v1 in d1.items():\n",
    "                    result.update({k+\"_\"+k1:v1})\n",
    "            else: \n",
    "                # el valor no es un diccionario. Detectamos si el valor es en realidad una cadena\n",
    "                # codificando un booleano (esto es particular de este conjunto de datos) y lo transformamos en \n",
    "                # booleano, si aplica. Si no, pues devolvemos el valor en modo transparente\n",
    "                if (v=='True') or (v=='true'):\n",
    "                    result.update({k:True})\n",
    "                elif (v=='False') or (v=='false'):\n",
    "                    result.update({k:False})\n",
    "                elif (v=='None') or (v=='none') or (v is None) or (v==\"u'None'\") or (v==\"u'none'\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    result.update({k:v})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nJvyqhxV_tM"
   },
   "outputs": [],
   "source": [
    "restaurantes['flat_attributes'] = restaurantes['attributes'].apply(lambda x:flatten_dict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66796,
     "status": "ok",
     "timestamp": 1614846094032,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "UQ0OVy7UV_tM",
    "outputId": "5ebf0537-38bf-422f-9226-3eaacbbf4445"
   },
   "outputs": [],
   "source": [
    "print(restaurantes.index[23])\n",
    "restaurantes.iloc[23]['flat_attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h80qjj8mV_tN"
   },
   "source": [
    "Ahora que el campo `attributes` está aplanado podemos aplicar `DictVectorizer` para codificar la información contenida en este campo. \n",
    "\n",
    "En este apartado notad que, para simplificar el análisis, estoy **haciendo trampas** porque voy a aprender el transformador con todos los datos, es decir, usando el test también. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rflrz2g4V_tO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "v.fit(restaurantes['flat_attributes'].values.tolist())\n",
    "atributos = v.transform(restaurantes['flat_attributes'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63684,
     "status": "ok",
     "timestamp": 1614846095553,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "D2kEFEdPV_tO",
    "outputId": "4a7b197b-38a5-46e7-9c6e-83aa9b7e62c9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa=pd.Series(atributos[0,:], index=v.feature_names_)\n",
    "for ii in aa.index:\n",
    "    print(\"{0}: {1}\".format(ii, aa[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LlbeHP7xBNL"
   },
   "source": [
    "Vemos que en algunos casos sería conveniente realizar un preprocesamiento que unificase claves como `Alcohol='beer_and_wine'` y `Alcohol=u'beer_and_wine'` o `Alcohol='none'`, `Alcohol=None` y `Alcohol=u'none'`, pero lo dejamos como ejercicio..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXOW_SeBV_tO"
   },
   "source": [
    "#### División entre entrenamiento y test\n",
    "\n",
    "Para evaluar la capacidad de generalización de los modelos que construyamos podemos dividir la base de datos en dos particiones, entrenamiento y test, al 50% cada una y aplicamos la transformación del `DictVectorizer` a las dos particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLO4CXCOV_tO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "restaurantes_train, restaurantes_test = train_test_split(restaurantes, \n",
    "                                                         test_size=0.5, \n",
    "                                                         random_state=42)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38cW4StNV_tO"
   },
   "outputs": [],
   "source": [
    "attr_train = v.transform(restaurantes_train['flat_attributes'].values)\n",
    "attr_test = v.transform(restaurantes_test['flat_attributes'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpMpItcBV_tP"
   },
   "source": [
    "Aprendemos un modelo lineal con los logaritmos de los conteos de revisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr4EVQeyV_tP"
   },
   "outputs": [],
   "source": [
    "import numpy as np # to get access to math functionalities\n",
    "from sklearn.linear_model import Ridge\n",
    "# Training set observations\n",
    "X_train = restaurantes_train['log10_review_count'].values.reshape(-1,1) # column vector, 1 feature and many rows\n",
    "# Training set targets\n",
    "Y_train = restaurantes_train['stars'].values\n",
    "# Test set observations\n",
    "X_test = restaurantes_test['log10_review_count'].values.reshape(-1,1)\n",
    "# Test set targets\n",
    "Y_test = restaurantes_test['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43472,
     "status": "ok",
     "timestamp": 1614846097285,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "YhQvGYyuKslP",
    "outputId": "17e16b2a-5d91-4bf0-ed2e-b4e4d8a2e332"
   },
   "outputs": [],
   "source": [
    "linear_model = Ridge(alpha=0.001).fit(X_train, Y_train)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJF-Nw36V_tP"
   },
   "source": [
    "Ahora enriquecemos el modelo con la salida de `DictVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDrgFZ1zU4Ei"
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, attr_train))\n",
    "X_test = np.hstack((X_test, attr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "error",
     "timestamp": 1614846111461,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "wZnr4DM47IPP",
    "outputId": "2bb5d3b9-413a-4abd-a9e6-6e71abe55ec8"
   },
   "outputs": [],
   "source": [
    "linear_model_e = Ridge(alpha=0.001).fit(X_train, Y_train)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model_e.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model_e.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPBBTaUPV_tQ"
   },
   "source": [
    "Significativa mejora! Si imprimimos los pesos podemos discutir el impacto de alguna de las nuevas variables en la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1614837499971,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "phmMokRaKslS",
    "outputId": "192b2117-7106-46a8-e4c2-bfd904b2e97a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"w0 = {0:.4f}\".format(linear_model.intercept_))\n",
    "nombre_feature = ['log10_review_count']\n",
    "nombre_feature += v.get_feature_names()\n",
    "pesos = linear_model.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1614837501406,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "_u1UQ1BeV_tR",
    "outputId": "76440d5f-5d97-43c9-f15f-9ab55a1afc74",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"w0 = {0:.4f}\".format(linear_model_e.intercept_))\n",
    "nombre_feature = ['log10_review_count']\n",
    "nombre_feature += v.get_feature_names()\n",
    "pesos = linear_model_e.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvHkzsjfKslW"
   },
   "source": [
    "## 2.4 Feature templates para capturar información geográfica\n",
    "\n",
    "Los restaurantes a veces aparecen en los mismo barrios, y puede que restaurantes que están cerca tengan *ratings* parecidos. \n",
    "\n",
    "En la base de datos tenemos la **latitud** y la **longitud** de cada establecimiento, y podemos emplear estas dos variables para encontrar los restaurantes que estén más cerca. Si os acordáis en la primera sesión recurrimos a un modelo basado en kNN para capturar los patrones que dependiesen de la información geográfica porque no tiene sentido pensar que el rating va a ser proporcional a la latitud, o al cuadrado de la longitud, que es lo que podríamos haber capturado con los modelos lineales y las técnicas vistas hasta ahora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMVOmVGhzc_j"
   },
   "source": [
    "En esta sección vamos a usar *feature templates* para introducir información sobre la geolocalización del restaurante en el modelo lineal. Las *templates* van a ser celdas rectangulares definidas mediante un intervalo de latitudes y otro de longitudes de distintos tamaños.\n",
    "\n",
    "Para ello vamos a restringirnos a negocios que estén en las proximidades de Toronto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yg95KmmJV_tS"
   },
   "outputs": [],
   "source": [
    "restaurantes_toronto = restaurantes.loc[restaurantes['city']=='Toronto',:]\n",
    "restaurantes_train, restaurantes_test = train_test_split(restaurantes_toronto, \n",
    "                                                         test_size=0.5, \n",
    "                                                         random_state=42)\n",
    "xy_toronto = restaurantes_toronto.loc[:,['longitude','latitude']].values\n",
    "stars = restaurantes_toronto['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPbRTXuuKslW",
    "outputId": "10e220a2-a69f-4e8a-e70f-3e45fc681002"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for ss in np.unique(stars):\n",
    "    plt.scatter(xy_toronto[stars==ss,0], xy_toronto[stars==ss,1],marker='.',alpha=0.2,label=repr(ss))\n",
    "plt.legend()\n",
    "plt.xlabel('Longitud (º)')\n",
    "_=plt.ylabel('Latitud (º)')\n",
    "_=plt.title('Restaurantes de Toronto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjyTsMFKznX_"
   },
   "source": [
    "En primer lugar recordamos los resultados que obterndríamos usando kNN como en la sesión de introducción del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2711,
     "status": "ok",
     "timestamp": 1614837618696,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "_C9ArvoV0SQF",
    "outputId": "892a0436-ff21-4679-d999-58ef202f2154"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train = restaurantes_train.loc[:,['latitude','longitude','log10_review_count']].values\n",
    "Y_train = restaurantes_train['stars'].values\n",
    "X_test = restaurantes_test.loc[:,['latitude','longitude','log10_review_count']].values\n",
    "Y_test = restaurantes_test['stars'].values\n",
    "\n",
    "v_k = [1,5,10,25,50,100,250,500] # some values for the number of neighbours\n",
    "v_weights = ['uniform','distance'] # combination of each neighbour vote\n",
    "knn_params = {'n_neighbors':v_k,\n",
    "             'weights':v_weights}\n",
    "grid_knn = GridSearchCV(KNeighborsRegressor(), param_grid=knn_params, cv=5)\n",
    "grid_knn.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(grid_knn.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(grid_knn.score(X_test, Y_test)))\n",
    "print(\"Hyperparameters choosen with cross-validation\")\n",
    "print(grid_knn.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rTLCSoXV_tT"
   },
   "source": [
    "El primer paso para incorporar la información de localización al modelo lineal consiste en dividir la ciudad en celdas de diferentes tamaños para ganar intuiciones acerca de los tamaños de celda que pueden valernos. Para esta división nos apoyamos en [`KBinsDiscretizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html), que nos divide un rango continuo en bins siguiendo diferentes estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-WYHx_hKslW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "executionInfo": {
     "elapsed": 3419,
     "status": "ok",
     "timestamp": 1614837684639,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "N0xb-ksjV_tT",
    "outputId": "3af6f3cb-ab4f-4d4c-855b-9d88cd89fabc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nbins = [4,8,16,32] # numeros de celda por cada dimensión\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "kk = 0\n",
    "ff,aa = plt.subplots(nrows, ncols,sharex=True, sharey=True,figsize=(12,12))\n",
    "for rr in range(nrows):\n",
    "    for cc in range(ncols):\n",
    "        ax = aa[rr][cc]\n",
    "        for ss in np.unique(stars):\n",
    "            ax.scatter(xy_toronto[stars==ss,0], xy_toronto[stars==ss,1],marker='.',alpha=0.2,label=repr(ss))\n",
    "        n_bins = nbins[kk]\n",
    "        # partir el mapa de Toronto en n_bins x n_bins celdas usando el conjunto de entrenamiento\n",
    "        # strategy=uniform quiere decir que las celdas sean igual de grandes en área\n",
    "        celdas = KBinsDiscretizer(n_bins=n_bins,\n",
    "                                encode='ordinal',\n",
    "                 strategy='uniform')\n",
    "        celdas.fit(restaurantes_train[['longitude','latitude']].values)\n",
    "        for bb in range(len(celdas.bin_edges_[0])):\n",
    "            ax.plot([celdas.bin_edges_[0][bb], celdas.bin_edges_[0][bb]],\n",
    "                   [celdas.bin_edges_[1][0], celdas.bin_edges_[1][-1]],\n",
    "                   color='black',linestyle=':', linewidth=.5, alpha=0.5)\n",
    "            ax.plot([celdas.bin_edges_[0][0], celdas.bin_edges_[0][-1]],\n",
    "                    [celdas.bin_edges_[1][bb], celdas.bin_edges_[1][bb]],\n",
    "                   color='black',linestyle=':', linewidth=.5, alpha=0.5)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title('Toronto en resolución {0:d}$\\\\times${0:d}'.format(n_bins))\n",
    "        kk += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N_2i1MsV_tU"
   },
   "source": [
    "La aplicación de *feature templates* tiene que programarse *ad hoc* para cada tarea. El siguiente código se apoya en `KBinsDiscretizer` para automatizar la generación de las rejillas y codificar cada restaurante con un diccionario que tiene `True` en los valores de las celdas que lo contienen. Las celdas que no lo contienen no se le añaden al diccionario para llegar a una codificación eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jto7ORqV_tU"
   },
   "outputs": [],
   "source": [
    "class geo_feature_template(object):\n",
    "    def __init__(self, \n",
    "                 n_bins = 4, \n",
    "                 encode='ordinal',\n",
    "                 strategy='uniform'):\n",
    "        self.kb = KBinsDiscretizer(n_bins=n_bins,\n",
    "                                   encode=encode, \n",
    "                                   strategy=strategy)\n",
    "    def fit(self, x):\n",
    "        # aprende los límites de cada rejilla con los datos en x\n",
    "        self.kb.fit(x)\n",
    "        self.bin_edges_ = self.kb.bin_edges_\n",
    "        \n",
    "    def transform(self, x):\n",
    "        # por cada fila de x devuelve un diccionario disperso donde la clave codifica la posición de la celda\n",
    "        # y el valor es True porque el restaurante está dentro de la celda\n",
    "        n = x.shape[0]\n",
    "        d_list = []\n",
    "        bins = self.kb.transform(x)\n",
    "        for bb in bins:\n",
    "            b0 = int(bb[0])\n",
    "            b1 = int(bb[1])\n",
    "            key = \"latitude_{0:.3f}_{1:.3f}_\".format(self.bin_edges_[0][b0],\n",
    "                                               self.bin_edges_[0][b0+1])\n",
    "            key+=\"longitude_{0:.3f}_{1:.3f}\".format(self.bin_edges_[1][b1],\n",
    "                                               self.bin_edges_[1][b1+1])\n",
    "            value = True\n",
    "            d_list.append({key:value})\n",
    "        return np.array(d_list)\n",
    "    def fit_transform(self, x):\n",
    "        self.kb.fit(x)\n",
    "        self.bin_edges_ = self.kb.bin_edges_\n",
    "        return self.transform(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFiYO3EOV_tU"
   },
   "source": [
    "La siguiente función fusiona diccionarios con rejillas de diferente resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN0N6X9iV_tU"
   },
   "outputs": [],
   "source": [
    "def merge_dicts_in_list(ld1, ld2):\n",
    "    # ld1: lista de diccionarios (uno por cada restaurante) \n",
    "    # ld2: lista de diccionarios (uno por cada restaurante) \n",
    "    # las listas deben estar alineadas!! es decir, el elemento i de cada lista debe ser el mismo restaurante!!\n",
    "    for ii, dd in enumerate(ld1):\n",
    "        dd.update(ld2[ii])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNiim-JdV_tU"
   },
   "outputs": [],
   "source": [
    "gft = geo_feature_template(n_bins=128)\n",
    "geo_train = gft.fit_transform(restaurantes_train[['longitude','latitude']].values)\n",
    "geo_test = gft.transform(restaurantes_test[['longitude','latitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1614837701892,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "WGNsN8jlV_tU",
    "outputId": "30bc4268-b4f4-4597-ffdc-abd46fee9b2c"
   },
   "outputs": [],
   "source": [
    "restaurantes_train[['longitude','latitude']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1614837702195,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "QgYSHFRBV_tV",
    "outputId": "8654e312-c0d7-4234-d4c8-f261fe72cf87"
   },
   "outputs": [],
   "source": [
    "geo_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEu8p-wrV_tV"
   },
   "source": [
    "El siguiente código genera las rejillas $64\\times 64$, $32\\times 32$ y  $16\\times 16$ y las añade al diccionario que tiene la rejilla $128\\times 128$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCybGKmlV_tV"
   },
   "outputs": [],
   "source": [
    "for rr in [64,32,16]:\n",
    "    mi_geo_transf = geo_feature_template(n_bins=rr)\n",
    "    mi_geo = mi_geo_transf.fit_transform(restaurantes_train[['longitude','latitude']].values)\n",
    "    merge_dicts_in_list(geo_train, mi_geo)\n",
    "    merge_dicts_in_list(geo_test, mi_geo_transf.transform(restaurantes_test[['longitude','latitude']].values))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1614837705843,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "MVP50hdBV_tV",
    "outputId": "b1fcccef-e1c9-4b45-92d4-f3a1f0d35ad9"
   },
   "outputs": [],
   "source": [
    "geo_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6bi9NCV_tW"
   },
   "source": [
    "Aplicamos `DictVectorizer` para codificar *one-hot* la pertenencia de cada restaurante a cada celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIDVAFiIKslW"
   },
   "outputs": [],
   "source": [
    "gg = DictVectorizer(sparse=False)\n",
    "casillas_train = gg.fit_transform(geo_train)\n",
    "casillas_test = gg.transform(geo_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEAOewFRV_tW"
   },
   "source": [
    "Modelo lineal solo con conteos de revisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdN3SBo_V_tX"
   },
   "outputs": [],
   "source": [
    "X_train = restaurantes_train['log10_review_count'].values.reshape(-1,1) # column vector, 1 feature and many rows\n",
    "# Training set targets\n",
    "Y_train = restaurantes_train['stars'].values\n",
    "# Test set observations\n",
    "X_test = restaurantes_test['log10_review_count'].values.reshape(-1,1)\n",
    "# Test set targets\n",
    "Y_test = restaurantes_test['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1614837710559,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "n3pSol0nV_tX",
    "outputId": "13047b21-1c64-418a-82b3-8eab59d6f18c"
   },
   "outputs": [],
   "source": [
    "linear_model = Ridge(alpha=0.001).fit(X_train, Y_train)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C7TFmzWV_tX"
   },
   "source": [
    "Añadiendo la localización al modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yaSQ-tNV_tX"
   },
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(np.hstack((X_train, casillas_train)))\n",
    "X_test = np.nan_to_num(np.hstack((X_test, casillas_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24032,
     "status": "ok",
     "timestamp": 1614837789016,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "BiwcK_9xKslX",
    "outputId": "7486d636-acb2-48a8-ca5d-e94872598952",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_alpha = [.1, 1, 10, 100, 1000]\n",
    "grid_lr = GridSearchCV(Ridge(), param_grid={'alpha':v_alpha})\n",
    "grid_lr.fit(X_train, Y_train)\n",
    "linear_model = grid_lr.best_estimator_\n",
    "print(grid_lr.best_params_)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model.score(X_test, Y_test)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"w0 = {0:.4f}\".format(linear_model.intercept_))\n",
    "nombre_feature = ['log10_review_count']\n",
    "nombre_feature += gg.get_feature_names()\n",
    "pesos = linear_model.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden[:20]:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oAN0rdZV_tY"
   },
   "source": [
    "La precisión en términos de $R^2$ mejora respecto de no usar información de localización y con respecto al modelo basado en kNN. Podemos analizar en más detalle las celdas que obtienen los pesos más altos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1614837833202,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "GuCCzunFV_tY",
    "outputId": "1f38b746-f14a-45ca-c8c7-abd3a2f1b9bc"
   },
   "outputs": [],
   "source": [
    "orden[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2925,
     "status": "ok",
     "timestamp": 1614837835956,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "EeUbzONcV_tY",
    "outputId": "b64a9c81-8a09-4ca7-b8b8-e8237393112f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for oo in orden[1:5]:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(restaurantes_train['longitude'].values,\n",
    "               restaurantes_train['latitude'].values, marker='.',alpha=0.2)\n",
    "    x1 = float(nombre_feature[oo].split(\"_\")[1])\n",
    "    x2 = float(nombre_feature[oo].split(\"_\")[2])\n",
    "    y1 = float(nombre_feature[oo].split(\"_\")[4])\n",
    "    y2 = float(nombre_feature[oo].split(\"_\")[5])\n",
    "    plt.plot([x1, x1],\n",
    "                   [gft.bin_edges_[1][0], gft.bin_edges_[1][-1]],\n",
    "                   color='black',linestyle=':', linewidth=1, alpha=0.5)\n",
    "    plt.plot([celdas.bin_edges_[0][0], celdas.bin_edges_[0][-1]],\n",
    "                    [y1, y1],color='black',linestyle=':', linewidth=1, alpha=0.5)\n",
    "    plt.plot([x2, x2],\n",
    "                   [gft.bin_edges_[1][0], gft.bin_edges_[1][-1]],\n",
    "                   color='black',linestyle=':', linewidth=1, alpha=0.5)\n",
    "    plt.plot([celdas.bin_edges_[0][0], celdas.bin_edges_[0][-1]],\n",
    "                    [y2, y2],color='black',linestyle=':', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    cuales = np.where(X_train[:,oo]==1)[0]\n",
    "    rr_df = restaurantes_train.iloc[cuales]\n",
    "    mi_Stars = rr_df['stars'].values\n",
    "    for ss in np.unique(mi_Stars):\n",
    "        plt.scatter(rr_df.loc[rr_df['stars']==ss,'longitude'],\n",
    "                rr_df.loc[rr_df['stars']==ss,'latitude'],\n",
    "                marker='o',alpha=0.5,label=repr(ss))\n",
    "    plt.title('Peso {0:.2f}, Stars media: {1:.1f}, num restaurantes: {2:d}'.format(pesos[oo], \n",
    "                                                                                   np.mean(rr_df['stars'].values),\n",
    "                                                                                  len(cuales)))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpl89xBsV_tY"
   },
   "source": [
    "Combinando la información de los atributos con la localización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YfU-RKiV_tY"
   },
   "outputs": [],
   "source": [
    "attr_train = v.transform(restaurantes_train['flat_attributes'].values)\n",
    "attr_test = v.transform(restaurantes_test['flat_attributes'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhPH1gxDV_tY"
   },
   "source": [
    "Conteos más atributos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGbzU541V_tZ"
   },
   "outputs": [],
   "source": [
    "X_train = restaurantes_train['log10_review_count'].values.reshape(-1,1) # column vector, 1 feature and many rows\n",
    "X_train = np.nan_to_num(np.hstack((X_train,  attr_train)))\n",
    "# Training set targets\n",
    "Y_train = restaurantes_train['stars'].values\n",
    "# Test set observations\n",
    "X_test = restaurantes_test['log10_review_count'].values.reshape(-1,1)\n",
    "X_test = np.nan_to_num(np.hstack((X_test,  attr_test)))\n",
    "# Test set targets\n",
    "Y_test = restaurantes_test['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1614837846592,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "iUSsAUEdV_tZ",
    "outputId": "6391cd32-e964-421d-e565-572a26654332"
   },
   "outputs": [],
   "source": [
    "v_alpha = [.1, 1, 10, 100, 1000]\n",
    "grid_lr = GridSearchCV(Ridge(), param_grid={'alpha':v_alpha})\n",
    "grid_lr.fit(X_train, Y_train)\n",
    "linear_model = grid_lr.best_estimator_\n",
    "print(grid_lr.best_params_)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model.score(X_test, Y_test)))\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1614837846800,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "ZXE3bxOVKslv",
    "outputId": "29a36564-5504-4dc0-a696-93898aa7957e"
   },
   "outputs": [],
   "source": [
    "print(\"w0 = {0:.4f}\".format(linear_model.intercept_))\n",
    "nombre_feature = ['log10_review_count']\n",
    "nombre_feature += gg.get_feature_names()\n",
    "nombre_feature += v.get_feature_names()\n",
    "\n",
    "pesos = linear_model.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden[:20]:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOXOkAeSV_ta"
   },
   "source": [
    "Conteos más atributos más localización mejora sensiblemente cualquiera de las dos fuentes tratadas por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35447,
     "status": "ok",
     "timestamp": 1614837890936,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "QwQ9DxzhV_ta",
    "outputId": "c73bb27c-083e-4049-b3ef-b94bb53fdcc6"
   },
   "outputs": [],
   "source": [
    "X_train = restaurantes_train['log10_review_count'].values.reshape(-1,1) # column vector, 1 feature and many rows\n",
    "X_train = np.nan_to_num(np.hstack((X_train, casillas_train, attr_train)))\n",
    "# Training set targets\n",
    "Y_train = restaurantes_train['stars'].values\n",
    "# Test set observations\n",
    "X_test = restaurantes_test['log10_review_count'].values.reshape(-1,1)\n",
    "X_test = np.nan_to_num(np.hstack((X_test, casillas_test, attr_test)))\n",
    "# Test set targets\n",
    "Y_test = restaurantes_test['stars'].values\n",
    "\n",
    "grid_lr.fit(X_train, Y_train)\n",
    "linear_model = grid_lr.best_estimator_\n",
    "print(grid_lr.best_params_)\n",
    "print(\"Score with the training data R^2={0:.4f}\".format(linear_model.score(X_train, Y_train)))\n",
    "print(\"Score with the test data R^2={0:.4f}\".format(linear_model.score(X_test, Y_test)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"w0 = {0:.4f}\".format(linear_model.intercept_))\n",
    "nombre_feature = ['log10_review_count']\n",
    "nombre_feature += gg.get_feature_names()\n",
    "nombre_feature += v.get_feature_names()\n",
    "\n",
    "pesos = linear_model.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden[:20]:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ8Q9C0DV_ta"
   },
   "source": [
    "## 2.5 Feature Templates con información de fechas y horas\n",
    "\n",
    "La obtención de caracteristicas que capturen información necesaria para representar patrones relacionados con cómo se distributyen las observaciones a lo largo del tiempo también puede llevarse a cabo mediante *feature templates*. En este caso el dato crudo que vamos a explotar es la marca temporal *timestamp* en la que sucede la observación. \n",
    "\n",
    "En general un *timestamp* contiene información sobre fecha y hora. Los campos que esperamos encotrar son:\n",
    "- año\n",
    "- mes\n",
    "- día\n",
    "- hora\n",
    "- minuto\n",
    "- segundo (con precisión de milisegundos o más fina)\n",
    "\n",
    "Mediante la aplicación de *feature templates* sobre marcas temporales podremos recuperar información relevante a:\n",
    "- día:\n",
    "    - mañana, tarde, noche\n",
    "    - horario comercial, fuera de  horario\n",
    "- semana:\n",
    "    - día de la semana\n",
    "    - día laborable o fin de semana\n",
    "    - primeros, mediados o finales de semana\n",
    "- mes: \n",
    "    - primeros, mediados o finales de mes\n",
    "- anual:\n",
    "    - Saber el trimestre, cuatrimestre, estación, etc\n",
    "    \n",
    "    \n",
    "Extraer esta información es más o menos inmediato de automatizar. Hay otras informaciones como \"festivo nacional\", \"vacaciones escolares\", \"puente\", etc para las que serían necesario cruzar con un calendarios local. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxG0thAtV_ta"
   },
   "source": [
    "### Dataset sobre el uso de bicicletas municipales en Seúl\n",
    "\n",
    "La siguiente celda carga un cojunto de datos sobre la utilización del servicio público de alquiler de bicicletas en Seúl. El *target* consiste en predecir el número de bicicletas que se van a alquilar en una hora en función de variables temporales y de climatología.\n",
    "\n",
    "Nos vamos a centrar sobre todo en encontrar patrones temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spA9-RHJV_tb"
   },
   "outputs": [],
   "source": [
    "seoul_bikes_df= pd.read_csv('data_seoul_bikes.csv', index_col=0, header=0, parse_dates=['Timestamp'],dtype={'Rented Bike Count': int,\n",
    "                                                                               'Temperature(C)':float, \n",
    "                                                                               'Humidity(%)':float,\n",
    "                                                                               'Wind speed (m/s)':float, \n",
    "                                                                               'Visibility (10m)':float, \n",
    "                                                                               'Dew point temperature(C)':float,\n",
    "                                                                               'Solar Radiation (MJ/m2)':float, \n",
    "                                                                               'Rainfall(mm)':float,\n",
    "                                                                               'Snowfall (cm)':float,\n",
    "                                                                               'Seasons':str,\n",
    "                                                                               'Holiday':str, \n",
    "                                                                               'Functioning Day':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ_R6mYUV_tb",
    "outputId": "6d491111-c9a2-4c94-a956-4a012d0bfa4c"
   },
   "outputs": [],
   "source": [
    "seoul_bikes_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPh_ovc9V_tb",
    "outputId": "3768254c-ac45-4b29-fcd5-55cfe5f92fa4"
   },
   "outputs": [],
   "source": [
    "seoul_bikes_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu9xbhFLV_tb"
   },
   "source": [
    "#### disclaimer\n",
    "\n",
    "Solo tenemos un año de información, así que no vamos a emplear el modelo con fines predictivos explícitamente, sino como ejemplo para ilustrar cómo se aplican las plantillas de características a información de fechas y horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWJLb49nV_tc"
   },
   "outputs": [],
   "source": [
    "# En primer lugar dividimos los datos en las particiones de entrenamiento y test correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knkg4WAxV_tc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "seoul_train, seoul_test = train_test_split(seoul_bikes_df, \n",
    "                                                         test_size=0.2, \n",
    "                                                         random_state=42)\n",
    "\n",
    "Ytrain = seoul_train['Rented Bike Count'].values\n",
    "Ytest = seoul_test['Rented Bike Count'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgBRfJgHV_tc"
   },
   "source": [
    "Como primera aproximación aprendemos un modelo lineal **\"estacionario\"**, es decir, descartando toda la información temporal y empleando únicamente las variables climatológicas (que están obviamente correlacionadas con los patrones temporales, pero no explícitamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4AdeS7JV_tc",
    "outputId": "61256ba2-5ed2-4676-d346-b5a740f16ebb"
   },
   "outputs": [],
   "source": [
    "\n",
    "continuas = ['Temperature(C)',\n",
    "             'Humidity(%)','Wind speed (m/s)',\n",
    "             'Visibility (10m)',\n",
    "             'Dew point temperature(C)',\n",
    "             'Solar Radiation (MJ/m2)',\n",
    "             'Rainfall(mm)',\n",
    "             'Snowfall (cm)']\n",
    "\n",
    "# escalamos los datos\n",
    "XX = seoul_train[continuas].values\n",
    "sc = StandardScaler().fit(XX)\n",
    "Xtrain_c = sc.transform(XX)\n",
    "Xtest_c = sc.transform(seoul_test[continuas].values)\n",
    "\n",
    "\n",
    "v_alpha = [1e-3, 1e-2, 1e-1, 1, 10, 100,1000,1e4]\n",
    "grid_rr = GridSearchCV(Ridge(),{'alpha':v_alpha})\n",
    "grid_rr.fit(Xtrain_c, Ytrain)\n",
    "print(grid_rr.best_params_)\n",
    "print('R^2 train: {0:.3f}'.format(grid_rr.score(Xtrain_c, Ytrain)))\n",
    "print('R^2 test: {0:.3f}'.format(grid_rr.score(Xtest_c, Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nddDEwvPV_td"
   },
   "source": [
    "A continuación vamos a desplegar diferentes *feature templates* para enriquecer el modelo lineal con patrones dependientes de la temporalidad de las observaciones. Comenzamos con patrones que capturen información semanal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sA0YVvQV_td"
   },
   "source": [
    "### Día de la semana\n",
    "\n",
    "El objetivo de esta *feature template* es codificar una función que para cada observación del conjunto de datos nos devuelva un diccionario con una clave indicando el día de la semana y otra indicando si es fin de semana o día laborable:\n",
    "- `Monday`, `Tuesday`, `Wednesday`,`Thursday`,`Friday`,`Saturday`,`Sunday`\n",
    "- `WorkDay`,`Weekend`\n",
    "\n",
    "Por ejemplo:\n",
    "$$\n",
    "x=\\mbox{ 5 marzo 2021 } \\Rightarrow \\boldsymbol \\phi(x) = \\{\\mbox{Friday : True,} \\mbox{WorkDay : True} \\}\n",
    "$$\n",
    "\n",
    "asumimos que el resto de *values* estarán en `False` y por ello no se generan explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1614837903944,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "K7fBptmZV_te",
    "outputId": "ed55a7d8-40f3-4539-ac5c-7045998f0e41"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "print(dt.datetime.today())\n",
    "dt.datetime.today().weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3E8dY9jV_te"
   },
   "source": [
    "La siguiente celda contiene la función `dict_week_day` que nos devuelve ese diccionario cuando recibe un objeto python `datetime` como argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDJ0scRuV_te"
   },
   "outputs": [],
   "source": [
    "lista_dias = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "def dict_week_day(t):\n",
    "    dow = t.weekday()\n",
    "    d = {lista_dias[dow]:True}\n",
    "    if dow<5:\n",
    "        d.update({'WorkDay':True})\n",
    "    else:\n",
    "        d.update({'Weekend':True})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1614837905848,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "4Xo3M1lHV_te",
    "outputId": "5c9d65ea-490a-4ec6-9dbf-e8657354d3eb"
   },
   "outputs": [],
   "source": [
    "dict_week_day(dt.datetime.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RxdZwqPV_tg"
   },
   "source": [
    "Apoyandonos en `dict_week_day` vamos a programar el despliegue de la *feature template* para extraer información de la semana en el conjunto de datos:\n",
    "1. Construir el diccionario con la información de la semana para cada registro del conjunto de entrenamiento y de test\n",
    "2. Usar `DictVectorizer` para codificar *one-hot* la información de semana\n",
    "3. Aprender el modelo lineal enriquecido con estas nuevas features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "error",
     "timestamp": 1614837907259,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "Lr98-nzlV_tg",
    "outputId": "f480dac8-862a-41af-d209-6e5c48c8fc80"
   },
   "outputs": [],
   "source": [
    "# desplegar feature templates\n",
    "dict_semana_train = seoul_train['Timestamp'].apply(dict_week_day)\n",
    "dict_semana_test = seoul_test['Timestamp'].apply(dict_week_day)\n",
    "\n",
    "# Llamar a DictVectorizer\n",
    "dv_semana = DictVectorizer(sparse=False)\n",
    "print(\"Solo dias\")\n",
    "Xtrain_w = dv_semana.fit_transform(dict_semana_train.values)\n",
    "Xtest_w = dv_semana.transform(dict_semana_test.values)\n",
    "\n",
    "# Entrenar modelo lineal *solo con información de la semana*\n",
    "\n",
    "grid_rr = GridSearchCV(Ridge(),{'alpha':v_alpha})\n",
    "grid_rr.fit(Xtrain_w, Ytrain)\n",
    "print(grid_rr.best_params_)\n",
    "print('R^2 train: {0:.3f}'.format(grid_rr.score(Xtrain_w, Ytrain)))\n",
    "print('R^2 test: {0:.3f}'.format(grid_rr.score(Xtest_w, Ytest)))\n",
    "\n",
    "\n",
    "# Entrenar modelo lineal *enriquecido con información de la semana*\n",
    "\n",
    "print(\"\\nDias + variables continuas\")\n",
    "Xtrain = np.hstack((Xtrain_c, Xtrain_w))\n",
    "Xtest = np.hstack((Xtest_c, Xtest_w))\n",
    "grid_rr.fit(Xtrain, Ytrain)\n",
    "print(grid_rr.best_params_)\n",
    "print('R^2 train: {0:.3f}'.format(grid_rr.score(Xtrain, Ytrain)))\n",
    "print('R^2 test: {0:.3f}'.format(grid_rr.score(Xtest, Ytest)))\n",
    "print(\"\")\n",
    "print(\"w0 = {0:.4f}\".format(grid_rr.best_estimator_.intercept_))\n",
    "nombre_feature = continuas[:]\n",
    "nombre_feature += dv_semana.get_feature_names()\n",
    "\n",
    "pesos = grid_rr.best_estimator_.coef_\n",
    "orden = np.argsort(np.absolute(pesos))[::-1]\n",
    "for ii in orden:\n",
    "    print(\"peso {0} = {1:.4f}\".format(nombre_feature[ii], pesos[ii]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tz44xWwV_th"
   },
   "source": [
    "No parece que la información de la semana contribuya mucho a mejorar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGOrVhAvV_th"
   },
   "source": [
    "### Información mensual\n",
    "\n",
    "Adaptad el procesamiento anterior para programar un despliegue de *feature templates* que extraiga información sobre el mes. Concretamente el mes en el que se está y el cuatrimestre:\n",
    "- `Jan`, `Feb`,`Mar`,`Apr`,`May`,`Jun`,`Jul`,`Aug`, `Sep`, `Oct`, `Nov`, `Dec`\n",
    "- `Q1`, `Q2`,`Q3`,`Q4`\n",
    "\n",
    "Para ello:\n",
    "- Cree una función `dict_month` análoga a `dict_week_day`\n",
    "- Aplique *feature templates* a los datos\n",
    "- Invoque `DictVectorizer`\n",
    "- Aprenda el modelo lineal correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epFiNd__V_ti",
    "outputId": "8dcf0d09-9515-4cb7-dbb3-8e21715c2131"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# YOUR CODE #\n",
    "#############\n",
    "\n",
    "#################\n",
    "# END YOUR CODE #\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apHEW7tvV_ti"
   },
   "source": [
    "### Dia del mes\n",
    "Adaptad el procesamiento anterior para programar un despliegue de *feature templates* que extraiga información sobre el día dentor del mes. Concretamente el mes en el que se está y el cuatrimestre:\n",
    "- `Early`, `Mid`, `Late` \n",
    "- `1`, `2`, ..., `31`\n",
    "\n",
    "Cread una función `dict_day` análoga a `dict_week_day`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uin31gIYV_ti"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# YOUR CODE #\n",
    "#############\n",
    "\n",
    "#################\n",
    "# END YOUR CODE #\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCTwu-jpV_tk"
   },
   "source": [
    "Ahora combine las 3 *feature templates* junto con las variables numéricas en el mismo modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVULDW7wV_tk",
    "outputId": "bddbfde1-de80-4550-e513-331bd4c58bb2"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# YOUR CODE #\n",
    "#############\n",
    "\n",
    "#################\n",
    "# END YOUR CODE #\n",
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YZezw--V_tk"
   },
   "source": [
    "El mejor resultado se obtiene combinando las 3 *feature templates* en el mismo predictor lineal, esto quiere decir que cada fuente de información está siendo contribuyente neta en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OtfaEa8V_tk"
   },
   "source": [
    "# 3. Síntesis de características mediante *machine learning*\n",
    "\n",
    "Además de las *feature templates*, que obtienen juegos de características sintéticas aplicando cálculos más o menos sencillos capaces de extraer patrones de las variables originales, podemos emplear técnicas de aprendizaje automático para el diseño de características sintéticas más sofisticadas.\n",
    "\n",
    "Nuevamente recordamos que normalmente la ganancia en la riqueza expresiva de las características que se puedan generar con aprendizaje automático puede comprometer parte de la interpretabilidad de los resultados del aprendizaje. En este sentido, las técnicas que vamos a presentar en esta sección pueden considerarse un paso intermedio antes del empleo de *deep learning* para el diseño de características; las técnicas que vamos a presentar a continuación sí permiten controlar algunos parámetros del proceso de diseño de las características que se sintetizan frente a la total automatización del proceso que sucede al emplear *deep learning*.\n",
    "\n",
    "## 3.1 Síntesis de características mediante *ensembles*\n",
    "\n",
    "El modo de extraer las predicciones finales en un modelo de conjuntos o *ensemble* consiste en combinar las predicciones individuales efectuadas por los miembros del conjunto en una predicción unificada. En un problema de clasificación la clase de salida para cada observación de test es la moda de las clases predichas por los miembros del conjunto y en un modelo de regresión suele emplearse la media como método para combinar de estas predicciones individuales. \n",
    "\n",
    "La predicción basada en la media de las predicciones individuales de los miembros del conjunto puede **asimilarse a un modelo lineal** en el que:\n",
    "- Las características son las salidas de los miembros del conjunto\n",
    "- Los pesos son $1/M$ donde $M$ es el tamaño del *ensemble* (número de *weak learners*)\n",
    "\n",
    "Esta analogía motiva el primer método de síntesis de características, entrenar un número de *weak learners* y usar sus predicciones como características para un modelo lineal. Conceptualmente está relacionado con el *stacking* de clasificadores que vimos en el módulo de *ensembles*, considerando un modelo lineal como *metaclasificador* que combina los clasificadores que actúan directamente sobre las características originales.\n",
    "\n",
    "Para evitar problemas de *leakage*, es prudente dividir el entrenamiento del modelo global en dos etapas, idealmente cada una con su conjunto de entrenamiento independiente:\n",
    "1. **Sintetizar características**. Entrenar *weak learners* con  una parte de los datos de entrenamiento. A cada *weak learner* se le proporciona un subconjunto de esa parte de datos de entrenamiento siguiendo un esquema de *bagging*, por ejemplo.\n",
    "\n",
    "2. Entrenar un **modelo lineal** empleando como características de entrada las salidas de los *weak learners* de la etapa anterior. Estas características se habrán sintentizado a partir de un conjunto de entrenamiento separado del usado en el punto anterior para evitar *leakage*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Madtuf2JV_tk"
   },
   "source": [
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTERMEDIO/features1.png' width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9wEa3S2V_tk"
   },
   "source": [
    "Vamos a emplear este esquema para resolver el problema de la predicción del alquiler de bicicletas en Seúl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDgAIwXUV_tk"
   },
   "outputs": [],
   "source": [
    "# Dividimos el conjunto de entrenamiento en dos partes\n",
    "seoul_train_features, seoul_train_model = train_test_split(seoul_train, \n",
    "                                                         test_size=0.5, \n",
    "                                                         random_state=42)\n",
    "Ytrain_features = seoul_train_features['Rented Bike Count'].values\n",
    "Ytrain_model = seoul_train_model['Rented Bike Count'].values\n",
    "\n",
    "# Aplicamos el escalado que ya habíamos aprendido\n",
    "Xtrain_features_c = sc.transform(seoul_train_features[continuas].values)\n",
    "Xtrain_model_c = sc.transform(seoul_train_model[continuas].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hyt55RZPV_tk"
   },
   "source": [
    "### Empleo de árboles de decisión como aprendices débiles\n",
    "\n",
    "El siguiente código genera 7 ensembles, de tamaños comprendidos entre 10 y 1000 miembros usando árboles de decisión como *weak learners* y un esquema de *bagging* para conseguir los conjuntos de entrenamiento de cada árbol, es decir, 7 *Random Forests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcyeq9-vV_tl"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "v_arboles = [10,20,50,100,200,500,1000]\n",
    "max_leaf_nodes = 20\n",
    "lista_rf  = []\n",
    "for n_estimators in v_arboles:\n",
    "    lista_rf.append(RandomForestRegressor(max_leaf_nodes=max_leaf_nodes,\n",
    "                                         n_estimators = n_estimators,\n",
    "                                         max_samples=0.5).fit(Xtrain_features_c, \n",
    "                                                                          Ytrain_features))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUiyQwvzV_tl"
   },
   "source": [
    "En la lista `lista_rf` hemos almacenado cada uno de los ensembles, es decir, cada elemento de `lista_rf` es un *random forest*. Cada uno de estos *random forest* tiene a su vez una lista con los árboles de decisión que lo pueblan. Esta lista es accesible a través del atributo `estimators_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMeEPH4SV_tl",
    "outputId": "fa88704c-1afb-4587-a4ae-fd835f761531"
   },
   "outputs": [],
   "source": [
    "lista_rf[0].estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy4Awi8wV_tm"
   },
   "source": [
    "La siguiente celda contiene la función `genera_features`, que recibe una lista de estimadores y un conjunto de datos y **sintetiza características** del siguiente modo:\n",
    "- una fila por cada elemento (fila) de `X`\n",
    "- una columna por cada estimador en la lista `lista_estimadores`\n",
    "- el elemento `Features[i,j]` es el resultado de estimar `X[i,:]` con el estimador `lista_estimadores[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9GNdf-CV_tm"
   },
   "outputs": [],
   "source": [
    "def genera_features(lista_estimadores, X):\n",
    "    n_rows = X.shape[0]\n",
    "    n_cols = len(lista_estimadores)\n",
    "    Features = np.empty((n_rows, n_cols))\n",
    "    for jj,estimador in enumerate(lista_estimadores):\n",
    "        Features[:,jj] = estimador.predict(X)\n",
    "    return Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0DPM06tV_tm"
   },
   "source": [
    "Finalmente la siguiente celda aprende los modelos lineales (notad que usando `Xtrain_model_c` como conjunto para sintetizar *features*, que son datos distintos a los empleados para aprender los *random forest*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eekK3hSPV_tm",
    "outputId": "2f817d5e-0b53-4a16-afa6-04e188460461"
   },
   "outputs": [],
   "source": [
    "r2_random_forest = np.empty(len(v_arboles))\n",
    "r2_rf = np.empty(len(v_arboles))\n",
    "for ii,n_estimators in enumerate(v_arboles):\n",
    "    r2_rf[ii] = lista_rf[ii].score(Xtest_c, Ytest)\n",
    "    lista_estimadores = lista_rf[ii].estimators_\n",
    "    print(len(lista_estimadores))\n",
    "    X_train = genera_features(lista_estimadores, Xtrain_model_c)\n",
    "    print(X_train.shape)\n",
    "    sc1 = StandardScaler()\n",
    "    sc1.fit(X_train)\n",
    "    X_train = sc1.transform(X_train)\n",
    "    X_test = sc1.transform(genera_features(lista_estimadores, Xtest_c))\n",
    "    grid_rr.fit(X_train, Ytrain_model)\n",
    "    print(\"{0:d} estimadores\".format(n_estimators))\n",
    "    print(grid_rr.best_params_)\n",
    "    r2_random_forest[ii] = grid_rr.score(X_test, Ytest)\n",
    "    print('R^2 train: {0:.3f}'.format(grid_rr.score(X_train, Ytrain_model)))\n",
    "    print('R^2 test: {0:.3f}'.format(r2_random_forest[ii]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLNttCAZV_tn"
   },
   "source": [
    "#### Extremely Random Trees\n",
    "Las características sintetizadas usando *random forest* tienen una motivación para funcionar bien, es decir, para aportar **poder predictivo** y es que no dejan de ser modelos, sencillos, pero orientados a resolver la tarea de predicción.\n",
    "\n",
    "Si bien para que un *ensemble* basado en *bagging* funcione es necesario exigirle a cada *weak learner* una mínima precisión (del orden de un acierto ligeramente superior al 50% en un problema de clasificación binaria con clases balanceadas), esta restricción se puede **relajar** si consideramos que los árboles se usan para sintetizar características que luego se van a combinar de **un modo más inteligente** que con un promedio, es decir, estas características sintéticas van a alimentar un modelo lineal.\n",
    "\n",
    "Relajar al máximo los *weak learners* implica no entrenarlos, sino generar árboles aleatorios. De esta manera se ahorran los costes de entrenar cada árbol descansando en que habrá un número suficientemente grande y variado de ellos para que el modelo lineal pueda resolver la tarea.\n",
    "\n",
    "La clase [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) implementa los *extremely random trees* en *scikit learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XSKK7aRV_tn"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "v_arboles = [10,20,50,100,200,500,1000]\n",
    "max_leaf_nodes = 20\n",
    "lista_et  = []\n",
    "for n_estimators in v_arboles:\n",
    "    lista_et.append(ExtraTreesRegressor(max_leaf_nodes=max_leaf_nodes,\n",
    "                                         n_estimators = n_estimators,\n",
    "                                         max_samples=0.5).fit(Xtrain_features_c, \n",
    "                                                                          Ytrain_features))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZa8EQwlV_tn",
    "outputId": "040e79fb-f9c9-425b-b499-33bf14fd5002"
   },
   "outputs": [],
   "source": [
    "r2_extremely_random = np.empty(len(v_arboles))\n",
    "r2_er = np.empty(len(v_arboles))\n",
    "for ii,n_estimators in enumerate(v_arboles):\n",
    "    r2_er[ii] = lista_et[ii].score(Xtest_c, Ytest)\n",
    "    lista_estimadores = lista_et[ii].estimators_\n",
    "    print(len(lista_estimadores))\n",
    "    X_train = genera_features(lista_estimadores, Xtrain_model_c)\n",
    "    print(X_train.shape)\n",
    "    sc1 = StandardScaler()\n",
    "    sc1.fit(X_train)\n",
    "    X_train = sc1.transform(X_train)\n",
    "    X_test = sc1.transform(genera_features(lista_estimadores, Xtest_c))\n",
    "    grid_rr.fit(X_train, Ytrain_model)\n",
    "    print(\"{0:d} estimadores\".format(n_estimators))\n",
    "    print(grid_rr.best_params_)\n",
    "    r2_extremely_random[ii] = grid_rr.score(X_test, Ytest)\n",
    "    print('R^2 train: {0:.3f}'.format(grid_rr.score(X_train, Ytrain_model)))\n",
    "    print('R^2 test: {0:.3f}'.format(r2_extremely_random[ii]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SfKFvCjV_tn",
    "outputId": "e0c326c7-378f-4605-8663-2efbe74ebd08"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(v_arboles, r2_rf, label='Random Forest')\n",
    "plt.plot(v_arboles, r2_random_forest, label='Sintesis Random Forest + Lineal')\n",
    "plt.plot(v_arboles, r2_er, label='Extremely random trees')\n",
    "plt.plot(v_arboles, r2_extremely_random, label='Sintesis Extremely random trees + lineal')\n",
    "plt.legend()\n",
    "plt.xlabel('Numero caracteristicas sinteticas/árboles')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZm4K1aeV_tn"
   },
   "source": [
    "### Empleo de regresores lineales o kNN como aprendices débiles\n",
    "\n",
    "Además de árboles de decisión podemos probar a sintetizar características con otros *weak learners* como regresores lineales (en el espacio original, claro) o kNN. \n",
    "\n",
    "Emplear *weak learners* nos proporciona **diversidad** en las características sintetizadas, lo que enriquece al modelo lineal con mayor capacidad expresiva.\n",
    "\n",
    "El *weak learner* elegido nos da una idea de cómo son los patrones que puedan capturar estas características.\n",
    "\n",
    "La clase [BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) nos permite construir estos *ensembles* con el *weak learner* que más se adecúe a nuestros intereses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI6JcL9UV_tn"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "v_arboles = [10,20,50,100,200,500,1000]\n",
    "max_leaf_nodes = 20\n",
    "lista_lr  = []\n",
    "for n_estimators in v_arboles:\n",
    "    lista_lr.append(BaggingRegressor(base_estimator= KNeighborsRegressor(n_neighbors=10,weights='distance'),\n",
    "                                         n_estimators = n_estimators,\n",
    "                                         max_samples=0.5).fit(Xtrain_features_c, \n",
    "                                                                          Ytrain_features))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZSRaO0zV_to",
    "outputId": "f5b90ea9-95ef-479a-b3e4-40a1088c7702"
   },
   "outputs": [],
   "source": [
    "r2_knn = np.empty(len(v_arboles))\n",
    "for ii,n_estimators in enumerate(v_arboles):\n",
    "    lista_estimadores = lista_lr[ii].estimators_\n",
    "    print(len(lista_estimadores))\n",
    "    X_train = genera_features(lista_estimadores, Xtrain_model_c)\n",
    "    print(X_train.shape)\n",
    "    sc1 = StandardScaler()\n",
    "    sc1.fit(X_train)\n",
    "    X_train = sc1.transform(X_train)\n",
    "    X_test = sc1.transform(genera_features(lista_estimadores, Xtest_c))\n",
    "    grid_rr.fit(X_train, Ytrain_model)\n",
    "    print(\"{0:d} estimadores\".format(n_estimators))\n",
    "    print(grid_rr.best_params_)\n",
    "    print('R^2 train: {0:.3f}'.format(grid_rr.score(X_train, Ytrain_model)))\n",
    "    print('R^2 test: {0:.3f}'.format(grid_rr.score(X_test, Ytest)))\n",
    "    print(\"\")\n",
    "    r2_knn[ii] = grid_rr.score(X_test, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **ejercicio** podéis sintetizar caracterísiticas usando este esquema pero con regresores lineales como aprendices débiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHJH0YB9V_to",
    "outputId": "0f13bb4f-ba00-4959-999d-d61728521fa2"
   },
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "# YOUR CODE #\n",
    "#############\n",
    "\n",
    "#################\n",
    "# END YOUR CODE #\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npAFriJHV_to",
    "outputId": "006d414d-9fd2-40c8-b45d-5316ac1a8541"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(v_arboles, r2_random_forest, label='Sintesis Random Forest + Lineal')\n",
    "plt.plot(v_arboles, r2_extremely_random, label='Sintesis Extremely random trees + lineal')\n",
    "plt.plot(v_arboles, r2_knn, label='Sintesis kNN + Lineal')\n",
    "plt.plot(v_arboles, r2_lm, label='Sintesis Linear Regression  + Lineal')\n",
    "plt.legend()\n",
    "plt.xlabel('Numero caracteristicas sinteticas/árboles')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA9m__VrV_tp"
   },
   "source": [
    "## 3.2 Características sintéticas basadas en *kernels*\n",
    "\n",
    "Los modelos basados en kernels (como las SVM) son exactamente **combinaciones lineales** de esas funciones kernels alojadas en algunas de las muestras de entrenamiento (los vectores soporte, en el caso de las SVM). Siguiendo el  punto de vista desarrollado en la subsección anterior, podemos asimilar el **rol de cada una de esas funciones kernel** al desempeñado por los *weak learners* y considerar que cada función kernel es una componente de la **transformación** que define un espacion de características alternativo en el que se aplica un modelo lineal. Concretamente:\n",
    "\n",
    "+ Solución de un modelo basado en *kernels*, donde $\\{(\\mathbf x_i, y_i)\\}_{i=1}^n$ son los pares observación, *target* que definen el conjunto de entrenamiento y $\\mathbf x_t$ es una observación genérica del conjunto de test para la que se desea predecir el target $\\mathbf y_t$. Consideramos una salida *blanda* (antes del decisor basado en signo) para unificar clasificación (estaríamos dando la distancia a la frontera de clasificación en el caso de la SVM, no la clase) y regresión.\n",
    "$$\n",
    "f(\\mathbf x_t) = \\sum_{i=1}^n{\\alpha_i \\kappa(\\mathbf x_i,\\mathbf x_t) }\n",
    "$$\n",
    "\n",
    "+ Esta suma se puede descomponer en un producto escalar\n",
    "$$\n",
    "f(\\mathbf x_t) = \\left[\\begin{array}{cccc} \\alpha_1 & \\alpha_2 & \\dots & \\alpha_n \\end{array}\\right] \\left[\\begin{array}{c} \\kappa(\\mathbf x_1,\\mathbf x_t) \\\\ \\kappa(\\mathbf x_2,\\mathbf x_t) \\\\ \\vdots \\\\ \\kappa(\\mathbf x_n,\\mathbf x_t)\\end{array}\\right] = \\mathbf w^\\top \\boldsymbol \\psi(\\mathbf x_t) \n",
    "$$\n",
    "+ Los coeficientes del modelo de kernels $\\{\\alpha_i\\}_{i=1}^n$ formarían el vector de pesos del modelo lineal\n",
    "$$\n",
    "\\mathbf w = \\left[\\begin{array}{c} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\end{array}\\right]\n",
    "$$\n",
    "+ Las funciones *kernel* $\\{\\kappa(\\mathbf x_i,\\mathbf x_t)\\}_{i=1}^n$ serían las encargadas de transformar cada observación del espacio de entrada en una observación del espacio de características:\n",
    "$$\n",
    "\\boldsymbol \\psi:\\mathbf x_t \\in \\mathbb R^d \\rightarrow \\boldsymbol \\psi(\\mathbf x_t)\\in \\mathbb R^n\n",
    "$$\n",
    "con\n",
    "$$\n",
    "\\boldsymbol \\psi(\\mathbf x_t) = \\left[\\begin{array}{c} \\psi_1(\\mathbf x_t)=\\kappa(\\mathbf x_1,\\mathbf x_t) \\\\ \\psi_2(\\mathbf x_t)=\\kappa(\\mathbf x_2,\\mathbf x_t) \\\\ \\vdots \\\\ \\psi_n(\\mathbf x_t)=\\kappa(\\mathbf x_n,\\mathbf x_t)\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dado que un *kernel* tiene una interpretación de función de similitud (es asimilable a un producto escalar), cada una de estas $n$ características sintéticas $\\psi_i(\\mathbf x_t)=\\kappa(\\mathbf x_i,\\mathbf x_t)$  tiene una interpretación en términos  del **parecido de la observación $\\mathbf x_t$ a la observación del conjunto de entrenamiento en la que se centra el kernel** que define la característica $i$-ésima: $\\mathbf x_i$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDptz9AEV_tp"
   },
   "source": [
    "Alternativamente a esta interpretación de cada función kernel como una característica sintética que mide el parecido con la observación del conjunto de entrenamiento correspondiente, no debemos dejar de tener en cuenta que tenemos una garantía teórica de que cuando seleccionamos una función *kernel* para construir una versión no lineal de un modelo lineal, en realidad estamos eligiendo una transformación a un espacio de características implícito en el que ese kernel calcula productos escalares:\n",
    "\n",
    "+ Elegir un *kernel* $\\kappa_{\\phi}(\\mathbf x_i, \\mathbf x_j)$ implica elegir una transformación $\\boldsymbol \\phi$ tal que dadas dos observaciones del conjunto de entrenamiento $\\mathbf x_i$ y $\\mathbf x_j$:\n",
    "$$\n",
    "\\kappa_{\\phi}(\\mathbf x_i, \\mathbf x_j) = \\boldsymbol \\phi(\\mathbf x_i)^\\top\\boldsymbol \\phi(\\mathbf x_j)\n",
    "$$\n",
    "\n",
    "En algunos casos puede ser relativamente accesible recuperar la transformación implícita  $\\boldsymbol \\phi()$, es decir, conocer cada uno de los elementos del vector $\\boldsymbol \\phi(\\mathbf x_i)$ explícitamente, o construir una aproximación a ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd_b_R4lV_tp"
   },
   "source": [
    "### Features sintéticas a partir del kernel polinómico\n",
    "\n",
    "Un *kernel*  polinómico obedede a la expresión\n",
    "$$\n",
    "\\kappa(\\mathbf u, \\mathbf v) = (\\mathbf u^\\top \\mathbf v + c)^d\n",
    "$$ con $c$ y $d$ constantes\n",
    "\n",
    "Esta expresión se puede desarrollar, vamos a hacerlo para el caso particular en que el grado del polinomio sea $d=2$ y las observaciones tengan dos dimensiones: $\\mathbf u, \\mathbf v \\in \\mathbb R^2$. Si las observaciones son en 2 dimensiones, el producto escalar interno del *kernel* puede desarrollarse como:\n",
    "$$\n",
    "\\mathbf u^\\top \\mathbf v = u_1v_1 + u_2v_2 \n",
    "$$\n",
    "\n",
    "Introduciendo este resultado en el cálculo del *kernel*\n",
    "\n",
    "$$\n",
    "(\\mathbf u^\\top \\mathbf v + c)^2 = (u_1v_1 + u_2v_2 + c)^2 = u_1^2v_1^2 + u_2^2v_2^2 + c^2 + 2u_1v_1u_2v_2 + 2u_1v_1c + 2u_2v_2c \n",
    "$$\n",
    "\n",
    "Separamos lo que depende de $\\mathbf u$ de lo que depende de $\\mathbf v$ en dos vectores:\n",
    "\n",
    "$$\n",
    "u_1^2v_1^2 + u_2^2v_2^2 + c^2 + 2u_1v_1u_2v_2 + 2u_1v_1c + 2u_2v_2c = \\left[\\begin{array}{cccccc}u_1^2 & u_2^2 & c & \\sqrt{2}u_1u_2 & \\sqrt{2c}u_1 & \\sqrt{2c}u_2 \\end{array}\\right]\\left[\\begin{array}{c} v_1^2 \\\\ v_2^2 \\\\ c \\\\ \\sqrt{2}v_1v_2 \\\\ \\sqrt{2c}v_1 \\\\ \\sqrt{2c}v_2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Llamamos $\\boldsymbol \\phi(\\mathbf u)$ al vector que contiene los elementos dependientes de $\\mathbf u$ y $\\boldsymbol \\phi(\\mathbf v)$ al que contiene los elementos dependientes de $\\mathbf v$.\n",
    "\n",
    "Luego el kernel $\\kappa(\\mathbf u, \\mathbf v)$ puede descomponerse en el producto escalar de dos vectores de características que son transformaciones de cada uno de los vectores originales\n",
    "$$\n",
    "\\kappa(\\mathbf u, \\mathbf v) = \\boldsymbol \\phi(\\mathbf u)^\\top\\boldsymbol \\phi(\\mathbf v)\n",
    "$$\n",
    "\n",
    "donde para un vector observación del espacio original de entrada, $\\mathbf v$ la transformación al nuevo espacio de características inducido por el *kernel* polinómico viene dada por\n",
    "\n",
    "$$\n",
    "\\boldsymbol \\phi(\\mathbf v) = \\left[\\begin{array}{c} v_1^2 \\\\ v_2^2 \\\\ c \\\\ \\sqrt{2}v_1v_2 \\\\ \\sqrt{2c}v_1 \\\\ \\sqrt{2c}v_2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Es decir, $\\mathbf v$, que tiene dos características ($v_1$ y $v_2$) , se ha transformado en un vector $\\boldsymbol \\phi(\\mathbf v)$ con 6 características que vienen dadas por la transformación\n",
    "$$\n",
    "\\left \\{ \\begin{array}{lcl} \\phi_1(v_1,v_2) &=& v_1^2 \\\\ \\phi_2(v_1,v_2) &=&v_2^2 \\\\ \\phi_3(v_1,v_2) &=&c \\\\ \\phi_4(v_1,v_2) &=&\\sqrt{2}v_1v_2 \\\\ \\phi_5(v_1,v_2) &=&\\sqrt{2c}v_1 \\\\ \\phi_6(v_1,v_2) &=&\\sqrt{2c}v_2\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ek75TjVV_tp"
   },
   "source": [
    "Supongamos ahora que en ese escenario hemos entrenado un método kernel que nos ha dado un modelo con 2 vectores soporte\n",
    "$$\n",
    "f(\\mathbf v) = \\sum_{i=1}^2\\alpha_i\\kappa(\\mathbf v_i, \\mathbf v) \n",
    "$$\n",
    "y que los vectores soporte son las observaciones $\\mathbf v_1 = \\mathbf a$ y $\\mathbf v_2 = \\mathbf b$ \n",
    "\n",
    "Entonces el modelo puede escribirse como\n",
    "$$\n",
    "f(\\mathbf v) = \\alpha_1\\kappa(\\mathbf a, \\mathbf v) + \\alpha_2\\kappa(\\mathbf b, \\mathbf v) \n",
    "$$\n",
    "\n",
    "Expresando los kernels como productos escalares con la transformación de antes:\n",
    "\n",
    "$$\n",
    "f(\\mathbf v) = \\alpha_1 \\left[\\begin{array}{cccccc}a_1^2 & a_2^2 & c & \\sqrt{2}a_1a_2 & \\sqrt{2c}a_1 & \\sqrt{2c}a_2 \\end{array}\\right]\\left[\\begin{array}{c} v_1^2 \\\\ v_2^2 \\\\ c \\\\ \\sqrt{2}v_1v_2 \\\\ \\sqrt{2c}v_1 \\\\ \\sqrt{2c}v_2\\end{array}\\right]+ \\alpha_2 \\left[\\begin{array}{cccccc}b_1^2 & b_2^2 & c & \\sqrt{2}b_1b_2 & \\sqrt{2c}b_1 & \\sqrt{2c}b_2 \\end{array}\\right]\\left[\\begin{array}{c} v_1^2 \\\\ v_2^2 \\\\ c \\\\ \\sqrt{2}v_1v_2 \\\\ \\sqrt{2c}v_1 \\\\ \\sqrt{2c}v_2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left[\\begin{array}{cccccc}\\alpha_1 a_1^2 + \\alpha_2 b_1^2 &  \\alpha_1 a_2^2 + \\alpha_2 b_2^2 &c (\\alpha_1 +\\alpha_2) & \\sqrt{2}(\\alpha_1 a_1a_2 + \\alpha_2 b_1b_2) & \\sqrt{2c}(\\alpha_1 a_1 + \\alpha_2 b_2) & \\sqrt{2c}(\\alpha_1 a_2 + \\alpha_2 b_2) \\end{array}\\right]\\left[\\begin{array}{c} v_1^2 \\\\ v_2^2 \\\\ c \\\\ \\sqrt{2}v_1v_2 \\\\ \\sqrt{2c}v_1 \\\\ \\sqrt{2c}v_2\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\mathbf w^\\top \\mathbf v\n",
    "$$\n",
    "\n",
    "Con lo que tenemos que el modelo no lineal en el espacio de entrada original (2D) es un modelo lineal en el nuevo espacio de características inducido por el kernel (6D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtW08QBsV_tp"
   },
   "source": [
    "Este resultado se puede generalizar para kernel polinómicos con grados mayores que 2 ($d>2$) y con dimensiones del espacio original mayores que dos $(m>2)$ mediante:\n",
    "$$\n",
    "\\left(\\mathbf u^\\top \\mathbf v + c\\right)^d = \\left(\\sum_{j=1}^mu_jv_j+c\\right)^d = \\left(\\sum_{j=1}^mu_jv_j\\right)^d + \\left(\\sum_{j=1}^mu_jv_j\\right)^{d-1}c + \\left(\\sum_{j=1}^mu_jv_j\\right)^{d-2}c^2+\\dots + \\left(\\sum_{j=1}^mu_jv_j\\right)^2 c^{d-2} +  \\left(\\sum_{j=1}^mu_jv_j\\right)c^{d-1} + c^d  \n",
    "$$\n",
    "\n",
    "Desarrollando la expresión anterior y separando en dos vectores, los términos que dependen de $\\mathbf u$ de los términos que dependen de $\\mathbf v$ llegamos a la definición explícita de la transformación $\\boldsymbol \\phi()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCoeXdKbV_tp"
   },
   "source": [
    "### Síntesis de características con *kernels* RBF\n",
    "\n",
    "En el caso del kernel RBF la teoría también nos garantiza que el kernel puede escribirse como un producto escalar de dos vectores de un espacio de características en el que se habrían proyectado las observaciones originales, pero es más difícil de recuperar la función de transformación $\\boldsymbol \\phi:\\mathbf v \\rightarrow \\boldsymbol \\phi(\\mathbf v)$.\n",
    "\n",
    "Vamos a presentar dos maneras de generar características sintéticas con kernels RBF:\n",
    "- La inmediata consistente en generar características con kernels RBF centrados en algunas observaciones del conjunto de entrenamiento, ya hemos repasado esto en la clase sobre SVM... A continuación lo ilustramos con un ejemplo sobre el conjunto de datos de uso de bicicletas en Seúl\n",
    "\n",
    "- Una aproximación a la transformación implícita $\\boldsymbol \\phi$ con *Random Fourier Features*, RFF. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtK6EAV_tq"
   },
   "source": [
    "#### Sintetizando características explícitamente con *kernels* RBF de distinta anchura\n",
    "\n",
    "Vamos a sintetizar $500$ características usando $500$ observaciones del espacio de entrada elegidas aleatoriamente del conjunto de entrenamiento y $500$ valores de anchura de *kernel* elegidos al azar entre los 4 valores $\\{0.1, 0.5, 1, 5\\}$ para definir $500$ funciones *kernel* RBF. Cada una de estas funciones nos va a proporcionar una característica sintética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FWotykHV_tq",
    "outputId": "d57b18e8-b15f-4baa-a89e-9d97d2ee983a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "class radial_basis_functions(object):\n",
    "    # centres: centros de los kernels RBF\n",
    "    # gammas: anchuras, cuanto más grande sea gamma, más estrecho es el kernel\n",
    "    def __init__(self, centres, gammas):\n",
    "        self.centres = centres\n",
    "        self.gammas = gammas\n",
    "    def transform(self, x):\n",
    "        salida = np.empty((x.shape[0], self.centres.shape[0]))\n",
    "        for ic,c in enumerate(self.centres):\n",
    "            salida[:,ic] = np.squeeze(rbf_kernel(x, c.reshape(1,-1), gamma = self.gammas[ic]))\n",
    "        return salida\n",
    "\n",
    "    \n",
    "n_centres = 500 # num features a sintetizar\n",
    "\n",
    "# elegir los centros aleatoriamente\n",
    "posi_centros = np.random.randint(0,len(Ytrain),size=n_centres,dtype=int) \n",
    "C = Xtrain_c[posi_centros, :]\n",
    "\n",
    "# elegir las anchuras aleatoriamente \n",
    "v_gammas = np.array([1e-1, .5, 1, 5])\n",
    "gammas = v_gammas[np.random.randint(0,len(v_gammas),size=n_centres,dtype=int)]\n",
    "\n",
    "# generar n_centres RBFs con gamma diferente\n",
    "rbf = radial_basis_functions(C, gammas)\n",
    "\n",
    "# transformar las características numéricas originales\n",
    "X_train_rbf = rbf.transform(Xtrain_c)\n",
    "X_test_rbf = rbf.transform(Xtest_c)\n",
    "\n",
    "print(\"{0:d} características originales transformadas en {1:d} características sintéticas\".format(Xtrain_c.shape[1],\n",
    "                                                                                                 X_train_rbf.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lR8b95dV_tq"
   },
   "source": [
    "Aprendemos un modelo lineal regularizado con LASSO para que se haga una selección implícita de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRZek1AfV_tq",
    "outputId": "e871b109-833d-4021-e107-b6b65f30a90f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "v_alpha = [1e-3,1e-2, 1e-1, 1,10,100]\n",
    "\n",
    "#fijamos la tolerancia del Lasso en 1 para que converja\n",
    "grid_lasso = GridSearchCV(Lasso(tol=1, normalize=True), param_grid={'alpha':v_alpha}).fit(X_train_rbf, Ytrain)\n",
    "\n",
    "print(grid_lasso.best_params_)\n",
    "print('R^2 train: {0:.3f}'.format(grid_lasso.score(X_train_rbf, Ytrain)))\n",
    "print('R^2 test: {0:.3f}'.format(grid_lasso.score(X_test_rbf, Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slYeq7GZV_tq"
   },
   "source": [
    "La siguiente celda recupera la posición de las características que se usan explícitamente en el modelo lineal, es decir, cuyos pesos son distintos de $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rRIZa2lV_tq",
    "outputId": "17276113-3b79-46a8-9d39-8256f3a42a73"
   },
   "outputs": [],
   "source": [
    "non_neg_gamma = np.where(grid_lasso.best_estimator_.coef_ != 0)[0]\n",
    "print(\"se usan en el modelo lineal el {0:.0f}% de las características sintetizadas\".format(100*np.mean(grid_lasso.best_estimator_.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf0Ti0ZSV_tr"
   },
   "source": [
    "En función del signo de la característica podemos deducir si parecerse mucho al vector soporte correspondiente implica que el número de bicicletas que se van a alquilar esa hora del día aumente o disminuya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo4_0JK-V_tr",
    "outputId": "4e420a07-7e82-4154-c1ef-488a21de5efe"
   },
   "outputs": [],
   "source": [
    "grid_lasso.best_estimator_.coef_[non_neg_gamma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lZXKZOLV_ts",
    "outputId": "ee6df7ce-c593-4879-cfed-00546e97ec84"
   },
   "outputs": [],
   "source": [
    "imc = np.argmax((grid_lasso.best_estimator_.coef_))\n",
    "mc = posi_centros[imc]\n",
    "print(\"la caracteristica con mayor peso positivo es la {0:d}, el peso es {1:.2f} \".format(mc, grid_lasso.best_estimator_.coef_[imc]))\n",
    "print(seoul_train.iloc[mc][continuas])\n",
    "#print(pd.Series(Xtrain_c[mc,:], index=continuas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZguYUpHV_ts",
    "outputId": "cf916426-a8e6-4cd1-d29c-37d8b88ac1ed"
   },
   "outputs": [],
   "source": [
    "imc = np.argmin((grid_lasso.best_estimator_.coef_))\n",
    "mc = posi_centros[imc]\n",
    "print(\"la caracteristica con mayor peso negativo es la {0:d}, el peso es {1:.2f} \".format(mc, grid_lasso.best_estimator_.coef_[imc]))\n",
    "print(seoul_train.iloc[mc][continuas])\n",
    "#print(pd.Series(Xtrain_c[mc,:], index=continuas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24TNnYE4V_ts"
   },
   "source": [
    "También podemos interpretar la resolución del kernel mirando a la proporción de `gamma` en las características que pasan a tener peso no nulo después de aprender el modelo lineal con Lasso. \n",
    "\n",
    "En la figura de abajo las barras azules son el número de `gamma` de cada resolución antes de optimizar con LASSO y las barras naranjas el número de pesos no nulos del modelo lineal que se corresponden con cada valor `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6Y1J5byV_ts",
    "outputId": "37d09529-50ad-4b8b-d667-0eaea68199b9"
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(np.unique(gammas)))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "plt.figure()\n",
    "nn,bins = np.histogram(gammas,bins=[0,0.15,0.6,1.2,10])\n",
    "plt.bar(x - width/2,nn,width, label='antes')\n",
    "nn1,bins1 = np.histogram(gammas[non_neg_gamma],bins=[0,0.15,0.6,1.2,10])\n",
    "plt.bar(x + width/2,nn1, width,label='después')\n",
    "plt.xticks(x, v_gammas)\n",
    "plt.xlabel('gamma')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrTwRqfGV_ts"
   },
   "source": [
    "#### Random Fourier Features\n",
    "\n",
    "La descomposición del *kernel* RBF en la transformación que lleva al espacio de características inducido donde ese *kernel* es equivalente al producto escalar no es fácil de conseguir, entre otras cosas porque el número de términos de la transformación es infinito, es decir, $\\boldsymbol \\phi(\\mathbf x)$ tiene infinitas componentes.\n",
    "\n",
    "Hay una manera de conseguir una aproximación a la $\\boldsymbol \\phi(\\mathbf x)$ real con un número finito de componentes que consiste en obtener muestras de su **transformada de Fourier**.\n",
    "\n",
    "La **transformada de Fourier** consiste básicamente en aproximar una función mediante una combinación lineal de sinusoides con distintas frecuencias.\n",
    "\n",
    "Por ejemplo  las siguientes celdas describen cómo se puede aproximar una función compuesta por escalones con combinaciones de sinusoides. La transformada de Fourier serían los coeficientes de las sinusoides en la mezcla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI37GTzGV_ts"
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8zpl9Q5V_tt",
    "outputId": "1404bc8c-fe07-4a59-be47-ab50c775df2c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import *\n",
    "t = np.linspace(-10, 10, 2**11) #2^11 points on [-10, 10]\n",
    "esc = (t[1]-t[0])/10\n",
    "#making the signal: 5 on (-7.5, -2.5), -5 on (2.5, 7.5) and 0 otherwise\n",
    "x = np.zeros(len(t))\n",
    "x[((t < -2.5) & (t > -7.5))] = 5\n",
    "x[((t > 2.5) & (t < 7.5))] = -5\n",
    "\n",
    "f = rfft(x) #the rfft\n",
    "#esc = np.max(np.absolute(x))/np.max(np.absolute(f.imag))\n",
    "freal = f.real*esc #real part\n",
    "fimag = f.imag*esc #imaginary part\n",
    "\n",
    "fq = rfftfreq(len(t), t[1]-t[0]) \n",
    "plt.plot(t,x)\n",
    "f_show = [1,3,5,7,9,11,15,21,101]\n",
    "reconst = np.zeros(len(t))\n",
    "for ff in range(1,len(fimag)):\n",
    "    update = fimag[ff]*np.sin(2*np.pi*t*fq[ff])\n",
    "    reconst += update\n",
    "    \n",
    "    if ff in f_show:\n",
    "        plt.figure()\n",
    "        plt.title('{0:d} componentes'.format(ff))\n",
    "        plt.plot(t,x)\n",
    "        plt.plot(t, reconst)\n",
    "        plt.plot(t,update,linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUI860iaV_tt"
   },
   "source": [
    "Vamos a plantear un ejemplo en 2D resuelto con una SVM con *kernel* RBF para comparar con la alternativa de resolverlo usando una SVM lineal pero alimentada con características sintetizadas usando RFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7W-rMuJV_tt"
   },
   "outputs": [],
   "source": [
    "def plot_kernel_classifier(xmin, xmax, ymin, ymax,xc,gamma,alpha,ax,b=0, margin=False, color='black',contr=True):\n",
    "    # generate a grid of points to evaluate classifier\n",
    "    \n",
    "    Xg = np.linspace(xmin-.05*np.absolute(xmin),\n",
    "                   xmax + .05*np.absolute(xmax),\n",
    "                   20) # or whatever values for x\n",
    "    \n",
    "    Yg = np.linspace(ymin-.05*np.absolute(ymin),\n",
    "                   ymax + .05*np.absolute(ymax),\n",
    "                   20)   # or whatever values for y\n",
    "    X, Y = np.meshgrid(Xg, Yg)\n",
    "    Xshape = X.shape\n",
    "    \n",
    "    x_test = np.hstack((X.reshape(-1,1), Y.reshape(-1,1)))\n",
    "    zz = rbf_kernel(x_test, xc, gamma).dot(alpha) + b\n",
    "    zz = zz.reshape(Xshape)\n",
    "    if contr:\n",
    "      ax.contour(X, Y, zz, [0], colors=color)\n",
    "    if margin:\n",
    "      ax.contour(X, Y, zz, [-1,1], linestyles='dashed', colors=color)\n",
    "    return ax.contourf(X, Y, zz, alpha=0.2, cmap='PuOr')\n",
    "\n",
    "def plot_rff_component(xmin, xmax, ymin, ymax,rbfsampler,alpha,i_alpha,ax):\n",
    "    # generate a grid of points to evaluate classifier\n",
    "    \n",
    "    Xg = np.linspace(xmin-.05*np.absolute(xmin),\n",
    "                   xmax + .05*np.absolute(xmax),\n",
    "                   20) # or whatever values for x\n",
    "    \n",
    "    Yg = np.linspace(ymin-.05*np.absolute(ymin),\n",
    "                   ymax + .05*np.absolute(ymax),\n",
    "                   20)   # or whatever values for y\n",
    "    X, Y = np.meshgrid(Xg, Yg)\n",
    "    Xshape = X.shape\n",
    "    \n",
    "    x_test = np.hstack((X.reshape(-1,1), Y.reshape(-1,1)))\n",
    "    zz = rbfsampler.transform(x_test)[:,i_alpha]*alpha\n",
    "    zz = zz.reshape(Xshape)\n",
    "    return ax.contourf(X, Y, zz, alpha=0.2, cmap='PuOr')\n",
    "\n",
    "def plot_rff_classifier(xmin, xmax, ymin, ymax,rbfsampler,classifier,ax):\n",
    "    # generate a grid of points to evaluate classifier\n",
    "    \n",
    "    Xg = np.linspace(xmin-.05*np.absolute(xmin),\n",
    "                   xmax + .05*np.absolute(xmax),\n",
    "                   20) # or whatever values for x\n",
    "    \n",
    "    Yg = np.linspace(ymin-.05*np.absolute(ymin),\n",
    "                   ymax + .05*np.absolute(ymax),\n",
    "                   20)   # or whatever values for y\n",
    "    X, Y = np.meshgrid(Xg, Yg)\n",
    "    Xshape = X.shape\n",
    "    \n",
    "    x_test = np.hstack((X.reshape(-1,1), Y.reshape(-1,1)))\n",
    "    zz = classifier.decision_function(rbfsampler.transform(x_test))\n",
    "    zz = zz.reshape(Xshape)\n",
    "    return ax.contourf(X, Y, zz, alpha=0.2, cmap='PuOr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUr1DWL4V_tu",
    "outputId": "064013af-fb80-4ad5-83b7-0afc85916061"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "nc = 15\n",
    "xc, yc = make_circles(nc, factor=0.3, noise=.1, random_state=42)\n",
    "yc[yc==0]=-1\n",
    "xk = np.vstack((xc, np.array([[0.2,0.03],[-0.09,-0.2],[.6, 0.75],[0.6, 0.5]])))\n",
    "yk = np.hstack((yc,np.array([-1., -1., 1. , -1.])))\n",
    "ff2,aa2 = plt.subplots(1,1, figsize=(7,6))\n",
    "aa = aa2\n",
    "aa.scatter(xk[yk==-1,0], xk[yk==-1,1], marker='x', s=80, label='train - class' )\n",
    "aa.scatter(xk[yk==1,0], xk[yk==1,1], marker='o', s=80, label='train + class' )\n",
    "aa.set_xlabel('$x_0$')\n",
    "aa.set_ylabel('$x_1$')\n",
    "aa.set_title('Kernel, soft margin')\n",
    "from sklearn import svm\n",
    "gamma=1\n",
    "model = svm.SVC(C=1,kernel='rbf', gamma=gamma)\n",
    "model.fit(xk,yk)\n",
    "akkyk = model.dual_coef_.T \n",
    "bkk = model.intercept_\n",
    "cs = plot_kernel_classifier(-1.3, 1.3, -1.3, 1.3,model.support_vectors_,gamma,akkyk,aa,bkk,True)\n",
    "_ = ff2.colorbar(cs, ax=aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbPBBPzAV_tu"
   },
   "source": [
    "El modelo basado en kernels puede descomponerse en las contribuciones de cada kernel y asimilar cada una de ellas a una característica sintética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utXCTpBMV_tu",
    "outputId": "889fb8f7-1e72-41c1-feb3-af7484a06fee"
   },
   "outputs": [],
   "source": [
    "ff, aa = plt.subplots(5,4,figsize=(20,24))\n",
    "col = 0\n",
    "row = 0\n",
    "nc = xk.shape[0]\n",
    "for iic, c in enumerate(model.support_):\n",
    "  ax = aa[row][col]\n",
    "  cs = plot_kernel_classifier(-1.2, 1.2, -1.2, 1.2,xk[c,:].reshape(1,-1),gamma,akkyk[iic],ax,0,False,contr= False)\n",
    "  _ = ff.colorbar(cs, ax=ax)\n",
    "  ax.scatter(xk[yk==-1,0], xk[yk==-1,1], marker='x', s=80, label='train - class' )\n",
    "  ax.scatter(xk[yk==1,0], xk[yk==1,1], marker='o', s=80, label='train + class' )\n",
    "  ax.scatter(xk[c,0], xk[c,1],marker='d', color='black', label='SVs')\n",
    "  ax.set_xlabel('$x_0$')\n",
    "  ax.set_ylabel('$x_1$')\n",
    "  ax.set_title('$f_{2}{0:d}{3}(x)={1:.2f}\\\\kappa(x_{2}{0:d}{3},x)$'.format(c+1,akkyk[iic][0],\"{\",\"}\"))\n",
    "  ax.legend()\n",
    "  col += 1\n",
    "  if col == 4:\n",
    "    col = 0\n",
    "    row += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s--TTsZdV_tu"
   },
   "source": [
    "La clase [RBFSampler](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html) de  scikit learn devuelve muestras de la transformada de Fourier de un kernel RBF. Se usa como las demás transformaciones que hemos visto a lo largo del curso (`fit`, `fit_transform` y `transform`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwOayL17V_tu"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# especificar la gamma del kernel y el numero de frecuencias que queremos obtener\n",
    "feature_map_fourier = RBFSampler(gamma=gamma, \n",
    "                                         random_state=1,\n",
    "                                        n_components=20) \n",
    "\n",
    "# aprendemos la transformación\n",
    "feature_map_fourier.fit(xk, yk)\n",
    "# H tiene las características sintetizadas del conjunto de entrenamiento\n",
    "# cada columna una característica\n",
    "H = feature_map_fourier.transform(xk)\n",
    "\n",
    "#aprender el modelo **lineal**\n",
    "lsvm = SVC(kernel='linear',C=1).fit(H, yk)\n",
    "alfa = lsvm.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzUrq8rCV_tu",
    "outputId": "1eed7a0f-3450-4d7f-c917-0727fe5a0941"
   },
   "outputs": [],
   "source": [
    "lsvm.predict(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCC9_bstV_tv"
   },
   "source": [
    "Representación de la salida del modelo lineal alimentado con las características obtenidas mediante RFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYnmTlOwV_tv",
    "outputId": "ad6dedc9-f44d-405e-ff20-f364d3377d8e"
   },
   "outputs": [],
   "source": [
    "ff2,ax = plt.subplots(1,1, figsize=(7,6))\n",
    "cs = plot_rff_classifier(-1.3, 1.3, -1.3, 1.3,feature_map_fourier,lsvm,ax)\n",
    "_ = ff2.colorbar(cs, ax=ax)\n",
    "aa = ax\n",
    "aa.scatter(xk[yk==-1,0], xk[yk==-1,1], marker='x', s=80, label='train - class' )\n",
    "aa.scatter(xk[yk==1,0], xk[yk==1,1], marker='o', s=80, label='train + class' )\n",
    "aa.set_xlabel('$x_0$')\n",
    "aa.set_ylabel('$x_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc8aCHhSV_tv"
   },
   "source": [
    "Representación de cada una de las 20 características sintetizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyBeLzaiV_tw",
    "outputId": "e7b46f05-3b8c-4921-f463-e92fb04d33fb"
   },
   "outputs": [],
   "source": [
    "ff, aa = plt.subplots(5,4,figsize=(20,24))\n",
    "col = 0\n",
    "row = 0\n",
    "nc = xk.shape[0]\n",
    "for c in range(20):\n",
    "  ax = aa[row][col]\n",
    "  \n",
    "  cs = plot_rff_component(-1.2, 1.2, -1.2, 1.2,feature_map_fourier,alfa[c],c,ax)\n",
    "  _ = ff.colorbar(cs, ax=ax)\n",
    "  ax.scatter(xk[yk==-1,0], xk[yk==-1,1], marker='x', s=80, label='train - class' )\n",
    "  ax.scatter(xk[yk==1,0], xk[yk==1,1], marker='o', s=80, label='train + class' )\n",
    "  ax.set_xlabel('$x_0$')\n",
    "  ax.set_ylabel('$x_1$')\n",
    "  ax.set_title('$f_{2}{0:d}{3}(x)={1:.2f}\\\\phi_{2}{0:d}{3}(x)$'.format(c+1,alfa[c],\"{\",\"}\"))\n",
    "  ax.legend()\n",
    "  col += 1\n",
    "  if col == 4:\n",
    "    col = 0\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YBQcuDkV_tw"
   },
   "source": [
    "Representación del clasificador lineal obtenido en función del número de características RFF empleadas para alimentarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40mcagoBV_tw",
    "outputId": "24c11cbf-5c3f-4fd7-8947-ac51d70b94ab"
   },
   "outputs": [],
   "source": [
    "n_rff = [2,3,4,5,\n",
    "         10,20,40,50,\n",
    "        100,200,400,500,\n",
    "        1000,2000,5000,10000]\n",
    "ff, aa = plt.subplots(4,4,figsize=(20,20))\n",
    "col = 0\n",
    "row = 0\n",
    "nc = xk.shape[0]\n",
    "for inc, nc in enumerate(n_rff):\n",
    "  ax = aa[row][col]\n",
    "  feature_map_fourier = RBFSampler(gamma=gamma, \n",
    "                                         random_state=1,\n",
    "                                        n_components=nc) \n",
    "\n",
    "  feature_map_fourier.fit(xk, yk)\n",
    "  H = feature_map_fourier.transform(xk)\n",
    "  lsvm = SVC(kernel='linear',C=1).fit(H, yk)\n",
    "  cs = plot_rff_classifier(-1.3, 1.3, -1.3, 1.3,feature_map_fourier,lsvm,ax)\n",
    "  _ = ff2.colorbar(cs, ax=ax)\n",
    "  ax.scatter(xk[yk==-1,0], xk[yk==-1,1], marker='x', s=80, label='train - class' )\n",
    "  ax.scatter(xk[yk==1,0], xk[yk==1,1], marker='o', s=80, label='train + class' )\n",
    "  ax.set_xlabel('$x_0$')\n",
    "  ax.set_ylabel('$x_1$')\n",
    "  ax.set_title('{0:d} Random Fourier Features'.format(nc))\n",
    "  ax.legend()\n",
    "  col += 1\n",
    "  if col == 4:\n",
    "    col = 0\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNv9fvTVV_tx"
   },
   "source": [
    "A continuación aplicamos esta técnica de síntesis de características al problema de estimar el alquiler de  bicicletas en Seúl. \n",
    "\n",
    "El modelo de regresión que usamos es [KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html). Vamos a emplear una versión reducida a únicamente 1000 observaciones del conjunto de entrenamiento porque KernelRidge no es disperso.\n",
    "\n",
    "Optimizamos modelo e hiperparámetros empleando validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO55LOPdV_tx",
    "outputId": "bd9ff4cf-a6d1-414c-f7f6-10c7733ef310"
   },
   "outputs": [],
   "source": [
    "dim = Xtrain_c.shape[1]\n",
    "v_gamma = [.001, .01,  .1, 1,  10]\n",
    "v_alpha = [ .001, 0.01, .1, 1, 10]\n",
    "\n",
    "nm=1000\n",
    "parameters = { \n",
    "              'gamma' : v_gamma, \n",
    "              'alpha' : v_alpha}\n",
    "kr_grid = GridSearchCV(KernelRidge(kernel='rbf'), parameters, cv=5)\n",
    "kr_grid.fit(Xtrain_c[:nm,:], Ytrain[:nm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5P8SothV_tx",
    "outputId": "fd68795f-ea68-4bc9-8d56-9c9241c95232"
   },
   "outputs": [],
   "source": [
    "print('mejores hiperparámetros')\n",
    "print(kr_grid.best_params_)\n",
    "print('')\n",
    "print(\"R^2 estimado por CV {0:.2f}\".format(kr_grid.best_score_))\n",
    "print(\"R^2 en test {0:.2f}\".format(kr_grid.score(Xtest_c, Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gLd4NnrV_tx"
   },
   "source": [
    "Ahora vamos a aproximar el kernel RBF del modelo óptimo usando RFF con distinto número de componentes (dados por la lista `v_N_components_FS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3wrL-IyV_tx"
   },
   "outputs": [],
   "source": [
    "n_train = Xtrain_c.shape[0]\n",
    "v_percentage_RFF = [.01,  .02, .03, .04, .05, .06, .07, .08, .09,\n",
    " .1,  .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.1, 1.2, 1.5, 2, 3,5,10, 20, 40, 80]\n",
    "\n",
    "v_N_components_FS = [int(n_train * mumu) for mumu in v_percentage_RFF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRdpWKbHV_ty"
   },
   "outputs": [],
   "source": [
    "\n",
    "# seleccionamos los hiperparámetros que encontramos por CV para el KernelRidge original\n",
    "gamma=kr_grid.best_params_['gamma']\n",
    "alpha = kr_grid.best_params_['alpha']\n",
    "r2_test = np.empty(len(v_N_components_FS))\n",
    "for inc, nc in enumerate(v_N_components_FS):\n",
    "    # Fourier Transform of the RBF kernel\n",
    "    feature_map_fourier = RBFSampler(gamma=gamma, \n",
    "                                         random_state=1,\n",
    "                                        n_components=nc) \n",
    "\n",
    "    feature_map_fourier.fit(Xtrain_c[:nm,:], Ytrain[:nm])\n",
    "    H = feature_map_fourier.transform(Xtrain_c[:nm,:])\n",
    "    Htest = feature_map_fourier.transform(Xtest_c)\n",
    "    \n",
    "    lr = Ridge(alpha=alpha)\n",
    "    lr.fit(H,Ytrain[:nm])\n",
    "    r2_test[inc] = lr.score(Htest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdjnrnVHV_ty"
   },
   "source": [
    "La siguiente gráfica muestra cómo el modelo `RFF + LinearRegression` converge hacia el `KernelRidge` original a medida que se aumenta el número de componentes RFF que se añaden, es decir, que la aproximación de RFF kernel a la proyección implícita $\\boldsymbol \\phi()$  asociada al kernel es más precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jx9Ud9nZV_ty",
    "outputId": "8b37ccac-c868-4862-98f1-9490ee3f8fb5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(v_N_components_FS, np.ones(len(v_N_components_FS))*kr_grid.score(Xtest_c, Ytest), label='RBF kernel')\n",
    "\n",
    "plt.plot(v_N_components_FS, r2_test, label='RFF approximtion')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('Num RFFs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02wCfm2XV_ty"
   },
   "source": [
    "# 4. Feature Tools: una librería python para sintetizar características automáticamente\n",
    "\n",
    "La última parte de la sesión la vamos a dedicar a presentar [`featuretools`](https://featuretools.alteryx.com/en/stable/index.html), una libería python desarrollada por un grupo de investigación del MIT para sintetizar características a partir de datos almacenados en una base de datos relacional.\n",
    "\n",
    "Este [artículo](https://www.bbvaapimarket.com/es/mundo-api/deep-feature-synthesis-el-algoritmo-que-automatizara-el-machine-learning/) presenta la librería y se explica la metodología seguida en su desarrollo. El artículo original lo podéis encontrar [aquí](https://www.bbvaapimarket.com/wp-content/uploads/2016/01/DSAA_DSM_2015.pdf)\n",
    "\n",
    "Vamos a seguir los pasos del tutorial incluido en la documentación para familizarnos con esta herramienta que automatiza algunos de los contenidos que hemos revisado hoy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6400,
     "status": "ok",
     "timestamp": 1614869439372,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "8e2WpEv8t-Sd",
    "outputId": "8319ffb2-a29e-4e23-840d-88a956d15f51"
   },
   "outputs": [],
   "source": [
    "!pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1612,
     "status": "ok",
     "timestamp": 1614869442636,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "1SovcL_GV_tz"
   },
   "outputs": [],
   "source": [
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIvW2T2Htxy3"
   },
   "source": [
    "## Cargar datos del tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1614869452087,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "jPDsKWmfV_tz"
   },
   "outputs": [],
   "source": [
    "data=ft.demo.load_mock_customer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2NS3R9BuKsd"
   },
   "source": [
    "`Data` es un diccionario con 3 dataframes:\n",
    "- `customers` con información de clientes \n",
    "- `sessions` que han protagonizado los clientes\n",
    "- `transactions` que han ocurrido en la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1614869564589,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "nPutid6upSJr",
    "outputId": "a6f6cd37-97d3-4f15-a22a-b4be61444b28"
   },
   "outputs": [],
   "source": [
    "customers_df = data[\"customers\"]\n",
    "print(customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1614869626753,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "87V5fRoGugpH",
    "outputId": "a018cd96-c452-43bd-af73-3701c24ef938"
   },
   "outputs": [],
   "source": [
    "sessions_df = data[\"sessions\"]\n",
    "print(sessions_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1614869618610,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "beKzLaWcumYD",
    "outputId": "72229844-6256-4563-9b94-3219e885fc99"
   },
   "outputs": [],
   "source": [
    "transactions_df = data[\"transactions\"]\n",
    "print(transactions_df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByHBxRxZwRwp"
   },
   "source": [
    "Definir un diccionario con las **entidades**. Entidad es cada una de las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1614870121681,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "Rs35SyKRut2c"
   },
   "outputs": [],
   "source": [
    "entities = {\n",
    "       \"customers\" : (customers_df, \"customer_id\"),\n",
    "       \"sessions\" : (sessions_df, \"session_id\", \"session_start\"),\n",
    "       \"transactions\" : (transactions_df, \"transaction_id\", \"transaction_time\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_4RL6yHw9aG"
   },
   "source": [
    "Definir **relaciones** entre las entidades. Las características que se generan con esta herramienta reflejan estas relaciones. Las relaciones entre entidades pueden ser **uno-a-muchos**:\n",
    "- Un cliente ha realizado varias sesiones, luego el cliente puede enriquecerse con características sintetizadas de sus sesiones y las sesiones pueden enriquecerse con características del cliente que las realizó. En este útlimo caso las características de cliente son iguales para todas sus sesiones.\n",
    "- En una sesión hay varias transacciones. Cada sesión puede enriquecerse con características sintetizadas de todas sus transacciones. Pero **recursivamente** cada cliente puede enriquecerse con características de las transacciones a través de la sesión.\n",
    "\n",
    "Una relación se define con una *tupla* de 4 elementos:\n",
    "~~~\n",
    "(entidad_padre, variable_padre, entidad_hijo, variable_hijo)\n",
    "~~~\n",
    "\n",
    "En este conjunto de datos hay dos relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1614870768128,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "Yjo3sttkwoos"
   },
   "outputs": [],
   "source": [
    "relationships = [(\"sessions\", \"session_id\", \"transactions\", \"session_id\"),\n",
    "                 (\"customers\", \"customer_id\", \"sessions\", \"customer_id\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNXJbFmhzK2H"
   },
   "source": [
    "### Sintetizar características\n",
    "\n",
    "Hay que llamar al motor de síntesis con:\n",
    "- conjunto de entidades\n",
    "- lista de relaciones\n",
    "- entidad *target*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1614870856336,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "xUnS3mz_zGfD"
   },
   "outputs": [],
   "source": [
    "feature_matrix_customers, features_defs = ft.dfs(entities=entities,\n",
    "                                                    relationships=relationships,\n",
    "                                                     target_entity=\"customers\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1614870906433,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "sSARyTCkzb9N",
    "outputId": "4cc4c5c4-796c-425b-f23d-5e3817513f6b"
   },
   "outputs": [],
   "source": [
    "feature_matrix_customers.loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1614871085664,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "tvd3RluH0QI5",
    "outputId": "75bfdad3-396b-464c-c204-d85d0e649317"
   },
   "outputs": [],
   "source": [
    "len(feature_matrix_customers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1614870897301,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "Npmoqk5GzdtV",
    "outputId": "88a06a1c-22f8-4ec8-a988-69fac26198cd"
   },
   "outputs": [],
   "source": [
    "customers_df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1614871106715,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "jkfogxmy0WMo",
    "outputId": "4022849e-f870-4ba8-fe93-5a1ee81f9e73"
   },
   "outputs": [],
   "source": [
    "len(customers_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrVSk8USzu7J"
   },
   "source": [
    "`Featuretools` aplica una serie de primitivas a los registros de las tablas para sintetizar automáticamente las características. \n",
    "\n",
    "También hay utilidades para entender las características sintetizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1614871200404,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "GI5qwF8zzgr4",
    "outputId": "58566368-2f2a-49c6-c2be-1dbde9e76601"
   },
   "outputs": [],
   "source": [
    "feature = features_defs[3]\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1614871211340,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "DR7GWGX-0p7M",
    "outputId": "ea194032-dfc0-4550-cc75-b1b9568103f3"
   },
   "outputs": [],
   "source": [
    "ft.graph_feature(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1614871259208,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "sKdoKOjk0yoc",
    "outputId": "c27ee808-7dd3-4f7e-b7f3-ba48d04b32fd"
   },
   "outputs": [],
   "source": [
    "feature = features_defs[76]\n",
    "ft.graph_feature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNRFFvRe1VHn"
   },
   "source": [
    "Podemos cambiar la entidad *target*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1614871379501,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "BDRp90Wt0-Yw"
   },
   "outputs": [],
   "source": [
    "feature_matrix_sessions, features_defs = ft.dfs(entities=entities,\n",
    "                                                   relationships=relationships,\n",
    "                                                    target_entity=\"sessions\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1614871393530,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "8UCfFTiV1bru",
    "outputId": "a00d2133-5839-4b6a-c4e8-36204aa83f5b"
   },
   "outputs": [],
   "source": [
    "feature_matrix_sessions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoHC4EW813fR"
   },
   "source": [
    "### Primitivas\n",
    "\n",
    "Featuretools tiene dos tipos de primitivas:\n",
    "- **Transformaciones** toman como entrada una variable de una entidad y la transforman en otra variable para esa entidad. Por ejemplo sacar el año en que el cliente se dio de alta a partir de la fecha de alta\n",
    "- **Agregaciones**: Toman como entrada instancias relacionadas con la entidad y las agregan en un único valor (estilo `GroupBy`). Por ejemplo sacar el número de sesiones que ha realizado un cliente. Para esto hay que agrupar las sesiones por el identificador de cliente y contar.\n",
    "\n",
    "Algunos ejemplos de primitivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1614871728497,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "xGmeksr-1fJu",
    "outputId": "2f408255-c102-4419-eb5a-f5485325559a"
   },
   "outputs": [],
   "source": [
    "ft.list_primitives().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ncdioyv27Hy"
   },
   "source": [
    "Featuretools también permite codificar primitivas *custom*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_j8C2tG_2w9F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Features_from_machine_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
